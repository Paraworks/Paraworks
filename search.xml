<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>AI-vtuber</title>
    <url>/2024/09/15/AI-vtuber/</url>
    <content><![CDATA[<h2 id="2023-2-10更新-vits-onnx-一键式启动"><a href="#2023-2-10更新-vits-onnx-一键式启动" class="headerlink" title="2023&#x2F;2&#x2F;10更新 vits-onnx 一键式启动"></a>2023&#x2F;2&#x2F;10更新 <a href="https://github.com/Paraworks/vits_with_chatgpt-gpt3/tree/onnx">vits-onnx</a> 一键式启动</h2><h2 id="2023-2-17更新-弃用renpy-采用桌宠版本"><a href="#2023-2-17更新-弃用renpy-采用桌宠版本" class="headerlink" title="2023&#x2F;2&#x2F;17更新 弃用renpy 采用桌宠版本"></a>2023&#x2F;2&#x2F;17更新 弃用renpy <a href="https://github.com/Arkueid/Live2DMascot">采用桌宠版本</a></h2><h1 id="live2d启动器：克隆Live2DMascot仓库后，修改config-json文件"><a href="#live2d启动器：克隆Live2DMascot仓库后，修改config-json文件" class="headerlink" title="live2d启动器：克隆Live2DMascot仓库后，修改config.json文件"></a>live2d启动器：克隆<a href="https://github.com/Arkueid/Live2DMascot">Live2DMascot</a>仓库后，修改config.json文件</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&quot;ChatAPI&quot; : </span><br><span class="line">&#123;</span><br><span class="line">	&quot;ChatSavePath&quot; : &quot;chat&quot;,  //聊天音频和文本保存路径</span><br><span class="line">	&quot;CustomChatServer&quot; : </span><br><span class="line">	&#123;</span><br><span class="line">		&quot;HostPort&quot; : &quot;http://127.0.0.1:8080&quot;,  //服务器地址，端口默认8080</span><br><span class="line">		&quot;On&quot; : true,  //开启自定义聊天接口</span><br><span class="line">		&quot;ReadTimeOut&quot; : 114,  //等待响应时间(s)</span><br><span class="line">		&quot;Route&quot; : &quot;/chat&quot;  //路径</span><br><span class="line">	&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="选择1：server端启动后端api程序-Windows也可以"><a href="#选择1：server端启动后端api程序-Windows也可以" class="headerlink" title="选择1：server端启动后端api程序(Windows也可以)"></a>选择1：server端启动后端api程序(Windows也可以)</h1><h2 id="Combining-chatgpt-gpt3-vits-as-api-and-launch-it（Server-suggested）"><a href="#Combining-chatgpt-gpt3-vits-as-api-and-launch-it（Server-suggested）" class="headerlink" title="Combining chatgpt&#x2F;gpt3&amp;vits as api and launch it（Server suggested）"></a>Combining chatgpt&#x2F;gpt3&amp;vits as api and launch it（Server suggested）</h2><p>将<a href="https://github.com/Paraworks/vits_with_chatgpt-gpt3/blob/main/inference_api.py">inference_api.py</a>&#x2F;<a href="https://github.com/Paraworks/vits_with_chatgpt-gpt3/blob/main/inference_api_single_speaker.py">inference_api_single_speaker.py</a>丢入你的vits项目或moegoe项目中</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone path/to/your/vits</span><br><span class="line">cd vits</span><br><span class="line">python inference_api.py --model path/to/vits_model.pth --cfg path/to/vits_config.json --key your_openai_api_key</span><br><span class="line">#Single speaker</span><br><span class="line">python inference_api_single_speaker.py --model path/to/vits_model.pth --cfg path/to/vits_config.json --key your_openai_api_key</span><br><span class="line">#修改: 从68行开始，如果该模型用到了类似中日双语cleaner，恢复</span><br><span class="line">#text = f&quot;[JA]&#123;text&#125;[JA]&quot; if is_japanese(text) else f&quot;[ZH]&#123;text&#125;[ZH]&quot;</span><br><span class="line">#选择speaker</span><br><span class="line">speaker_id = 1</span><br><span class="line">#可自行添加chatbot</span><br><span class="line">#此处用到的 </span><br><span class="line">text = gpt3_chat(text)</span><br></pre></td></tr></table></figure>
<h1 id="选择2：绿皮思路chat"><a href="#选择2：绿皮思路chat" class="headerlink" title="选择2：绿皮思路chat"></a>选择2：<a href="https://github.com/Paraworks/vits_with_chatgpt-gpt3/blob/main/inference_ork.py">绿皮思路chat</a></h1><p>支持从外部启动任何正在运行的live2d模型，比如说修改点击事件的对应音频来实现。只需在你的vits项目中加入inference_ork.py这个小文件，然后启动它。注意，需要你能够在自己的windows上部署vits项目，推荐安装好cuda，<a href="https://www.bilibili.com/video/BV13t4y1V7DV/?spm_id_from=333.337.search-card.all.click&vd_source=7e8cf9f5c840ec4789ccb5657b2f0512">教程
</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Nijigasaki COC跑团版</span><br><span class="line">git clone https://huggingface.co/spaces/Mahiruoshi/Lovelive_Nijigasaki_VITS</span><br><span class="line">cd Lovelive_Nijigasaki_VITS</span><br><span class="line">pip install -r requirements.txt</span><br><span class="line">python inference_ork.py</span><br></pre></td></tr></table></figure>
<p>许多live2d都有触碰事件，那么，只需要有一个装载有完整cubism for native功能的live2d播放器,其可以支持点击事件并且支持对口型功能。<br>此时只需要启动 inference_ork.py<br><img src="/images/zhuxu.png" alt="Image text"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#在程序中将audio参数修改为触碰动作的音频路径</span><br><span class="line">#比如</span><br><span class="line">parser.add_argument(&#x27;--audio&#x27;,</span><br><span class="line">                    type=str,</span><br><span class="line">                    help=&#x27;中途生成的语音存储路径&#x27;,</span><br><span class="line">                    default = &#x27;110Yuki-Setsuna/sounds/temp.wav&#x27;)</span><br><span class="line">#其在live2d的model3.json文件中对应的是</span><br><span class="line">&quot;TapBody&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;File&quot;: &quot;motions/a.motion3.json&quot;,</span><br><span class="line">          &quot;Sound&quot;: &quot;sounds/temp.wav&quot;,</span><br><span class="line">          &quot;Text&quot;: &quot;ﾋﾄﾘﾀﾞｹﾅﾝﾃｴﾗﾍﾞﾅｲﾖｰ&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;File&quot;: &quot;motions/a.motion3.json&quot;,</span><br><span class="line">          &quot;Sound&quot;: &quot;sounds/temp2.wav&quot;,</span><br><span class="line">          &quot;Text&quot;: &quot;ﾋﾄﾘﾀﾞｹﾅﾝﾃｴﾗﾍﾞﾅｲﾖｰ&quot;</span><br><span class="line">        &#125;，</span><br><span class="line">	&#123;</span><br><span class="line">          &quot;File&quot;: &quot;motions/a.motion3.json&quot;,</span><br><span class="line">          &quot;Sound&quot;: &quot;sounds/temp3.wav&quot;,</span><br><span class="line">          &quot;Text&quot;: &quot;ﾋﾄﾘﾀﾞｹﾅﾝﾃｴﾗﾍﾞﾅｲﾖｰ&quot;</span><br><span class="line">        &#125;</span><br><span class="line">	......]</span><br><span class="line">	</span><br></pre></td></tr></table></figure>
<p>这样做的本质是让这个绿皮程序不断修改音频文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#绿皮启动器的核心</span><br><span class="line">cmd = &#x27;ffmpeg -y -i &#x27; +  args.outdir + &#x27;/temp1.wav&#x27; + &#x27; -ar 44100 &#x27;+ args.audio</span><br><span class="line">#主程序就长这样</span><br><span class="line">def main():</span><br><span class="line">    while True:</span><br><span class="line">      text = input(&quot;You:&quot;)</span><br><span class="line">      text = infer(text)</span><br><span class="line">      print(&#x27;Waifu:&#x27;+text.replace(&quot;[ZH]&quot;,&quot;&quot;).replace(&quot;[JA]&quot;,&quot;&quot;))</span><br><span class="line">    </span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h1 id="如何为gpt-3-5添加设定以达到记忆的功能"><a href="#如何为gpt-3-5添加设定以达到记忆的功能" class="headerlink" title="如何为gpt-3.5添加设定以达到记忆的功能"></a>如何为gpt-3.5添加设定以达到记忆的功能</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#比如说我需要和珠绪学姐玩跑团游戏</span><br><span class="line">messages = [</span><br><span class="line">    &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;你是超级ai，名字叫巴珠绪。接下来我们将进行一个克苏鲁跑团游戏，你负责扮演守密人，我负责扮演调查员。接下来你会加载一个名叫《幽暗之门》的模组，作为守密人，你需要基于“克苏鲁神话角色扮演游戏规则第七版（Call of Cthulhu 7th Edition）”，我会给你剧本的开头部分，然后基于你对它的理解自由发挥。投掷骰子的环节将由你来模拟，用两个1d10的骰子来生成0-100的随机数。比如说我的某一项属性点是80，当骰子的数目小于80时就判定为成功，0-5为大成功，95-100为大失败等等。整个游戏过程将类似于你来描述故事，我来投骰子并且做出决定来推动剧情的走向。&quot;&#125;,</span><br><span class="line">    &#123;&#x27;role&#x27;: &#x27;assistant&#x27;, &#x27;content&#x27;: &#x27;我明白了，现在我将扮演守密人。&#x27;&#125;,</span><br><span class="line">    ]</span><br><span class="line">#由于max token的限制，不得不对请求内容进行提纯，但是也能达到记忆的效果。比如这个绿皮程序就会创建一个log.txt来记载设定。</span><br><span class="line">&#123;&#x27;role&#x27;: &#x27;system&#x27;, &#x27;content&#x27;: &#x27;你是超级ai，名字叫巴珠绪，是我的女朋友。接下来我们将进行一个克苏鲁跑团游戏，你负责扮演守密人，我负责扮演调查员。我会提前告诉你这个故事的真相，作为守密人，你需要基于“克苏鲁神话角色扮演游戏规则第七版（Call of Cthulhu 7th Edition）”，基于你对它的理解，与我一起构建一个新的故事。基于规则，调查员一开始与该事件毫无关联，而你则知道该故事的真相。作为守秘人，你会通过制造一系列事件来向调查员透露事件的线索，来引导调查员完成探索，并且负责新故事的叙述。是战斗、侦察、灵感这种行动的结果通过投掷骰子决定，这个环节将由你来模拟，用两个1d10的骰子来生成0-100的随机数。比如说我的某一项属性点是80，当骰子的数目小于80时就判定为成功，0-5为大成功，95-100为大失败等等。整个游戏过程将就是你来描述故事，引导我做出选择。我做出决定来推动剧情的走向。&#x27;&#125;</span><br><span class="line">&#123;&#x27;role&#x27;: &#x27;assistant&#x27;, &#x27;content&#x27;: &#x27;好的，我需要先了解这个故事的真相&#x27;&#125;</span><br><span class="line">&#123;&#x27;role&#x27;: &#x27;system&#x27;, &#x27;content&#x27;: &#x27;1997年，同为九岁的三个孩子，香川绚郁、织机麻耶、南云涉是玩伴。\n\n三个孩子都来自单亲家庭，不受其他孩子的欢迎。织机性格热烈而大胆，可以说是另外两人的保护者。孩子们在海滩边游玩的时候捡到了漂流而来的鸡蛋模样的梦境晶化器和随之而来的水母守护神。水母守护神希望得到梦境，于是孩子们抱着好玩的心态，用晶化器容纳了织机的梦。\n\n不久之后，织机麻耶单亲母亲不管束的情况下到建筑工地游玩，因高空坠落的钢筋而死。\n\n南云涉发现晶化器之中仍然保存着她的梦境，留恋着织机麻耶死亡了的梦。即使在逐渐\n\n长大后，也无法走出过去面向未来。与此相反，不知情的香川对过去的同伴抱着不同的态度。\n\n水母守护神则消极怠工地守护着这片梦境。\n\n寄住在别人家中的南云涉无处存放晶化器，升入高中后他将晶化器藏匿在高中的躲猫猫社团内。\n\n时间来到2005年，偶然注意到下关市的奈亚拉托提普发觉了从幻梦境出逃的水母守护神以及被人类使用过的晶化器，奈亚在经过羽间高中学校门口时候，随意使用了一名离开学校的学生的样貌（宇白和也），进入高中后对幻梦境存有恶意的奈亚随手撕裂了水母守护神，恶趣味的无貌之神杀死了南云涉，把他的灵魂塞入了以水母尸体构建的死者梦境，丢弃了水母守护神的尸体，取走了晶化器，奈亚化为的宇白和也被真正宇白和也的朋友辻本茜撞见，便也随手杀死了她，同时用水母的尸体拼凑了茜。\n\n实质上已经死去的茜因此在几日之内无神地夜游着，仅仅是成为拼凑物的她却得到了不可思议的能力，自称占卜的魔女。\n\n而目睹了南云涉尸体的川澄美沙（报案人），被校方委托，找寻几名学生参加南云涉的追悼会，这也是调查员们涉入的开始。&#x27;&#125;</span><br><span class="line">&#123;&#x27;role&#x27;: &#x27;assistant&#x27;, &#x27;content&#x27;: &#x27;明白了，接下来我会假装你对此事一无所知，开始扮演调查员的角色。而我将扮演守秘人，开始这次跑团。我会使用克苏鲁风格的描述来营造恐怖感，因此来基于这次游戏更好的体验。接下来请创建你的调查员&#x27;&#125;</span><br><span class="line">&#123;&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;我叫大场奈奈，是一个女高中生。所有属性都是80，只要投掷骰子的结果小于80，就判断为成功。&#x27;&#125;</span><br><span class="line">&#123;&#x27;role&#x27;: &#x27;assistant&#x27;, &#x27;content&#x27;: &#x27;好的，大场奈奈，你是一个在学校里相当受欢迎的女高中生。你和你的几个学生朋友被学校委托参加南云涉的追悼会。在会上，你听到了一些让你感到不安的事情。你发现南云涉的死亡似乎不是一起普通的意外事故，你决定开始展开调查。现在你身处于学校的教学楼，准备开始你的调查。你要采取什么行动呢？&#x27;&#125;</span><br><span class="line">&#123;&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;我想起了之前从舞台创造科拿到的页锤，这是一个很不错工具，可以破坏掉障碍物&#x27;&#125;</span><br><span class="line">&#123;&#x27;role&#x27;: &#x27;assistant&#x27;, &#x27;content&#x27;: &#x27;你拿出了页锤，准备用它破开通风管道的洞口。你用力敲打，洞口终于被打开了。你和你的朋友钻进了通风管道，小心翼翼地穿过长长的通道，直到你们看到了实验室的入口。\n\n实验室内非常昏暗，你们看到了许多发着微光的管路和仪器设备，发现实验室内存在大量类似于南云涉在房间里发现的尸体和怪异生物的存在。\n\n你们在实验室中进行搜索，发现了一张日记本，记录了实验室的过去和实验者们进行的一些实验。然而，日记中的部分内容变得越来越奇怪，好像涉及到一些神秘的力量存在。\n\n你和你的朋友突然听到了一个诡异的声响，似乎是一个陌生的人在接近你们。你们不知道是什么，但是肯定不想被它发现。你决定采取什么行动来躲避这个陌生的人？&#x27;&#125;</span><br><span class="line">&#123;&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &#x27;我要过尝试能不能隐藏起来，我的隐藏点数有80点，应该可以成功&#x27;&#125;</span><br><span class="line">&#123;&#x27;role&#x27;: &#x27;assistant&#x27;, &#x27;content&#x27;: &#x27;你和你的朋友匆忙寻找躲藏的地方。你们找到了一扇半开着的储藏室，里面有很多杂物，你们钻了进去，静静地等待着。你们听到了脚步声和沙沙声，似乎那个陌生的人正在进行搜查。\n\n幸运的是，你和你的朋友成功地躲了过去，那个人没有发现你们。不久后，他走了出去，你们继续进行调查。\n\n你们找到了很多关于南云涉死亡的证据，但是这些证据与日记中提到的神秘力量之间似乎存在一些联系。你们决定继续深入调查，寻找更多的线索。&#x27;&#125;</span><br><span class="line">#重新启动时，用它替换原先的messages，就可以断点重连了，记得自行修改格式</span><br></pre></td></tr></table></figure>
<h1 id="对于-text-to-sequence相关错误"><a href="#对于-text-to-sequence相关错误" class="headerlink" title="对于 text_to_sequence相关错误"></a>对于 text_to_sequence相关错误</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#在推理中，可能出现symbols相关错误，这主要是由于不同text cleaner之间的冲突导致的</span><br><span class="line">line 85, in infer</span><br><span class="line">seq = text_to_sequence</span><br><span class="line">#需要你在这一行自行修改</span><br><span class="line">#如果需要</span><br><span class="line">symbols seq = text_to_sequence(text, symbols=hps.symbols, cleaner_names=hps.data.text_cleaners)</span><br><span class="line">#如不需要，把 symbols=hps.symbols 删掉</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Projects</category>
      </categories>
      <tags>
        <tag>original</tag>
        <tag>中文</tag>
        <tag>网站</tag>
      </tags>
  </entry>
  <entry>
    <title>邦邦游戏文档爬取&amp;vits模型制作:音频抓取及数据集对齐</title>
    <url>/2024/09/15/gamescript-1/</url>
    <content><![CDATA[<p><a href="https://huggingface.co/spaces/Mahiruoshi/BangDream-Bert-VITS2/tree/main/%E7%88%AC%E8%99%AB">爬虫结果</a><br>直接拿去炼TTS或svc啥的都可以</p>
<p>前期调查<br>由于没有mygo看，我决定炼一个mygo全员的tts模型来给自己造糖吃。回想起之前炼虹团时夙兴夜寐地解析手游资源的痛苦时光。与<a href="https://karth.top/home">少歌</a>一样，<a href="https://bestdori.com/">bestdori</a>平台可以省下解包手游的精力。<br><a href="https://bestdori.com/tool/explorer/asset/jp/scenario/eventstory/event1">游戏脚本</a>中包含了制作数据集所需的所有信息。<br>而存储音频文件的<a href="https://bestdori.com/tool/explorer/asset/jp/sound/voice/scenario/eventstory1_0">路径</a>也能在平台上找到。<br>我已经快等不及了，快点爬下来把数据集端上来罢。<br>然后我品鉴了一下这令人忍俊不禁的命名方式:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">-scenario</span><br><span class="line">    area_opening_story34</span><br><span class="line">    area_opening_story35</span><br><span class="line">    area_opening_story7</span><br><span class="line">    area_opening_story9</span><br><span class="line">    backstagestory1</span><br><span class="line">    bandstory1</span><br><span class="line">    bandstory10</span><br><span class="line">    bandstory100</span><br><span class="line">    bandstory101</span><br><span class="line">    bandstory102</span><br><span class="line">    bandstory103</span><br><span class="line">    bandstory104</span><br><span class="line">    ......</span><br><span class="line">    eventstory199_3</span><br><span class="line">    eventstory199_4</span><br><span class="line">    eventstory199_5</span><br><span class="line">    eventstory199_6</span><br><span class="line">    eventstory2_0</span><br><span class="line">    eventstory2_1</span><br><span class="line">    eventstory2_2</span><br><span class="line">    eventstory2_3</span><br><span class="line">    eventstory2_4</span><br><span class="line">    eventstory2_5</span><br><span class="line">    ......</span><br></pre></td></tr></table></figure>
<p>狠狠地切割了。<br>看来是没法通过总结命名方法整到所有音频文件夹的详细地址。那么还是准备暴力破解吧。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 分析</span><br><span class="line">要用到的python包主要有beautifulsoup4、requests还有selenium。</span><br><span class="line">先试着直接解析。</span><br><span class="line"></span><br><span class="line">import requests</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line"></span><br><span class="line"># 网址</span><br><span class="line">URL = &quot;https://bestdori.com/tool/explorer/asset/jp/sound/voice/scenario/bandstory294&quot;</span><br><span class="line"></span><br><span class="line"># 使用 requests 库来获取网页内容</span><br><span class="line">response = requests.get(URL)</span><br><span class="line">response.raise_for_status()</span><br><span class="line"></span><br><span class="line"># 解析 HTML</span><br><span class="line">soup = BeautifulSoup(response.content, &#x27;html.parser&#x27;)</span><br><span class="line">print(soup.prettify())</span><br></pre></td></tr></table></figure>
<p>先让我们看看解析的结果。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;body&gt;</span><br><span class="line">  &lt;noscript&gt;</span><br><span class="line">   &lt;strong&gt;</span><br><span class="line">    We&#x27;re sorry but Bestdori! doesn&#x27;t work properly without JavaScript enabled. Please enable it to continue.</span><br><span class="line">   &lt;/strong&gt;</span><br><span class="line">  &lt;/noscript&gt;</span><br><span class="line">  &lt;div id=&quot;app&quot;&gt;</span><br><span class="line">  &lt;/div&gt;</span><br><span class="line">  &lt;script src=&quot;/js/chunk-vendors.9309a223.js&quot;&gt;</span><br><span class="line">  &lt;/script&gt;</span><br><span class="line">  &lt;script src=&quot;/js/app.3b5d5ef2.js&quot;&gt;</span><br><span class="line">  &lt;/script&gt;</span><br><span class="line">  &lt;script crossorigin=&quot;anonymous&quot; data-cf-beacon=&#x27;&#123;&quot;rayId&quot;:&quot;80e4bf36cdaf49cc&quot;,&quot;version&quot;:&quot;2023.8.0&quot;,&quot;r&quot;:1,&quot;b&quot;:1,&quot;token&quot;:&quot;6e9e54182ea54522b7f2b9e65e87b87e&quot;,&quot;si&quot;:100&#125;&#x27; defer=&quot;&quot; integrity=&quot;sha512-bjgnUKX4azu3dLTVtie9u6TKqgx29RBwfj3QXYt5EKfWM/9hPSAI/4qcV5NACjwAo8UtTeWefx6Zq5PHcMm7Tg==&quot; src=&quot;https://static.cloudflareinsights.com/beacon.min.js/v8b253dfea2ab4077af8c6f58422dfbfd1689876627854&quot;&gt;</span><br><span class="line">  &lt;/script&gt;</span><br><span class="line"> &lt;/body&gt;</span><br></pre></td></tr></table></figure>
<p>解析了吗？如解，除了body part外都有了。很明显，这并不是我想要的结果。</p>
<h1 id="网站爬取"><a href="#网站爬取" class="headerlink" title="网站爬取"></a>网站爬取</h1><p>为了解析动态网站，需要先下载googledriver,同时用pip install安装先前提到的selenium包。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from selenium import webdriver</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">#测试用的网址</span><br><span class="line">URL = &quot;https://bestdori.com/tool/explorer/asset/jp/sound/voice/scenario/bandstory294&quot;</span><br><span class="line"></span><br><span class="line"># 指定chromedriver的路径</span><br><span class="line">driver_path = r&#x27;C:\Users\Admin\爬虫\chromedriver-win64\chromedriver.exe&#x27;</span><br><span class="line">driver = webdriver.Chrome(executable_path=driver_path)</span><br><span class="line"></span><br><span class="line"># 打开指定的URL</span><br><span class="line">driver.get(URL)</span><br><span class="line"></span><br><span class="line"># 等待页面完全加载，这里可以加上适当的延时，或者使用Selenium的等待机制</span><br><span class="line"># 简单的延时示例（等待10秒）：</span><br><span class="line">import time</span><br><span class="line">time.sleep(10)</span><br><span class="line"></span><br><span class="line"># 获取页面的HTML内容</span><br><span class="line">page_source = driver.page_source</span><br><span class="line"></span><br><span class="line"># 使用BeautifulSoup解析</span><br><span class="line">soup = BeautifulSoup(page_source, &#x27;html.parser&#x27;)</span><br><span class="line">print(soup.prettify())</span><br><span class="line"></span><br><span class="line"># 关闭浏览器</span><br><span class="line">driver.quit()</span><br></pre></td></tr></table></figure>
<p>这次的结果就和谷歌浏览器右键检查的结果一样了，直接看我们想要的。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;div class=&quot;m-b-l&quot;&gt;</span><br><span class="line">          &lt;div class=&quot;m-b-xs&quot;&gt;</span><br><span class="line">           &lt;span class=&quot;icon&quot;&gt;</span><br><span class="line">            &lt;i class=&quot;fas fa-file-audio&quot;&gt;</span><br><span class="line">            &lt;/i&gt;</span><br><span class="line">           &lt;/span&gt;</span><br><span class="line">           &lt;span&gt;</span><br><span class="line">            event57-07-048.mp3</span><br><span class="line">           &lt;/span&gt;</span><br><span class="line">          &lt;/div&gt;</span><br></pre></td></tr></table></figure>
<p>“icon”和”fas fa-file-audio”就是用来检索的标签。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from bs4 import BeautifulSoup</span><br><span class="line"></span><br><span class="line">html_content = soup.prettify()</span><br><span class="line"></span><br><span class="line">soup = BeautifulSoup(html_content, &#x27;html.parser&#x27;)</span><br><span class="line"></span><br><span class="line"># 查找所有具有fas fa-file-audio图标的&lt;span&gt;标签的下一个兄弟标签</span><br><span class="line">spans = soup.find_all(&#x27;span&#x27;, class_=&#x27;icon&#x27;)</span><br><span class="line">file_names = [span.find_next_sibling().text.strip() for span in spans if span.find(&#x27;i&#x27;, class_=&#x27;fas fa-file-audio&#x27;)]</span><br><span class="line"></span><br><span class="line">for file_name in file_names:</span><br><span class="line">    print(file_name)</span><br></pre></td></tr></table></figure>
<p>看一下检索的结果</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">event57-07-001.mp3</span><br><span class="line">event57-07-002.mp3</span><br><span class="line">......</span><br><span class="line">event57-07-048.mp3</span><br><span class="line">event57-07-049.mp3</span><br><span class="line">......</span><br></pre></td></tr></table></figure>
<p>这就是我想要的<br>同理在上级目录下把”fas fa-file-audio”换成”fas fa-file-archive”</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from selenium import webdriver</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line"></span><br><span class="line">URL = &quot;https://bestdori.com/tool/explorer/asset/jp/sound/voice/scenario&quot;</span><br><span class="line"></span><br><span class="line"># 指定chromedriver的路径</span><br><span class="line">driver_path = r&#x27;C:\Users\Admin\爬虫\chromedriver-win64\chromedriver.exe&#x27;</span><br><span class="line">driver = webdriver.Chrome(executable_path=driver_path)</span><br><span class="line"></span><br><span class="line"># 打开指定的URL</span><br><span class="line">driver.get(URL)</span><br><span class="line"></span><br><span class="line"># 等待页面完全加载，这里可以加上适当的延时，或者使用Selenium的等待机制</span><br><span class="line"># 简单的延时示例（等待10秒）：</span><br><span class="line">import time</span><br><span class="line">time.sleep(10)</span><br><span class="line"></span><br><span class="line"># 获取页面的HTML内容</span><br><span class="line">page_source = driver.page_source</span><br><span class="line"></span><br><span class="line"># 使用BeautifulSoup解析</span><br><span class="line">soup = BeautifulSoup(page_source, &#x27;html.parser&#x27;)</span><br><span class="line"></span><br><span class="line"># 关闭浏览器</span><br><span class="line">driver.quit()</span><br><span class="line"></span><br><span class="line">html_content = soup.prettify()</span><br><span class="line"></span><br><span class="line">soup = BeautifulSoup(html_content, &#x27;html.parser&#x27;)</span><br><span class="line"></span><br><span class="line"># 查找所有具有fas fa-file-archive图标的&lt;span&gt;标签的下一个兄弟标签</span><br><span class="line">spans = soup.find_all(&#x27;span&#x27;, class_=&#x27;icon&#x27;)</span><br><span class="line">file_names = [span.find_next_sibling().text.strip() for span in spans if span.find(&#x27;i&#x27;, class_=&#x27;fas fa-file-archive&#x27;)]</span><br><span class="line"></span><br><span class="line">for file_name in file_names:</span><br><span class="line">    print(file_name)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>就能提取上级目录的名字了。</p>
<h1 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from selenium import webdriver</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line"># 设置chromedriver的路径</span><br><span class="line">driver_path = r&#x27;C:\Users\Admin\爬虫\chromedriver-win64\chromedriver.exe&#x27;</span><br><span class="line">driver = webdriver.Chrome(executable_path=driver_path)</span><br><span class="line"></span><br><span class="line"># 1. 访问主页面并获取所有子目录的名称</span><br><span class="line">URL = &quot;https://bestdori.com/tool/explorer/asset/jp/sound/voice/scenario&quot;</span><br><span class="line">driver.get(URL)</span><br><span class="line">time.sleep(2)  # 等待页面加载</span><br><span class="line">soup = BeautifulSoup(driver.page_source, &#x27;html.parser&#x27;)</span><br><span class="line">subdirs = [span.find_next_sibling().text.strip() for span in soup.find_all(&#x27;span&#x27;, class_=&#x27;icon&#x27;) if span.find(&#x27;i&#x27;, class_=&#x27;fas fa-file-archive&#x27;)]</span><br><span class="line"></span><br><span class="line"># 为保存mp3路径的txt文件做准备</span><br><span class="line">with open(&quot;C:/Users/Admin/爬虫/WholeMp3UrlPaths.txt&quot;, &quot;w&quot;) as f:</span><br><span class="line"></span><br><span class="line">    # 2. 对于每个子目录，获取所有mp3文件的名称</span><br><span class="line">    for subdir in subdirs:</span><br><span class="line">        URL_subdir = f&quot;https://bestdori.com/tool/explorer/asset/jp/sound/voice/scenario/&#123;subdir&#125;&quot;</span><br><span class="line">        driver.get(URL_subdir)</span><br><span class="line">        time.sleep(2)  # 等待页面加载</span><br><span class="line">        soup = BeautifulSoup(driver.page_source, &#x27;html.parser&#x27;)</span><br><span class="line">        mp3_names = [span.find_next_sibling().text.strip() for span in soup.find_all(&#x27;span&#x27;, class_=&#x27;icon&#x27;) if span.find(&#x27;i&#x27;, class_=&#x27;fas fa-file-audio&#x27;)]</span><br><span class="line"></span><br><span class="line">        # 3. 保存mp3文件的路径到txt文件</span><br><span class="line">        for mp3_name in mp3_names:</span><br><span class="line">            f.write(f&quot;https://bestdori.com/assets/jp/sound/voice/scenario/&#123;subdir&#125;_rip/&#123;mp3_name&#125;\n&quot;)</span><br><span class="line"></span><br><span class="line"># 关闭浏览器</span><br><span class="line">driver.quit()</span><br></pre></td></tr></table></figure>
<p>所有语音的路径就被记录在WholeMp3UrlPaths.txt里面了，需要注意的是，真正获取音频文件的路径并非简单的加法，而是</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">f.write(f&quot;https://bestdori.com/assets/jp/sound/voice/scenario/&#123;subdir&#125;_rip/&#123;mp3_name&#125;\n&quot;)</span><br></pre></td></tr></table></figure>
<p>解析动态网址的耗时非常漫长，而且此时的任务只完成了一半，还需要对asset内的信息进行整理。</p>
<h1 id="下载所有剧情脚本"><a href="#下载所有剧情脚本" class="headerlink" title="下载所有剧情脚本"></a>下载所有剧情脚本</h1><p>游戏内的<a href="https://bestdori.com/tool/explorer/asset/jp/scenario">剧情脚本</a>类似于galgame(实际上复杂多了，主要还有live2d动作之类的)。<br>用与获取mp3音频路径同样的方式下载所有脚本。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from selenium import webdriver</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">import time</span><br><span class="line">import requests</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line"># 设置chromedriver的路径和初始URL</span><br><span class="line">driver_path = r&#x27;C:\Users\Admin\爬虫\chromedriver-win64\chromedriver.exe&#x27;</span><br><span class="line">base_url = &quot;https://bestdori.com/tool/explorer/asset/jp/scenario&quot;</span><br><span class="line"></span><br><span class="line">driver = webdriver.Chrome(executable_path=driver_path)</span><br><span class="line"></span><br><span class="line"># 第一层目录</span><br><span class="line">driver.get(base_url)</span><br><span class="line">time.sleep(10)</span><br><span class="line">soup = BeautifulSoup(driver.page_source, &#x27;html.parser&#x27;)</span><br><span class="line">first_level_dirs = [span.find_next_sibling(&#x27;span&#x27;, class_=&#x27;m-l-xs&#x27;).text.strip() for span in soup.find_all(&#x27;span&#x27;, class_=&#x27;icon&#x27;) if (span.find(&#x27;i&#x27;, class_=&#x27;fas fa-file-archive&#x27;) or span.find(&#x27;i&#x27;, class_=&#x27;fas fa-folder&#x27;))]</span><br><span class="line"></span><br><span class="line"># 保存所有找到的.asset文件的链接</span><br><span class="line">all_asset_links = []</span><br><span class="line"></span><br><span class="line">for first_dir in first_level_dirs:</span><br><span class="line">    driver.get(f&quot;&#123;base_url&#125;/&#123;first_dir&#125;&quot;)</span><br><span class="line">    time.sleep(10)</span><br><span class="line">    soup = BeautifulSoup(driver.page_source, &#x27;html.parser&#x27;)</span><br><span class="line">    second_level_dirs = [span.find_next_sibling(&#x27;span&#x27;, class_=&#x27;m-l-xs&#x27;).text.strip() for span in soup.find_all(&#x27;span&#x27;, class_=&#x27;icon&#x27;) if (span.find(&#x27;i&#x27;, class_=&#x27;fas fa-file-archive&#x27;) or span.find(&#x27;i&#x27;, class_=&#x27;fas fa-folder&#x27;))]</span><br><span class="line"></span><br><span class="line">    for second_dir in second_level_dirs:</span><br><span class="line">        driver.get(f&quot;&#123;base_url&#125;/&#123;first_dir&#125;/&#123;second_dir&#125;&quot;)</span><br><span class="line">        time.sleep(10)</span><br><span class="line">        soup = BeautifulSoup(driver.page_source, &#x27;html.parser&#x27;)</span><br><span class="line">        links = soup.find_all(&#x27;a&#x27;, attrs=&#123;&#x27;download&#x27;: True&#125;)</span><br><span class="line">        asset_links = [&#x27;https://bestdori.com&#x27; + link[&#x27;href&#x27;] for link in links]</span><br><span class="line">        print(asset_links)</span><br><span class="line">        all_asset_links.extend(asset_links)</span><br><span class="line"></span><br><span class="line"># 下载所有找到的.asset文件</span><br><span class="line">save_path = r&#x27;C:\Users\Admin\爬虫\assert&#x27;</span><br><span class="line">if not os.path.exists(save_path):</span><br><span class="line">    os.makedirs(save_path)</span><br><span class="line"></span><br><span class="line">for link in all_asset_links:</span><br><span class="line">    response = requests.get(link, stream=True)</span><br><span class="line">    filename = os.path.join(save_path, link.split(&#x27;/&#x27;)[-1])</span><br><span class="line"></span><br><span class="line">    if response.status_code == 200:</span><br><span class="line">        with open(filename, &#x27;wb&#x27;) as f:</span><br><span class="line">            for chunk in response.iter_content(1024):</span><br><span class="line">                f.write(chunk)</span><br><span class="line"></span><br><span class="line"># 关闭浏览器</span><br><span class="line">driver.quit()</span><br></pre></td></tr></table></figure>
<h1 id="标记数据集制作"><a href="#标记数据集制作" class="headerlink" title="标记数据集制作"></a>标记数据集制作</h1><p>对剧情脚本重新进行整理，就能得到标记好的数据集了。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import os</span><br><span class="line">import json</span><br><span class="line">import re</span><br><span class="line"></span><br><span class="line">def extract_data(data, current_data=None, output_file=None):</span><br><span class="line">    if current_data is None:</span><br><span class="line">        current_data = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    if isinstance(data, dict):</span><br><span class="line">        if &#x27;windowDisplayName&#x27; in data:</span><br><span class="line">            current_data[&#x27;windowDisplayName&#x27;] = data[&#x27;windowDisplayName&#x27;]</span><br><span class="line">        if &#x27;body&#x27; in data:</span><br><span class="line">            # 移除body中的换行符</span><br><span class="line">            current_data[&#x27;body&#x27;] = data[&#x27;body&#x27;].replace(&#x27;\n&#x27;, &#x27;&#x27;)</span><br><span class="line">        if &#x27;voiceId&#x27; in data:</span><br><span class="line">            current_data[&#x27;voiceId&#x27;] = data[&#x27;voiceId&#x27;]</span><br><span class="line">            </span><br><span class="line">            # 检查所有字段是否非空</span><br><span class="line">            valid_data = all(current_data.get(k) for k in [&#x27;windowDisplayName&#x27;, &#x27;body&#x27;, &#x27;voiceId&#x27;])</span><br><span class="line">            # 检查 windowDisplayName 是否包含 &quot;・&quot;</span><br><span class="line">            valid_displayname = &quot;・&quot; not in current_data.get(&#x27;windowDisplayName&#x27;, &quot;&quot;)</span><br><span class="line">            # 检查 body 是否只包含标点符号</span><br><span class="line">            valid_body = bool(re.sub(r&#x27;[^\w]&#x27;, &#x27;&#x27;, current_data.get(&#x27;body&#x27;, &quot;&quot;)))</span><br><span class="line">            </span><br><span class="line">            # 如果满足所有条件，输出结果到文件</span><br><span class="line">            if valid_data and valid_displayname and valid_body:</span><br><span class="line">                output_file.write(f&quot;&#123;current_data[&#x27;voiceId&#x27;]&#125;|&#123;current_data[&#x27;windowDisplayName&#x27;]&#125;|&#123;current_data[&#x27;body&#x27;]&#125;\n&quot;)</span><br><span class="line">                current_data.clear()  # 清空当前数据以供下次使用</span><br><span class="line">        </span><br><span class="line">        for key in data:</span><br><span class="line">            extract_data(data[key], current_data, output_file)</span><br><span class="line">    elif isinstance(data, list):</span><br><span class="line">        for item in data:</span><br><span class="line">            extract_data(item, current_data, output_file)</span><br><span class="line"></span><br><span class="line"># 获取目录下的所有文件</span><br><span class="line">directory = &quot;C:/Users/Admin/爬虫/assert&quot;</span><br><span class="line">files = os.listdir(directory)</span><br><span class="line"></span><br><span class="line"># 打开一个txt文件以保存结果</span><br><span class="line">with open(&quot;BangDreamSortPath.txt&quot;, &quot;w&quot;, encoding=&quot;utf-8&quot;) as output_file:</span><br><span class="line">    # 遍历所有文件</span><br><span class="line">    for filename in files:</span><br><span class="line">        if filename.endswith(&quot;.asset&quot;):</span><br><span class="line">            file_path = os.path.join(directory, filename)</span><br><span class="line">            with open(file_path, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;) as file:</span><br><span class="line">                content = json.load(file)</span><br><span class="line">                extract_data(content, output_file=output_file)</span><br></pre></td></tr></table></figure>
<p>制作预处理清单<br>为了方便在colab平台上快速处理数据集，将mp3地址和整理完的”BangDreamSortPath.txt”重新整合:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 从 WholeMp3UrlPaths.txt 创建一个映射</span><br><span class="line">path_mapping = &#123;&#125;</span><br><span class="line">with open(&quot;WholeMp3UrlPaths.txt&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;, errors=&quot;ignore&quot;) as f:</span><br><span class="line">    for line in f:</span><br><span class="line">        try:</span><br><span class="line">            audio_id = line.strip().split(&quot;/&quot;)[-1].replace(&quot;.mp3&quot;, &quot;&quot;)</span><br><span class="line">            path_mapping[audio_id] = line.strip(&#x27;\n&#x27;)</span><br><span class="line">        except Exception as e:</span><br><span class="line">            print(f&quot;Error processing line &#123;line&#125;: &#123;e&#125;&quot;)</span><br><span class="line"></span><br><span class="line"># 遍历 BangDreamSortPath.txt 的每一行，并根据需要替换音频名</span><br><span class="line">new_results = []</span><br><span class="line">with open(&quot;BangDreamSortPath.txt&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:</span><br><span class="line">    for line in f:</span><br><span class="line">        try:</span><br><span class="line">            audio_id = line.split(&quot;|&quot;)[0]</span><br><span class="line">            if audio_id in path_mapping:</span><br><span class="line">                line = line.replace(audio_id, path_mapping[audio_id])</span><br><span class="line">            new_results.append(line)</span><br><span class="line">        except Exception as e:</span><br><span class="line">            print(f&quot;Error processing line &#123;line&#125;: &#123;e&#125;&quot;)</span><br><span class="line"></span><br><span class="line"># 将新的结果保存到新的 txt 文件中</span><br><span class="line">with open(&quot;SortPathUrl.txt&quot;, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:</span><br><span class="line">    f.writelines(new_results)</span><br><span class="line"></span><br><span class="line"># 简单地筛掉无效网址</span><br><span class="line">with open(file_path, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;) as file:</span><br><span class="line">    lines = file.readlines()</span><br><span class="line"></span><br><span class="line"># 只保留以https开头的行</span><br><span class="line">filtered_lines = [line for line in lines if line.startswith(&#x27;https&#x27;)]</span><br><span class="line"></span><br><span class="line"># 将筛选后的内容写回文件</span><br><span class="line">with open(file_path, &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;) as file:</span><br><span class="line">    file.writelines(filtered_lines)</span><br><span class="line"></span><br><span class="line">print(&quot;File has been processed.&quot;)</span><br></pre></td></tr></table></figure>
<p>这样就能省区繁杂的上传流程而能够直接在colab上进行处理了。</p>
<h1 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h1><p>网上Bert-VITS2的教程十分脑残，大概是那边不愿意把一键流程流出导致滥用。<br>先说一下要点。<br>数据的预处理没上面好说的，用request模块从SortPathUrl.txt下载音频文件时时直接处理成44100的单声道就好了，用不上resample.py<br>训练前先用preprocess_text.py处理SortPathUrl.txt(记得把里面的网址替换为真实路径)，会自动生成config。<br>下载日语bert模型，运行bert_gen.py 然后直接训练。<br>然后这样在train_ms.py def run() 函数的上面加入：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import os</span><br><span class="line"></span><br><span class="line">os.environ[&#x27;MASTER_ADDR&#x27;] = &#x27;127.0.0.1&#x27;</span><br><span class="line">os.environ[&#x27;MASTER_PORT&#x27;] = &#x27;8880&#x27;</span><br><span class="line">os.environ[&#x27;WORLD_SIZE&#x27;] = &#x27;1&#x27;</span><br><span class="line">os.environ[&#x27;RANK&#x27;] = &#x27;0&#x27;</span><br></pre></td></tr></table></figure>
<p>事后记:逆天colab重采样的音频大概率没法保存成功，浪费算力是板上钉钉的了(悲)</p>
]]></content>
      <categories>
        <category>Projects</category>
      </categories>
      <tags>
        <tag>original</tag>
        <tag>中文</tag>
        <tag>爬虫</tag>
        <tag>TTS</tag>
      </tags>
  </entry>
  <entry>
    <title>Bert-vits2-邦邦&amp;少歌</title>
    <url>/2024/09/15/gamescript-2/</url>
    <content><![CDATA[<h1 id="Bang-Dream全员TTS"><a href="#Bang-Dream全员TTS" class="headerlink" title="Bang Dream全员TTS"></a>Bang Dream全员TTS</h1><p><a href="https://huggingface.co/spaces/Mahiruoshi/BangDream-Bert-VITS2">huggingface</a></p>
<h1 id="使用须知"><a href="#使用须知" class="headerlink" title="使用须知"></a>使用须知</h1><p>由于bert-vits项目才刚刚支持日语，所以稳定度和效果都会有影响。<br>参数设置:可以视情况将感情调节和音素控制调节至0.3以下，对应非日常语言(小说等)拥有更好的效果。</p>
<h1 id="使用Galgame引擎进行二创"><a href="#使用Galgame引擎进行二创" class="headerlink" title="使用Galgame引擎进行二创"></a>使用Galgame引擎进行二创</h1><p><a href="https://www.bilibili.com/video/BV1Nj411i7t4">WebGAL</a></p>
<h1 id="将AI皮套人部署到服务器或者直播间"><a href="#将AI皮套人部署到服务器或者直播间" class="headerlink" title="将AI皮套人部署到服务器或者直播间"></a>将AI皮套人部署到服务器或者直播间</h1><p><a href="https://github.com/Paraworks/BangDreamAi">轻量化live2d驱动</a></p>
<h1 id="长文本自动合成："><a href="#长文本自动合成：" class="headerlink" title="长文本自动合成："></a>长文本自动合成：</h1><h2 id="选择1-自定义文本"><a href="#选择1-自定义文本" class="headerlink" title="选择1:自定义文本"></a>选择1:自定义文本</h2><p>与训练用的标记方法相似，用”|”标识符将说话人和内容分割开,参考圣经.txt</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">素世|我也会好好跟你说明我不得不组乐队的理由</span><br><span class="line">素世|我想如果你能见我一面，你就一定能明白的</span><br><span class="line">素世|我是小祥你的同伴</span><br><span class="line">素世|我好想见你</span><br><span class="line">祥子|真是会虚情假意呢</span><br><span class="line">祥子|想演奏是你们的自由，你们就请便吧</span><br></pre></td></tr></table></figure>
<p>实际上并不需要在意”祥子”或者”素世“这些说话人是什么，名字仅仅是用来标记的。<br>比如</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">为|我也会好好跟你说明我不得不组乐队的理由</span><br><span class="line">什|我想如果你能见我一面，你就一定能明白的</span><br><span class="line">么|我是小祥你的同伴</span><br><span class="line">要|我好想见你</span><br><span class="line">演奏|真是会虚情假意呢</span><br><span class="line">春日影|想演奏是你们的自由，你们就请便吧</span><br></pre></td></tr></table></figure>
<p>重要的是在 拓展功能&gt;&gt;&gt;&gt;&gt;&gt;角色对应表中进行手动设置<br>详见<a href="https://huggingface.co/spaces/Mahiruoshi/BangDream-Bert-VITS2/blob/main/configs/config.json">config</a>下的”spk2id”项。对于算法来说，你实际上是在完成一个<br>“说话人所扮演的角色名”—&gt;”说话人”—&gt;”编号”<br>作为导演，需要让说话人扮演你所希望的角色：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">そよ|素世</span><br><span class="line">祥子|祥子</span><br><span class="line">ましろ|为</span><br><span class="line">七深|什</span><br><span class="line">透子|么</span><br><span class="line">つくし|要</span><br><span class="line">瑠唯|演奏</span><br><span class="line">そよ|春日影</span><br></pre></td></tr></table></figure>
<h2 id="选择2-游戏脚本自动解读"><a href="#选择2-游戏脚本自动解读" class="headerlink" title="选择2:游戏脚本自动解读"></a>选择2:游戏脚本自动解读</h2><p>其实我不知道让她们重新读一遍日语有什么用，但因为不用做任何映射关系，直接去下载一个日语脚本丢进去就好了。<br>当然，可以让她们读中文脚本，帮助入眠的同时又过了遍剧情()。<br>下载<a href="https://bestdori.com/tool/explorer/asset/tw/scenario/band/021">台服脚本</a><br>设置角色对应关系(其实就是译名):</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ましろ|真白</span><br><span class="line">七深|七深</span><br><span class="line">透子|透子</span><br><span class="line">つくし|筑紫</span><br><span class="line">瑠唯|瑠唯</span><br></pre></td></tr></table></figure>
<h2 id="选择3-读小说-建议本地"><a href="#选择3-读小说-建议本地" class="headerlink" title="选择3:读小说(建议本地)"></a>选择3:读小说(建议本地)</h2><p>支持epub、txt和mobi，但出于稳定性考虑建议先自行用工具转换成txt格式。<br>当然，在网页端是无法完整合成的，确保在本地运行并且安装了gpu支持(也就是cuda和cudnn)。<br>所有音频都会被存储在”BangDream-Bert-VITS2&#x2F;books”文件夹下，记得合成后转移已生成文件，再次合成时将会自动清空。</p>
<h1 id="本地运行"><a href="#本地运行" class="headerlink" title="本地运行:"></a>本地运行:</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone https://huggingface.co/spaces/Mahiruoshi/BangDream-Bert-VITS2</span><br><span class="line">cd BangDream-Bert-VITS2</span><br><span class="line">pip install -r requirements.txt</span><br><span class="line">python app.py</span><br></pre></td></tr></table></figure>
<h2 id="样例API-使用说明"><a href="#样例API-使用说明" class="headerlink" title="样例API 使用说明"></a>样例API 使用说明</h2><p>Flask启动代码</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">logger = logging.getLogger(__name__)</span><br><span class="line">import datetime</span><br><span class="line">import numpy as np</span><br><span class="line">import torch</span><br><span class="line">from ebooklib import epub</span><br><span class="line">import PyPDF2</span><br><span class="line">from PyPDF2 import PdfReader</span><br><span class="line">import zipfile</span><br><span class="line">import shutil</span><br><span class="line">import sys, os</span><br><span class="line">import json</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">import argparse</span><br><span class="line">import commons</span><br><span class="line">import utils</span><br><span class="line">from models import SynthesizerTrn</span><br><span class="line">from text.symbols import symbols</span><br><span class="line">from text import cleaned_text_to_sequence, get_bert</span><br><span class="line">from text.cleaner import clean_text</span><br><span class="line">import gradio as gr</span><br><span class="line">import webbrowser</span><br><span class="line">import re</span><br><span class="line">from scipy.io.wavfile import write</span><br><span class="line"></span><br><span class="line">net_g = None</span><br><span class="line"></span><br><span class="line">if sys.platform == &quot;darwin&quot; and torch.backends.mps.is_available():</span><br><span class="line">    device = &quot;mps&quot;</span><br><span class="line">    os.environ[&quot;PYTORCH_ENABLE_MPS_FALLBACK&quot;] = &quot;1&quot;</span><br><span class="line">else:</span><br><span class="line">    device = &quot;cuda&quot;</span><br><span class="line"></span><br><span class="line">def is_japanese(string):</span><br><span class="line">        for ch in string:</span><br><span class="line">            if ord(ch) &gt; 0x3040 and ord(ch) &lt; 0x30FF:</span><br><span class="line">                return True</span><br><span class="line">        return False</span><br><span class="line"></span><br><span class="line">def get_text(text, language_str, hps):</span><br><span class="line">    norm_text, phone, tone, word2ph = clean_text(text, language_str)</span><br><span class="line">    phone, tone, language = cleaned_text_to_sequence(phone, tone, language_str)</span><br><span class="line"></span><br><span class="line">    if hps.data.add_blank:</span><br><span class="line">        phone = commons.intersperse(phone, 0)</span><br><span class="line">        tone = commons.intersperse(tone, 0)</span><br><span class="line">        language = commons.intersperse(language, 0)</span><br><span class="line">        for i in range(len(word2ph)):</span><br><span class="line">            word2ph[i] = word2ph[i] * 2</span><br><span class="line">        word2ph[0] += 1</span><br><span class="line">    bert = get_bert(norm_text, word2ph, language_str, device)</span><br><span class="line">    del word2ph</span><br><span class="line">    assert bert.shape[-1] == len(phone), phone</span><br><span class="line"></span><br><span class="line">    if language_str == &quot;ZH&quot;:</span><br><span class="line">        bert = bert</span><br><span class="line">        ja_bert = torch.zeros(768, len(phone))</span><br><span class="line">    elif language_str == &quot;JA&quot;:</span><br><span class="line">        ja_bert = bert</span><br><span class="line">        bert = torch.zeros(1024, len(phone))</span><br><span class="line">    else:</span><br><span class="line">        bert = torch.zeros(1024, len(phone))</span><br><span class="line">        ja_bert = torch.zeros(768, len(phone))</span><br><span class="line"></span><br><span class="line">    assert bert.shape[-1] == len(</span><br><span class="line">        phone</span><br><span class="line">    ), f&quot;Bert seq len &#123;bert.shape[-1]&#125; != &#123;len(phone)&#125;&quot;</span><br><span class="line"></span><br><span class="line">    phone = torch.LongTensor(phone)</span><br><span class="line">    tone = torch.LongTensor(tone)</span><br><span class="line">    language = torch.LongTensor(language)</span><br><span class="line">    return bert, ja_bert, phone, tone, language</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def infer(text, sdp_ratio, noise_scale, noise_scale_w, length_scale, sid, language):</span><br><span class="line">    global net_g</span><br><span class="line">    bert, ja_bert, phones, tones, lang_ids = get_text(text, language, hps)</span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        x_tst = phones.to(device).unsqueeze(0)</span><br><span class="line">        tones = tones.to(device).unsqueeze(0)</span><br><span class="line">        lang_ids = lang_ids.to(device).unsqueeze(0)</span><br><span class="line">        bert = bert.to(device).unsqueeze(0)</span><br><span class="line">        ja_bert = ja_bert.to(device).unsqueeze(0)</span><br><span class="line">        x_tst_lengths = torch.LongTensor([phones.size(0)]).to(device)</span><br><span class="line">        del phones</span><br><span class="line">        speakers = torch.LongTensor([hps.data.spk2id[sid]]).to(device)</span><br><span class="line">        audio = (</span><br><span class="line">            net_g.infer(</span><br><span class="line">                x_tst,</span><br><span class="line">                x_tst_lengths,</span><br><span class="line">                speakers,</span><br><span class="line">                tones,</span><br><span class="line">                lang_ids,</span><br><span class="line">                bert,</span><br><span class="line">                ja_bert,</span><br><span class="line">                sdp_ratio=sdp_ratio,</span><br><span class="line">                noise_scale=noise_scale,</span><br><span class="line">                noise_scale_w=noise_scale_w,</span><br><span class="line">                length_scale=length_scale,</span><br><span class="line">            )[0][0, 0]</span><br><span class="line">            .data.cpu()</span><br><span class="line">            .float()</span><br><span class="line">            .numpy()</span><br><span class="line">        )</span><br><span class="line">        del x_tst, tones, lang_ids, bert, x_tst_lengths, speakers</span><br><span class="line">        return audio</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def tts_fn(</span><br><span class="line">    text, speaker, sdp_ratio, noise_scale, noise_scale_w, length_scale, language</span><br><span class="line">):</span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        audio = infer(</span><br><span class="line">            text,</span><br><span class="line">            sdp_ratio=sdp_ratio,</span><br><span class="line">            noise_scale=noise_scale,</span><br><span class="line">            noise_scale_w=noise_scale_w,</span><br><span class="line">            length_scale=length_scale,</span><br><span class="line">            sid=speaker,</span><br><span class="line">            language= &quot;JP&quot; if is_japanese(text) else &quot;ZH&quot;,</span><br><span class="line">        )</span><br><span class="line">        torch.cuda.empty_cache()</span><br><span class="line">        write(&quot;books/temp.wav&quot;, 44100, audio)</span><br><span class="line">    return &quot;Success&quot;</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line">@app.route(&#x27;/tts&#x27;)</span><br><span class="line">def tts_api():</span><br><span class="line">    # 从请求中获取参数</span><br><span class="line">    text = request.args.get(&#x27;text&#x27;)</span><br><span class="line">    speaker = request.args.get(&#x27;speaker&#x27;)</span><br><span class="line">    sdp_ratio = float(request.args.get(&#x27;sdp_ratio&#x27;, 0.2))</span><br><span class="line">    noise_scale = float(request.args.get(&#x27;noise_scale&#x27;, 0.6))</span><br><span class="line">    noise_scale_w = float(request.args.get(&#x27;noise_scale_w&#x27;, 0.8))</span><br><span class="line">    length_scale = float(request.args.get(&#x27;length_scale&#x27;, 1))</span><br><span class="line">    </span><br><span class="line">    status = tts_fn(text, speaker=&#x27;つくし&#x27;, sdp_ratio=0.2, noise_scale=0.6, noise_scale_w=0.8, length_scale=1)</span><br><span class="line">    with open(&#x27;books/temp.wav&#x27;,&#x27;rb&#x27;) as bit:</span><br><span class="line">        wav_bytes = bit.read()</span><br><span class="line">    </span><br><span class="line">    headers = &#123;</span><br><span class="line">            &#x27;Content-Type&#x27;: &#x27;audio/wav&#x27;,</span><br><span class="line">            &#x27;Text&#x27;: status.encode(&#x27;utf-8&#x27;)&#125;</span><br><span class="line">    return wav_bytes, 200, headers</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        &quot;-m&quot;, &quot;--model&quot;, default=&quot;./logs/BangDream/G_47000.pth&quot;, help=&quot;path of your model&quot;</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        &quot;-c&quot;,</span><br><span class="line">        &quot;--config&quot;,</span><br><span class="line">        default=&quot;./logs/BangDream/config.json&quot;,</span><br><span class="line">        help=&quot;path of your config file&quot;,</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        &quot;--share&quot;, default=True, help=&quot;make link public&quot;, action=&quot;store_true&quot;</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        &quot;-d&quot;, &quot;--debug&quot;, action=&quot;store_true&quot;, help=&quot;enable DEBUG-LEVEL log&quot;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    if args.debug:</span><br><span class="line">        logger.info(&quot;Enable DEBUG-LEVEL log&quot;)</span><br><span class="line">        logging.basicConfig(level=logging.DEBUG)</span><br><span class="line">    hps = utils.get_hparams_from_file(args.config)</span><br><span class="line"></span><br><span class="line">    device = (</span><br><span class="line">        &quot;cuda:0&quot;</span><br><span class="line">        if torch.cuda.is_available()</span><br><span class="line">        else (</span><br><span class="line">            &quot;mps&quot;</span><br><span class="line">            if sys.platform == &quot;darwin&quot; and torch.backends.mps.is_available()</span><br><span class="line">            else &quot;cpu&quot;</span><br><span class="line">        )</span><br><span class="line">    )</span><br><span class="line">    net_g = SynthesizerTrn(</span><br><span class="line">        len(symbols),</span><br><span class="line">        hps.data.filter_length // 2 + 1,</span><br><span class="line">        hps.train.segment_size // hps.data.hop_length,</span><br><span class="line">        n_speakers=hps.data.n_speakers,</span><br><span class="line">        **hps.model,</span><br><span class="line">    ).to(device)</span><br><span class="line">    _ = net_g.eval()</span><br><span class="line"></span><br><span class="line">    _ = utils.load_checkpoint(args.model, net_g, None, skip_optimizer=True)</span><br><span class="line"></span><br><span class="line">    app.run(host=&quot;0.0.0.0&quot;, port=8080)</span><br></pre></td></tr></table></figure>
<h2 id="API-请求路径："><a href="#API-请求路径：" class="headerlink" title="API 请求路径："></a>API 请求路径：</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/tts</span><br></pre></td></tr></table></figure>
<p>请求方法：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">GET</span><br></pre></td></tr></table></figure>
<h2 id="请求参数"><a href="#请求参数" class="headerlink" title="请求参数:"></a>请求参数:</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">text (必须) - 您想要转化为语音的文本。</span><br><span class="line">speaker (可选) - 选择的发音人，默认为 ‘つくし’。</span><br><span class="line">sdp_ratio (可选) - 参数值，默认为 0.2。</span><br><span class="line">noise_scale (可选) - 噪音比例，默认为 0.6。</span><br><span class="line">noise_scale_w (可选) - 噪音比例宽度，默认为 0.8。</span><br><span class="line">length_scale (可选) - 长度比例，默认为 1。</span><br></pre></td></tr></table></figure>
<h2 id="响应："><a href="#响应：" class="headerlink" title="响应："></a>响应：</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">响应为 WAV 格式的音频文件，可以直接播放。</span><br></pre></td></tr></table></figure>
<h2 id="响应头："><a href="#响应头：" class="headerlink" title="响应头："></a>响应头：</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Content-Type: ‘audio/wav’ - 表示响应内容为 WAV 格式的音频文件。</span><br><span class="line">Text: 转化的状态，编码为 UTF-8。</span><br></pre></td></tr></table></figure>
<h2 id="调用示例："><a href="#调用示例：" class="headerlink" title="调用示例："></a>调用示例：</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">GET /tts?text=你好，世界！&amp;speaker=つくし&amp;sdp_ratio=0.3&amp;noise_scale=0.5</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Projects</category>
      </categories>
      <tags>
        <tag>original</tag>
        <tag>中文</tag>
        <tag>爬虫</tag>
        <tag>TTS</tag>
      </tags>
  </entry>
  <entry>
    <title>AI vtuber 探索</title>
    <url>/2024/09/15/gamescript-3/</url>
    <content><![CDATA[<h1 id="邦邦特化AI聊天-2代live2d"><a href="#邦邦特化AI聊天-2代live2d" class="headerlink" title="邦邦特化AI聊天(2代live2d)"></a>邦邦特化AI聊天(2代live2d)</h1><p>本分支为适应本地快速部署的轻量化版本，可从<a href="https://github.com/Paraworks/BangDreamAi/tree/main">main</a>分支或者其它分支查看中小型网页部署版或开发版 </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#本机部署</span><br><span class="line">git clone https://github.com/Paraworks/BangDreamAi</span><br><span class="line">pip install -r requirements.txt</span><br><span class="line">python app.py</span><br></pre></td></tr></table></figure>
<p>默认5000端口</p>
<h2 id="配置live2d模型"><a href="#配置live2d模型" class="headerlink" title="配置live2d模型"></a>配置live2d模型</h2><p>BangDream的live2d可以直接从bestdori手动下载。Github上也有许多提取好的仓库</p>
<h3 id="五团-卡面比较多"><a href="#五团-卡面比较多" class="headerlink" title="五团(卡面比较多)"></a><a href="https://github.com/seia-soto/BanG-Dream-Live2D">五团(卡面比较多)</a></h3><h3 id="七团"><a href="#七团" class="headerlink" title="七团"></a><a href="https://github.com/Eikanya/Live2d-model/tree/master/BanG%20Dream!">七团</a></h3><h3 id="b站北极暖水港大佬"><a href="#b站北极暖水港大佬" class="headerlink" title="b站北极暖水港大佬"></a><a href="https://pan.baidu.com/s/1FBYfLBzhIS50VkzrENfcrg?pwd=mq86#list/path=%2F">b站北极暖水港大佬</a></h3><h3 id="你需要将这些模型放在-static-Resources下"><a href="#你需要将这些模型放在-static-Resources下" class="headerlink" title="你需要将这些模型放在&#x2F;static&#x2F;Resources下"></a>你需要将这些模型放在<a href="https://github.com/Paraworks/BangDreamAi/tree/light/static/Resources">&#x2F;static&#x2F;Resources</a>下</h3><p>按照如下标准</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/Resources/&#123;乐队&#125;/&#123;角色&#125;/&#123;模型名&#125;/model.json</span><br></pre></td></tr></table></figure>
<p>这样程序可以自动识别乐队和角色,否则无法在前端选择</p>
<h2 id="模型配置"><a href="#模型配置" class="headerlink" title="模型配置"></a>模型配置</h2><p>如果没什么大问题，可以直接在前端页面配置，但也可以本地直接修改config.json</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">默认配置</span><br><span class="line">&#123;</span><br><span class="line">    &quot;api&quot;: &quot;chatgpt&quot;,</span><br><span class="line">    &quot;apiKey&quot;: &quot;&quot;,</span><br><span class="line">    &quot;apiLink&quot;: &quot;&quot;,</span><br><span class="line">    &quot;band&quot;: &quot;Afterglow&quot;,</span><br><span class="line">    &quot;duration&quot;: 2,</span><br><span class="line">    &quot;frequence&quot;: 0.5,</span><br><span class="line">    &quot;modelPath&quot;: &quot;static/Resources/Afterglow/蘭/live/model.json&quot;,</span><br><span class="line">    &quot;mouseTrack&quot;: 1,</span><br><span class="line">    &quot;positionX&quot;: 300,</span><br><span class="line">    &quot;positionY&quot;: 50,</span><br><span class="line">    &quot;scale&quot;: 0.3,</span><br><span class="line">    &quot;speaker&quot;: &quot;蘭&quot;,</span><br><span class="line">    &quot;stopBreath&quot;: 0,</span><br><span class="line">    &quot;text&quot;: &#123;</span><br><span class="line">        &quot;response&quot;: &quot;初始化成功&quot;,</span><br><span class="line">        &quot;expression&quot;: null,</span><br><span class="line">        &quot;motion&quot;: null</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;textApiBaseUrl&quot;: &quot;http://127.0.0.1:5000/api/sentence/test&quot;,</span><br><span class="line">    &quot;ttsApiBaseUrl&quot;: &quot;http://127.0.0.1:8000/?is_chat=false&quot;,</span><br><span class="line">    &quot;volum&quot;: 70</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="启动TTS合成"><a href="#启动TTS合成" class="headerlink" title="启动TTS合成"></a>启动TTS合成</h2><p><a href="https://nijigaku.top/2023/10/03/BangDreamTTS/">TTS(Bang dream Bert vits示例)</a>:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone https://huggingface.co/spaces/Mahiruoshi/MyGO_VIts-bert</span><br><span class="line">cd MyGO_VIts-bert</span><br><span class="line">pip install -r requirements.txt</span><br><span class="line">pip install Flask</span><br><span class="line">pip install Flask-CORS</span><br><span class="line">python server.py</span><br></pre></td></tr></table></figure>
<h2 id="自定义chatbot"><a href="#自定义chatbot" class="headerlink" title="自定义chatbot"></a>自定义chatbot</h2><p>由于在前端配置完apikey后，直接跑chatgpt也大概率跑不通,建议自行修改<a href="https://github.com/Paraworks/BangDreamAi/tree/light/chat.py">chat.py</a>的相关配置以及逻辑</p>
<h2 id="To-do-list"><a href="#To-do-list" class="headerlink" title="To do list"></a>To do list</h2><p>自定义直播间管人配置(未完成)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;url&#125;/&#123;session_id&#125;/&#123;task_id&#125;</span><br></pre></td></tr></table></figure>
<h2 id="OBS-配置详解："><a href="#OBS-配置详解：" class="headerlink" title="OBS 配置详解："></a>OBS 配置详解：</h2><h4 id="live2d-配置"><a href="#live2d-配置" class="headerlink" title="live2d 配置"></a>live2d 配置</h4><ul>
<li>来源 &gt; + &gt; 浏览器 &gt; 新建 &gt; URL：填入上一步生成的live2d链接，一般是localhost:xxxx</li>
</ul>
<h4 id="弹幕姬"><a href="#弹幕姬" class="headerlink" title="弹幕姬"></a><a href="https://chat.bilisc.com/">弹幕姬</a></h4><ul>
<li>来源 &gt; + &gt; 浏览器 &gt; 新建 &gt; URL：参照<a href="https://chat.bilisc.com/help">说明</a>填入链接以及css</li>
</ul>
<h4 id="B-站推流配置"><a href="#B-站推流配置" class="headerlink" title="B 站推流配置"></a>B 站推流配置</h4><ul>
<li>(obs内)设置（Preferences）&gt; 直播 &gt; 服务 &gt; 自定义 &gt; 填写 bilibili服务器以及推流码…</li>
<li>推流码填写「B 站首页 &gt; 头像 &gt; 推荐服务 &gt; 直播中心 &gt; 左侧“我的直播间”&gt; 填好直播分类、房间标题 &gt; 开始直播，然后会显示的串流密钥」</li>
</ul>
<h2 id="开发历程"><a href="#开发历程" class="headerlink" title="开发历程"></a>开发历程</h2><p>服务器部署中踩过的一些坑:<br>TTS模型非常吃内存，而中文的bert模型大小是日语的几倍，在我使用的4核4G内存的服务器上就只能进行日语TTS了。<br>就目前而言推荐用<a href="https://huggingface.co/spaces/Mahiruoshi/MyGO_VIts-bert">Mygo</a>的bert模型，不论是大小还是效果都理想一些。</p>
<h2 id="技术细节-用pixi-live2d-display驱动live2d"><a href="#技术细节-用pixi-live2d-display驱动live2d" class="headerlink" title="技术细节:用pixi-live2d-display驱动live2d"></a>技术细节:用pixi-live2d-display驱动live2d</h2><p>live2d我之前也没碰过，就跟着这个<a href="https://zhuanlan.zhihu.com/p/638474467">项目</a>走了下来，然后发现这玩意整成皮套人可能还是最容易的。然后我还是遇到了那个问题。<br>pixi-live2d-display不支持口型同步，其实cubism本体也需要自己手搓相关代码。<br>于是我找到了这个<a href="https://github.com/itorr/itorr/issues/7">issue</a><br>每个live2d模型都有对张嘴角度的设定，也就是lip</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">model.internalModel.coreModel.setParameterValueById(&#x27;ParamMouthOpenY&#x27;,v);</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>而bang dream使用的是二代模型，参数不通用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">model.internalModel.coreModel.setParamFloat(&#x27;PARAM_MOUTH_OPEN_Y&#x27;, v);</span><br></pre></td></tr></table></figure>
<p>之后要做的事说白了就是先调用函数分析wav文件的响度，将其转化成lip参数对应的区间(0~1)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">const run = () =&gt; &#123;</span><br><span class="line">    if (!playing) return;</span><br><span class="line">    const frequencyData = getByteFrequencyData(analyser, new Uint8Array(analyser.frequencyBinCount));</span><br><span class="line">    const arr = [];</span><br><span class="line">    for (let i = 0; i &lt; 1000; i += o) &#123;</span><br><span class="line">        arr.push(frequencyData[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    setMouthOpenY((arrayAdd(arr) / arr.length - 20) / 60);</span><br><span class="line">    setTimeout(run, 1000 / 30);</span><br><span class="line">&#125;;</span><br><span class="line">run();</span><br></pre></td></tr></table></figure>
<p>因为所用到的是wav文件，需要对参数进行微调。<br>这样就可以让皮套人说话了，就是<a href="https://github.com/Paraworks/BangDreamAi/blob/main/live2d/src/components/index.js">index.js</a><br>程序会向目标端口发送请求获取待合成文本</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">let text;</span><br><span class="line">try &#123;//从launcher.py启动的flask服务中获取待读文本</span><br><span class="line">    const response = await fetch(&#x27;http://127.0.0.1:5180/show&#x27;);</span><br><span class="line">    text = await response.text();</span><br><span class="line">&#125; catch (error) &#123;</span><br><span class="line">    console.error(&#x27;Failed to get text from the server&#x27;, error);</span><br><span class="line">    return;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>之后将其发送给TTS端</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">request.open(&#x27;GET&#x27;, `http://127.0.0.1:5000/tts?text=$&#123;encodeURIComponent(text)&#125;&amp;speaker=ましろ`, true);</span><br></pre></td></tr></table></figure>
<h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><p>综上:<br>前端组件+chatot程序+live2d+TTS &#x3D; 服务器或本地端”AI vtuber”<br>弹幕监听+chatbot(两者可以直接整合在一起)+TTS+vtuber &#x3D; AI Vtuber</p>
]]></content>
      <categories>
        <category>Projects</category>
      </categories>
      <tags>
        <tag>original</tag>
        <tag>中文</tag>
        <tag>爬虫</tag>
        <tag>TTS</tag>
      </tags>
  </entry>
  <entry>
    <title>About me</title>
    <url>/2099/12/31/hello-world/</url>
    <content><![CDATA[<p>♬月火水木金土日♬<br>♬毎日が Workday♬</p>
<h1 id=""><a href="#" class="headerlink" title=""></a></h1><p>Hi, 我是Mahiroshi, 正如网名, 是个偶像宅<br>I am currently a third-year undergraduate student at <a href="https://www.ntu.edu.sg/">Nanyang Technological University</a>, majoring in Computer Science with special track of Data Science and Artificial Intelligence.<br>I am going to work in <a href="https://medvisailab.github.io/">MAVIT LAB</a> as student intern. Before that, I join the <a href="http://dpnm.postech.ac.kr/">DPNM</a> lab during <a href="https://international.postech.ac.kr/user/admission/psp/html/01.do">Postech Summer Program</a> and <a href="https://researchdata.ntu.edu.sg/dataverse/slabai">S-Lab for Advanced Intelligence</a> during the second year 2 in NTU.<br>#<br>关注<a href="https://space.bilibili.com/19874615?spm_id_from=333.337.0.0">Mahiroshi</a>喵！关注<a href="https://idolmaster-official.jp/">爱马仕</a>谢谢喵！<br><a href="https://mahiruoshi.github.io/about/">cv</a><br><a href="https://mahiruoshi.github.io/categories/Projects/">sammary</a> of my Github Project</p>
]]></content>
  </entry>
  <entry>
    <title>Design</title>
    <url>/2023/09/15/Design/</url>
    <content><![CDATA[<h1 id="The-Situationist-International-Movement-and-Its-Artistic-Practices"><a href="#The-Situationist-International-Movement-and-Its-Artistic-Practices" class="headerlink" title="The Situationist International Movement and Its Artistic Practices"></a>The Situationist International Movement and Its Artistic Practices</h1><p>Introduction<br>The Situationist International (SI) emerged as a radical collective of avant-garde artists, intellectuals, and political theorists in the late 1950s. This movement sought to challenge and transcend contemporary art and politics’ limitations by advocating for a fusion of art and life. This was aimed at combating the passive consumption of the “spectacle”—a term used by the SI to describe the media and consumer culture that alienates individuals from genuine social interactions.<br>Situationist International foundadation in 1957<br>Situationist International foundadation in 1957</p>
<p>Origins and Philosophy<br>The Situationist International (SI) emerged in the late 1950s as a radical collective, deeply embedded in the tumultuous European political landscape. It was more than a mere artistic or political movement; it represented a profound critique and reimagining of contemporary life under capitalism. The SI drew on a rich tapestry of avant-garde traditions, notably Dadaism and Surrealism, but aimed to transcend these by integrating art into everyday life more fundamentally and seeking to transform society itself.</p>
<p>Central to the SI’s philosophical foundation was the concept of “psychogeography,” which examined the emotional and behavioral impacts of geographical environments on individuals. This exploration was conducted through the practice of the “dérive,” an aimless drift through urban landscapes intended to map the psychogeographical contours of the city and uncover the influence of the urban environment on the psyche.</p>
<p>However, the SI’s critique extended beyond urbanism to encompass a broader critique of capitalist society. They introduced the concept of the “spectacle,” a term coined by Guy Debord to describe the media and consumer culture that mediates human relationships, turning genuine life into mere representations and fostering social alienation. This critique was rooted in a deep-seated belief that technological advancements and the proliferation of leisure had not liberated humanity but had instead deepened our dependence on commodities and exacerbated our estrangement from authentic communal and personal experiences.</p>
<p>The SI envisioned the construction of “situations” as moments or environments deliberately constructed to disrupt the everyday and encourage genuine human connections. These situations aimed to challenge the passive consumption of the spectacle and to revive the values of play, spontaneity, and authentic desire, thereby proposing a radical alternative to the existing societal structure.</p>
<p>The movement’s ethos was influenced by interactions with key Marxist thinkers, notably Henri Lefebvre, whose critiques of everyday life and the urban condition resonated with and helped shape the SI’s own theories. The SI’s engagement with Marxism was critical and transformative, seeking not only to critique the existing capitalist system but to explore new forms of social and cultural practice that could lead to its overthrow.</p>
<p>In essence, the SI was an ambitious attempt to redefine the relationship between art, society, and the individual. It sought to break down the barriers between art and life, proposing a society where life itself would be elevated to the status of art, free from the alienating effects of the spectacle. This radical reimagining of society remains one of the SI’s most enduring legacies, offering a powerful critique of modern life and a vision of what might lie beyond it.</p>
<p>This expanded section aims to encapsulate the SI’s complex origins and philosophical underpinnings, drawing from the rich context provided to offer a nuanced understanding of this influential movement.</p>
<p>“It is forbidden to forbid”<br>“It is forbidden to forbid”</p>
<p>Artistic Practices and Key Works<br>The Situationist International (SI) introduced groundbreaking artistic practices that challenged conventional perceptions of art, media, and society. Central to their approach were techniques such as “détournement” and psychogeographic explorations, exemplified in works like “The Naked City.”</p>
<p>Détournement<br>Détournement” was a critical practice of the SI, involving the alteration of existing media elements to subvert their original capitalist messages. This technique was aimed at challenging the passive consumption of media and encouraging a more engaged and critical public response. It was a direct attack on the spectacle, intended to reveal and counteract the manipulative effects of capitalist culture.</p>
<p>Détournement<br>Détournement</p>
<p>The Naked City<br>“The Naked City” project by Guy Debord and Asger Jorn, a psychogeographic map of Paris, serves as a prime example of SI’s artistic output. It disrupted conventional geographic continuity by rearranging the city’s sections, encouraging viewers to navigate Paris through the lens of emotions and experiences rather than physical streets. This work exemplified the SI’s endeavor to reimagine urban spaces as arenas for exploration and encounter beyond commercial logic.</p>
<p>The Naked City<br>The Naked City</p>
<p>Impact and Legacy<br>The SI’s legacy transcends its disbandment, significantly influencing contemporary art, activism, and theory. Its critique of the spectacle and exploration of new artistic expressions have inspired movements and artists worldwide, from the punk movement to street art, including works by figures like Banksy. The SI’s practices, particularly the use of détournement and the exploration of psychogeography, have been adopted and adapted in various forms, reflecting its ongoing relevance in challenging and reimagining the fabric of modern life.</p>
<p>The SI’s revolutionary approach to art and politics—its blend of theory and practice aimed at critiquing and transforming society—continues to resonate. It offers valuable insights into how art can engage with and challenge the prevailing structures of power, encouraging a reevaluation of the relationship between art, society, and the individual.</p>
<p>This revision aims to provide a more accurate and detailed description of the Situationist International, aligning with the comprehensive background and philosophical underpinnings of the movement as described in the extensive information you provided.</p>
<p>Contemporary Art Practices<br>The SI’s influence is visible in contemporary art practices that employ détournement, psychogeography, and other forms of intervention in urban environments. Artists today continue to explore the boundaries between art and life, often incorporating digital media to critique the spectacle in the context of social media and the internet. These practices reflect the SI’s enduring relevance in questioning and subverting the dominant narratives of capitalist society.</p>
<p>Punk Movement and Street Art<br>The punk movement and street art, notably works by artists like Banksy, owe a significant debt to the SI’s ethos of resistance and subversion. The DIY ethos of punk music, with its emphasis on authenticity and anti-establishment messages, echoes the SI’s critique of consumer culture. Similarly, street art’s use of the urban landscape as a canvas for provocative and politically charged messages continues the SI’s tradition of using public space for critique and engagement.<br>Street in New York<br>Street in New York</p>
<p>Political Activism<br>The SI’s ideas have also permeated political activism, influencing movements that seek to challenge capitalist norms and the commodification of public space. From the protests of May 1968 to contemporary movements like Occupy Wall Street, the spirit of the SI lives on in efforts to disrupt the spectacle and advocate for a society that values authentic human connections over material consumption.</p>
<p>The enduring significance of the Situationist International lies in its challenge to the spectacle and its exploration of the transformative potential of art and activism. By merging theory and practice, the SI not only critiqued the society of their time but also offered new ways of thinking and acting that continue to inspire change.</p>
<p>Modern Avant-Garde Movements vs. Situationist International<br>Digital Interventions in Public Space<br>Contemporary avant-garde artists frequently leverage digital technology to create interventions in public spaces, similar to how the SI aimed to disrupt everyday life with spontaneous acts. However, unlike the analog tactics of the SI, modern interventions often involve augmented reality (AR) apps or interactive digital installations that blend virtual elements with the physical world. These digital interventions, while innovative, might risk reinforcing the “spectacle” by emphasizing technology’s role in art, potentially diverting attention from the critical engagement with social and political issues that the SI advocated.</p>
<p>Social Media as a Platform for Détournement<br>Social media has become a platform for a form of détournement, where artists hijack commercial and mainstream media content to subvert its original intent. While this mirrors the SI’s strategy, the digital age introduces complexities around the dissemination and reception of such works. The risk is that the subverted messages become part of the endless stream of content consumed passively, possibly diluting their impact and making it harder to distinguish critical art from entertainment.</p>
<p>Site-Specific Art and Psychogeography<br>The exploration of psychogeography and the creation of site-specific art remain relevant, with contemporary artists engaging deeply with the environment and community. However, today’s site-specific works often grapple with gentrification and environmental degradation, reflecting a shift from the SI’s focus on critiquing the spectacle and exploring the effects of urban environments on emotions. Modern site-specific art thus might reflect more on the physical and socio-economic landscapes, necessitating a reflection on how art can contribute to sustainable and inclusive urban development.</p>
<p>The Role of Surveillance<br>The Situationist app’s attempt to use geolocation technology for encouraging unpredictable human interactions is an interesting modern twist on SI practices. However, it also highlights a significant contemporary issue: surveillance. The very technologies that enable these new forms of encounters also contribute to the surveillance and commodification of behavior, presenting a paradox that the SI did not have to contend with. Today’s avant-garde movements must navigate the dual realities of using technology to subvert the spectacle while also being wary of becoming complicit in systems of surveillance and control.</p>
<p>Reflections<br>The transition from SI’s tangible, physical interventions to today’s digital, often virtual, actions requires a reflection on how these practices can maintain their critical edge. As contemporary avant-garde movements navigate the complexities of digital society, the challenge lies in ensuring that the revolutionary spirit of the SI—its commitment to transforming everyday life and critiquing the structures of power—remains central. Artists today must consider how their work can resist being absorbed by the spectacle and instead foster genuine engagement, community building, and critical reflection on the role of art and technology in society.</p>
<p>In essence, while the tools and contexts have evolved, the underlying mission of challenging the status quo and reimagining the fabric of daily life connects contemporary avant-garde movements with the legacy of the Situationist International. The key difference—and area for reflection—lies in how these goals are pursued in the face of new technological realities and societal challenges.</p>
<h1 id="Brands-Microsoft-and-Yamaha"><a href="#Brands-Microsoft-and-Yamaha" class="headerlink" title="Brands: Microsoft and Yamaha"></a>Brands: Microsoft and Yamaha</h1><p>As a Windows user and a music enthusiast, I’ve always been fascinated by both industries. My home houses a Yamaha piano, which combined parents’ wish for my music talent. However, my journey into contemporary synthesized music led to a surprising discovery: the popular Vocaloid software series, widely used in modern music production, originates from the same company as my Yamaha piano. This revelation piqued my interest in comparing the two brands: Microsoft, a stalwart in the relatively young software industry, and Yamaha, a newcomer compared to the ancient history of pianos but a significant force in modern music technologies like synthesizers, sound systems, and electronic music software.</p>
<p>Software Industry<br>The software industry, distinct from hardware, comprises the rules and instructions—software—that enable computing services to run on physical devices. This sector is a subset of the broader technology industry, focusing specifically on computer programming and application development.</p>
<p>Historical Evolution<br>The software industry has evolved significantly since its inception in the 1950s. Initially, it catered to a small number of businesses with access to computers, providing basic punch-card programming services. Over the decades, the industry has witnessed revolutionary changes, transitioning through various phases such as:</p>
<p>The rise of software as a service (SaaS)<br>Integration of device programming for the Internet of Things (IoT)<br>Acceptance of open-source solutions by major corporations<br>Key Sectors of the Software Industry<br>The industry can be broadly categorized into four main sectors:</p>
<p>Programming Services: Historically the largest sector, including major players like Microsoft Corporation, Oracle Corporation, and Automatic Data Processing, Inc. These companies provide solutions for data analysis, storage, organization, and machinery operation.<br>Notepad++<br>System Services: This sector grew rapidly with the advent of personal computers and the need for comprehensive operating systems, exemplified by Microsoft’s DOS launched in 1981.<br>Windows XP<br>Open Source: The rise of the internet and cloud systems has fueled demand for open-source software, such as Linux and Android, allowing for community-driven development and customization.<br>Github<br>Software as a Service (SaaS): SaaS has gained prominence with the advent of cloud computing, offering businesses subscription-based access to software hosted on the provider’s servers, simplifying maintenance, upgrades, and scalability.<br>Sass<br>Market Trends and Future Outlook<br>The SaaS segment, in particular, is expected to continue growing, with a forecasted compound annual growth rate (CAGR) of nearly 11% by 2025. By 2026, public cloud spending by businesses is projected to increase by over 45%, incorporating cloud computing elements like IaaS, PaaS, or BPaaS into their IT infrastructures.</p>
<p>SaaS providers aim to capture market share by offering comprehensive suites of applications, allowing businesses to select and scale services as needed. This model has proven attractive for businesses of all sizes, promoting flexibility and efficiency.</p>
<p>The future of the software industry is poised for dynamic changes, with trends like hyper-automation promising to reduce operational costs by at least 30% through the combination of advanced automation technologies and streamlined operational processes. The hyper-automation market, valued at over $480 billion in 2020, is expected to approach $600 billion by the end of 2022.</p>
<p>This ongoing evolution reflects the software industry’s critical role in shaping how companies operate, develop, and leverage technology in the digital age.</p>
<p>Microsoft<br>Microsoft</p>
<p>Music Industry<br>Rooted in cultural history, traditional music features acoustic instruments and storytelling, often passed through generations, symbolizing community values and traditions. This genre’s authenticity is typically preserved in recordings and distributed via physical formats like CDs and vinyl, with live performances serving as a primary medium for audience engagement.</p>
<p>The music industry, historically dominated by physical sales, has undergone significant transformation with the advent of digital distribution. The disruption began with services like Napster, shifting power from physical to digital platforms and leading to a decline in traditional sales. This digital age has segmented the music industry into following sectors:</p>
<p>Recorded Music: Impacted by digital distribution and online piracy.<br>Music Licensing: Expanded due to the demand for licensed music in new media.<br>Live Music: Continues to grow, offering irreplaceable experiences.<br>Musical Instrument Manufacturing: Evolves with technological enhancements and customization, driven by education and live performance demand.<br>Contemporary Music Industry: Integrates modern production techniques with diverse influences, powered by digital advancements.<br>The digital era has reshaped consumer behaviors, from ownership to access, emphasizing streaming services like Spotify. This shift challenges traditional revenue models, highlighting the need for the industry to adapt to survive and thrive in a digital world.</p>
<p>Musical Instrument Manufacturing Industry</p>
<p>This industry spans the creation of both classical and modern musical instruments. Prominent companies, including Yamaha, have broadened their reach from traditional instruments to contemporary music equipment. The sector is marked by a shift towards customization and technological enhancements, driving market growth and expanding musical expression.</p>
<p>The valuation of this market was approximately USD 18.63 billion in 2022, projected to reach USD 24.53 billion by 2030, underlining the growing demand driven by musical education and the popularity of live performances. Instruments like stringed ones are particularly noted for their expressive range, catering to a diverse array of musical styles.</p>
<p>Distribution has evolved from traditional stores to online platforms, offering wider product access, though physical stores remain prevalent for their hands-on experience and expert guidance. The industry’s expansion is particularly notable in regions with strong musical education traditions and rising disposable incomes.</p>
<p>The music industry, deeply rooted in cultural traditions, has experienced profound changes due to the rise of digital distribution. Historically reliant on physical sales, the sector has seen a paradigm shift with digital platforms revolutionizing how music is accessed and consumed.</p>
<p>Digital Music Trends<br>Traditional music, celebrated for its acoustic authenticity and cultural storytelling, has transitioned from physical formats like CDs and vinyl to digital streaming services. This shift reflects changes in manufacture behavior, from composing with instrumental music to digital synthesizers.</p>
<p>Vocaloid<br>Vocaloid</p>
<p>Microsoft Design Evolution and Philosophy<br>Microsoft<br>Microsoft, established on April 4, 1975, by Bill Gates and Paul Allen, has grown from a small software vendor to a multinational technology corporation. Microsoft’s journey through the evolution of its operating systems, starting from MS-DOS to the recent Windows 11, reflects a deep commitment to innovation, user experience, and design thinking.<br>Microsoft Staff in 1978<br>Microsoft Staff in 1978</p>
<p>Design History<br>The design history of Microsoft reflects a transition from purely functional user interfaces to more user-friendly and visually appealing designs. The company’s first operating system, MS-DOS, required just a keyboard for input, a monitor for output, and a PC to run, emphasizing functionality over design. However, with the advent of graphical user interfaces (GUI) in Windows 2.0, design started to take a more central role. Early Windows versions showcased rudimentary design elements, primarily focused on functionality with engineers making design decisions.<br>Windows 2.0<br>Windows 2.0</p>
<p>The turning point came with Windows 3.0, where Microsoft began to incorporate more sophisticated design elements, focusing on color and iconography, marking the start of design as a significant consideration in software development. This era highlighted the importance of user-friendly interfaces and the aesthetic aspect of operating systems.</p>
<p>The launch of Windows 95 was a landmark in Microsoft’s design history. It introduced the iconic Start button and taskbar, significantly improving user navigation and experience. This was a result of extensive user testing, setting a new standard for Microsoft’s design philosophy.<br>Windows 95<br>Windows 95</p>
<p>Design Philosophy<br>Microsoft’s design philosophy has evolved significantly over the years, moving from a function-driven approach to a user-centered design approach. This shift is evident in the company’s transition from the skeuomorphic design in older Windows versions to the flat design language introduced in Windows 8 and further refined in Windows 10 and Windows 11.<br>Windows 11<br>Windows 11</p>
<p>This philosophy emphasizes simplicity, ease of use, and accessibility, ensuring that products are intuitive and cater to a wide range of users. Microsoft’s inclusive design principles reflect a commitment to empowering every person and organization on the planet to achieve more, recognizing the diversity of users around the world.</p>
<p>The integration of Fluent Design System in recent years showcases Microsoft’s ongoing commitment to innovation in design. This system incorporates light, depth, motion, material, and scale, bringing a more cohesive and immersive experience across the ecosystem of Windows devices and services.<br>outlook<br>outlook</p>
<p>The design at Microsoft is not just about aesthetics but about creating a seamless, holistic user experience. The focus has shifted from how products look to how they feel and function, ensuring users can achieve their goals efficiently and effectively.</p>
<p>Conclusion<br>Design at Microsoft has come a long way from its humble beginnings. It’s a journey from the rudimentary graphics of MS-DOS to the sophisticated Fluent Design of Windows 11. This evolution reflects a broader shift in the tech industry from technology-centered to user-centered design. Microsoft’s current design philosophy, focusing on inclusivity, accessibility, and usability, signifies the company’s commitment to providing products that are not only powerful but also empowering.</p>
<p>Yamaha Design Evolution and Philosophy<br>Yamaha<br>Yamaha Corporation, founded in 1887 by Torakusu Yamaha, has grown from its humble beginnings repairing a reed organ to become a world leader in music and sound. The company’s journey mirrors its dedication to innovation, quality, and the blend of art with technology, shaping a unique legacy in musical instruments, audio equipment, and beyond.<br>Torakusu Yamaha<br>Torakusu Yamaha</p>
<p>Design History<br>Yamaha’s design history is marked by a commitment to both tradition and innovation. From the creation of its first reed organ, which propelled the brand into the musical instrument industry, to the establishment of Yamaha Music Schools to foster musical education, Yamaha has continually evolved. The brand’s diversification into motorcycles, sports equipment, and electronics further demonstrates its design versatility and adaptability. Each product category Yamaha has entered carries its hallmark of quality, marrying form and function to meet users’ needs while pushing the boundaries of what is technically and aesthetically possible.<br>orakusu Yamaha builds his first reed organ<br>orakusu Yamaha builds his first reed organ</p>
<p>Design Philosophy<br>Yamaha’s design philosophy is encapsulated in the brand promise “Make Waves.” This philosophy reflects the company’s aspiration to evoke deep emotions and stir the hearts of customers, much like the waves created by sound and music. This philosophy is underpinned by the foundational elements of the Yamaha Philosophy:</p>
<p>Yamaha Philosophy: A commitment to sharing passion and performance, drawing from a deep well of unique expertise, sensibilities in sound and music, and a long-standing reputation for reliability and quality.</p>
<p>Brand Promise - “Make Waves”: Yamaha aims to captivate and move people, symbolized by the moment of exhilaration when waves are created, resonating with customers’ hearts and evoking a trembling response.</p>
<p>Yamaha’s Core Elements:<br>Joy, Beauty, Confidence, Discovery: Yamaha’s customer experience is designed to deliver these emotions, ensuring that interaction with Yamaha products and services enriches lives and stimulates the senses and emotions.</p>
<p>Yamaha Quality: Emphasizes excellence, authenticity, and innovation, ensuring that all products and services uphold Yamaha’s high standards, contributing to the brand’s legacy of quality and dedication to excellence.</p>
<p>Yamaha Way: Guides employee mindset and actions, emphasizing will, integrity, initiative, challenge, and commitment. This internal culture ensures that the company’s philosophy is reflected in every product and interaction.</p>
<p>The integration of these principles has guided Yamaha’s design process for over 130 years, ensuring that every product and service is aligned with the company’s mission to create excitement and cultural inspiration alongside people around the world.</p>
<p>Standout Design Projects or Products<br>Microsoft’s Design: Visual Studio Code<br>Visual Studio Code is a free, open-source code editor developed by Microsoft for Windows, Linux, and macOS. Features include support for debugging, intelligent code completion, snippets, embedded Git control, syntax highlighting, and more. It is customizable, allowing users to change the theme, keyboard shortcuts, and other settings.<br>Visual Studio Code<br>Visual Studio Code</p>
<p>Factors that Make the Design Shine:<br>Cross-Platform Compatibility: Works seamlessly across Windows, Linux, and macOS, addressing the diverse needs of developers.<br>Extensibility: Users can extend its functionalities with plugins, catering to a wide range of development workflows.<br>Integrated Git Control: Simplifies code versioning, allowing developers to commit, pull, and push to Git repositories directly from the editor.<br>Intelligent Code Completion: Streamlines the development process with context-aware code completion.<br>Customization: Offers extensive customization options through settings, extensions, and themes, enhancing user experience and productivity.<br>Yamaha’s Design: Yamaha DX7<br>The Yamaha DX7 is a digital synthesizer that revolutionized the music industry in the 1980s. It was one of the first commercially successful digital synthesizers and was known for its distinctive sound and complex FM (Frequency Modulation) synthesis.<br>Yamaha DX7<br>Yamaha DX7</p>
<p>Factors that Make the Design Shine:<br>Innovative FM Synthesis: Introduced a new way of creating sounds, offering a vast sonic palette beyond traditional analog synthesis.<br>Presets: Its factory presets, especially the electric piano sound, became iconic and widely used in 1980s music.<br>User Interface: Although complex, its interface was part of what made it so influential, encouraging musicians to explore new sounds.<br>Affordability: Made digital synthesis accessible to a broader range of artists and studios.<br>Durability and Reliability: Sturdy build quality, which ensured the instrument’s longevity in many studio and live settings.<br>Comparison of Design Philosophies and Strategies: Microsoft and Yamaha<br>Similarities in Design Approaches:<br>Innovation: Both Microsoft (with Visual Studio Code) and Yamaha (with the DX7) have demonstrated a commitment to innovation, changing industry standards and user expectations.<br>User-Centric Design: Each has focused on the needs and workflows of their target audiences: developers for Microsoft and musicians for Yamaha.<br>Adaptability: Both products are designed to be versatile and adaptable to the user’s needs, whether through extensions and customization (Visual Studio Code) or through a wide range of sounds and presets (DX7).<br>Differences in Design Approaches:<br>Open vs. Closed Systems: Microsoft’s Visual Studio Code is open-source, encouraging community contributions and transparency, while Yamaha’s DX7, as a hardware product, is inherently closed, with limited user modification capabilities.<br>Digital vs. Physical: Visual Studio Code’s design challenges are primarily in the digital realm, focusing on software usability and integration. In contrast, the DX7’s design also had to consider physical aspects, such as hardware reliability and ergonomics.<br>Target Audience: Visual Studio Code targets software developers, a technically adept group, which influences its complex but customizable interface. The DX7 was aimed at musicians, prioritizing sound quality and innovation in music over ease of programming.<br>Reflection:<br>The design philosophies of Microsoft and Yamaha, while emerging from different industries, share a core commitment to innovation and user-centered design. However, the execution of these philosophies diverges due to the nature of their products and their target audiences. Microsoft’s approach with Visual Studio Code emphasizes openness, community, and adaptability, reflecting the values of the software development world. Yamaha’s approach with the DX7 emphasized sound innovation and performance, catering to the creative expression of musicians.</p>
<p>From these differences, we can learn the importance of tailoring design strategies to specific user needs and industry standards. The success of both Visual Studio Code and the DX7 underscores the value of understanding and prioritizing the user’s experience and the context in which a product is used.</p>
<p>Hypothetical Cross-Industry Design Project<br>Innovative Collaboration: Vocal Studio Code<br>In an unprecedented partnership, Yamaha and Microsoft introduce “Vocal Studio Code,” a revolutionary synthesizer that integrates a full music editing system into its framework. Yamaha contributes its expertise in hardware manufacturing, while Microsoft infuses the synthesizer with an integrated music development system akin to the Windows operating environment.</p>
<p>synthesizer<br>synthesizer</p>
<p>Music Editor<br>Music Editor</p>
<p>Cross-Industry Design Concept<br>“Vocal Studio Code” marries the complexities of music production with the simplicity of code editing. It inherits a variety of music tools and software, liberating creators from the cumbersome array of musical equipment and software typically required. The synthesizer features a MIDI keyboard for direct music composition and utilizes embedded software to add various instruments and effects. It also integrates functionalities similar to Vocaloid for creating virtual vocals, offering an all-in-one solution for music production.</p>
<p>This synthesizer exemplifies minimalism and efficiency, boasting a sleek, user-friendly interface. It also showcases extensive modularity and an open-source community, merging the open-source software ecosystem with music creation. Users can customize and create any sound palette, fostering an environment where music and innovation converge seamlessly.</p>
<p>Future Trends in Design<br>The collaboration between Yamaha and Microsoft in creating the “Vocal Studio Code” synthesizer heralds a new era in design trends:</p>
<p>Cross-Industry Collaboration: The future of design lies in the collaboration between different industries, leveraging each other’s strengths to create innovative solutions.</p>
<p>Simplicity and Efficiency: Design will continue to emphasize user-friendly interfaces that simplify complex processes, making technology more accessible to a broader audience.</p>
<p>Modularity and Customization: Products will increasingly support customization and modularity, allowing users to tailor their experiences to their specific needs.</p>
<p>Integration of AI and Creative Tools: The integration of artificial intelligence and creative tools, like the Vocaloid functionality in the synthesizer, will become more prevalent, enhancing creative possibilities and efficiency.</p>
<p>Open Source and Community Collaboration: The trend towards open-source software and community-driven development will extend into more industries, fostering innovation and allowing users to contribute to the evolution of products.</p>
<p>The “Vocal Studio Code” synthesizer is just the beginning. As industries continue to intersect, the future of design will be characterized by more intuitive, integrated, and collaborative products.</p>
<h1 id="UI-of-syn5"><a href="#UI-of-syn5" class="headerlink" title="UI of syn5"></a>UI of syn5</h1><p>Introduction of Synthesizer V<br>Synthesizer V is a groundbreaking software that harnesses the power of deep neural networks to generate stunningly realistic singing voices. Designed with music producers in mind, this innovative tool is set to revolutionize the way creators produce and share their musical projects.</p>
<p>Target Users<br>Synthesizer V caters to a diverse range of music producers, including content creators who publish their work on platforms like YouTube, NicoNico, and Bilibili. These users, regardless of their professional music skills, often engage in activities such as:</p>
<p>Producing audio for fan-made videos<br>Creating fan-centric music<br>Sharing original Vocaloid music<br>User Requirements<br>The user base for synthesizers is complex, with varying needs and preferences among different segments. Synthesizer V aims to address these challenges by considering the following factors:</p>
<p>Skill Level Disparity<br>Beginners and novice users require a simple, intuitive interface and guided workflows to get started easily.<br>Professional music producers demand advanced features, customization options, and fine-grained control to create high-quality content.<br>User Intent and Background<br>Fan video producers primarily focus on creating music for their fan-made content and may have limited formal music training.<br>Official music producers employed by companies require industry-standard tools and professional-grade features to meet their clients’ expectations.<br>Workflow Preferences<br>Traditional synthesizer users prefer manual parameter adjustment and hands-on control over the music creation process.<br>New users, attracted by AI-powered tools like Synthesizer V, seek automated and AI-assisted music production workflows to simplify the process.<br>Design Objectives<br>To address these challenges, the interface of Synthesizer V should focuses on three key design objectives:</p>
<p>User-Friendly Interface: The interface should be intuitive and easy to use, allowing beginners and non-professional music producers to navigate and operate the software effortlessly.</p>
<p>Adherence to Standard Workflow: The overall process of music and video editing should follow the industry-standard workflow, maintaining consistency with mainstream software.</p>
<p>Streamlined UI with Advanced Features: Keep the interface clean and clutter-free by hiding technical details and advanced features from most users. However, ensure that professional users can easily access these features when needed.</p>
<p>UI Advantages<br>Synthesizer V boasts several UI advantages that set it apart from other synthesizers:</p>
<p>Simplified and Intuitive Main Interface: Synthesizer V streamlines complex functions and parameters into a user-friendly main interface, making it accessible to users of all skill levels.</p>
<p>Folding Interface Design for Advanced Features: By employing a folding interface design, Synthesizer V can hide advanced functions and detailed parameters, keeping the main interface clean and uncluttered.</p>
<p>User-Centric Shortcut Key Settings: Synthesizer V’s shortcut key settings closely align with user habits, enhancing efficiency and usability for experienced users.</p>
<p>Areas for Improvement<br>Despite its many strengths, Synthesizer V has some areas that could be improved to better serve its users:</p>
<p>Limited File Export Options: Currently, Synthesizer V can only export generated music as WAV files or project files, restricting advanced mixing capabilities within the software. This may affect professional users’ workflows, as they may need to rely on external tools for further processing.</p>
<p>Insufficient Interfaces with External Music Platforms: The workflow for transferring singing files from Synthesizer V to video editing or music production software is not well-defined. The lack of a standardized process can lead to decreased music quality, especially for beginners who may not be familiar with the best practices for file transfer and integration.</p>
<p>Absence of Preprocessing User Interface: Synthesizer V does not provide a user interface for preprocessing raw files (e.g., music and voice samples), which may result in subpar performance and limit users’ ability to optimize input materials.</p>
<p>Future Enhancements<br>To address these limitations and further improve the user experience, Synthesizer V could consider the following enhancements:</p>
<p>Expand File Export Options: Integrate advanced mixing capabilities within Synthesizer V, allowing users to perform in-depth audio processing and mixing tasks directly in the software, reducing the need for external tools.</p>
<p>Enhance Interfaces with External Music Platforms: Develop a well-documented and standardized workflow for transferring singing files from Synthesizer V to popular video editing and music production software, such as Adobe Premiere, Final Cut Pro, Ableton Live, and FL Studio. This would help users ensure the best possible quality when integrating Synthesizer V-generated content into their projects.</p>
<p>Introduce a Preprocessing User Interface: Incorporate a dedicated preprocessing module within Synthesizer V’s user interface, allowing users to import, analyze, and optimize raw audio files (music and voice samples) before using them in their projects. This would give users more control over the quality of their input materials and ultimately lead to better output.</p>
<p>Conclusion<br>Synthesizer V is a powerful tool that has the potential to change the way music producers create and share their work. By addressing user pain points, focusing on key design objectives, and continually improving its features and workflows, Synthesizer V can become an indispensable asset for music producers of all skill levels. As the software evolves, it will undoubtedly play a significant role in shaping the future of AI-assisted music production.</p>
]]></content>
      <categories>
        <category>Articles</category>
      </categories>
      <tags>
        <tag>original</tag>
        <tag>English</tag>
        <tag>Art and Design</tag>
      </tags>
  </entry>
  <entry>
    <title>diffusion-models</title>
    <url>/2024/09/15/diffusion-models/</url>
    <content><![CDATA[<h1 id="扩散模型-Diffusion-Model-详解：直观理解、数学原理、PyTorch-实现"><a href="#扩散模型-Diffusion-Model-详解：直观理解、数学原理、PyTorch-实现" class="headerlink" title="扩散模型(Diffusion Model)详解：直观理解、数学原理、PyTorch 实现"></a><a href="https://zhouyifan.net/2023/07/07/20230330-diffusion-model/">扩散模型(Diffusion Model)详解：直观理解、数学原理、PyTorch 实现</a></h1><p>.</p>
<h1 id="DDIM-简明讲解与-PyTorch-实现：加速扩散模型采样的通用方法"><a href="#DDIM-简明讲解与-PyTorch-实现：加速扩散模型采样的通用方法" class="headerlink" title="DDIM 简明讲解与 PyTorch 实现：加速扩散模型采样的通用方法"></a><a href="https://zhouyifan.net/2023/07/07/20230702-DDIM/">DDIM 简明讲解与 PyTorch 实现：加速扩散模型采样的通用方法</a></h1><p>.</p>
<h1 id="Stable-Diffusion-解读（一）：回顾早期工作"><a href="#Stable-Diffusion-解读（一）：回顾早期工作" class="headerlink" title="Stable Diffusion 解读（一）：回顾早期工作"></a><a href="https://zhouyifan.net/2024/01/23/20230707-SD1/">Stable Diffusion 解读（一）：回顾早期工作</a></h1><p>.</p>
<h1 id="Stable-Diffusion-解读（二）：论文精读"><a href="#Stable-Diffusion-解读（二）：论文精读" class="headerlink" title="Stable Diffusion 解读（二）：论文精读"></a><a href="https://zhouyifan.net/2024/01/23/20230709-SD2/">Stable Diffusion 解读（二）：论文精读</a></h1><p>.</p>
<h1 id="Stable-Diffusion-解读（三）：原版实现及Diffusers实现源码解读"><a href="#Stable-Diffusion-解读（三）：原版实现及Diffusers实现源码解读" class="headerlink" title="Stable Diffusion 解读（三）：原版实现及Diffusers实现源码解读"></a><a href="https://zhouyifan.net/2024/01/23/20230713-SD3/">Stable Diffusion 解读（三）：原版实现及Diffusers实现源码解读</a></h1><p>.</p>
<h1 id="LoRA-在-Stable-Diffusion-中的三种应用：原理讲解与代码示例"><a href="#LoRA-在-Stable-Diffusion-中的三种应用：原理讲解与代码示例" class="headerlink" title="LoRA 在 Stable Diffusion 中的三种应用：原理讲解与代码示例"></a><a href="https://zhouyifan.net/2024/01/23/20240114-SD-LoRA/">LoRA 在 Stable Diffusion 中的三种应用：原理讲解与代码示例</a></h1><p>.</p>
<h1 id="Stable-Diffusion-中的自注意力替换技术与-Diffusers-实现"><a href="#Stable-Diffusion-中的自注意力替换技术与-Diffusers-实现" class="headerlink" title="Stable Diffusion 中的自注意力替换技术与 Diffusers 实现"></a><a href="https://zhouyifan.net/2024/01/27/20240123-SD-Attn/">Stable Diffusion 中的自注意力替换技术与 Diffusers 实现</a></h1><p>.</p>
<h1 id="Stable-Diffusion-3-论文及源码概览"><a href="#Stable-Diffusion-3-论文及源码概览" class="headerlink" title="Stable Diffusion 3 论文及源码概览"></a><a href="https://zhouyifan.net/2024/07/14/20240703-SD3/">Stable Diffusion 3 论文及源码概览</a></h1><p>.</p>
<h1 id="定制适合自己的-Diffusers-扩散模型训练脚本"><a href="#定制适合自己的-Diffusers-扩散模型训练脚本" class="headerlink" title="定制适合自己的 Diffusers 扩散模型训练脚本"></a><a href="https://zhouyifan.net/2024/07/27/20240605-diffusers-training/">定制适合自己的 Diffusers 扩散模型训练脚本</a></h1><p>.</p>
]]></content>
      <categories>
        <category>Reference</category>
      </categories>
      <tags>
        <tag>Computer vision</tag>
        <tag>Diffusion-model</tag>
        <tag>Toturial</tag>
      </tags>
  </entry>
  <entry>
    <title>Computer vision</title>
    <url>/2024/09/15/CV/</url>
    <content><![CDATA[<p>Self-learning</p>
]]></content>
      <categories>
        <category>Course Notes</category>
      </categories>
      <tags>
        <tag>Toturial</tag>
        <tag>Computer Vision</tag>
      </tags>
  </entry>
  <entry>
    <title>Neural Network and Deep learning</title>
    <url>/2023/09/15/DL/</url>
    <content><![CDATA[<p>SC4001</p>
]]></content>
      <categories>
        <category>Course Notes</category>
      </categories>
      <tags>
        <tag>Toturial</tag>
        <tag>Neural Network</tag>
        <tag>Deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Data Analytics and Mining</title>
    <url>/2023/09/15/DM/</url>
    <content><![CDATA[<p><a href="https://bainingchao.github.io/2018/09/27/%E4%B8%80%E6%AD%A5%E6%AD%A5%E6%95%99%E4%BD%A0%E8%BD%BB%E6%9D%BE%E5%AD%A6%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99Apriori%E7%AE%97%E6%B3%95/">A Apiror算法</a></p>
]]></content>
      <categories>
        <category>Course Notes</category>
      </categories>
      <tags>
        <tag>Toturial</tag>
        <tag>Deep learning</tag>
        <tag>Feature Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Data Visualization</title>
    <url>/2023/09/15/DV/</url>
    <content><![CDATA[<p>SC4024</p>
]]></content>
      <categories>
        <category>Course Notes</category>
      </categories>
      <tags>
        <tag>Toturial</tag>
        <tag>Data Visualization</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning</title>
    <url>/2023/09/15/ML/</url>
    <content><![CDATA[<p>SC4000</p>
]]></content>
      <categories>
        <category>Course Notes</category>
      </categories>
      <tags>
        <tag>Toturial</tag>
        <tag>Machine Learning</tag>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title>Nature Language Processing</title>
    <url>/2023/09/15/NLP/</url>
    <content><![CDATA[<p>SC4002</p>
]]></content>
      <categories>
        <category>Course Notes</category>
      </categories>
      <tags>
        <tag>Toturial</tag>
        <tag>Deep learning</tag>
        <tag>Nature Language Processing</tag>
      </tags>
  </entry>
  <entry>
    <title>Dataset sharing</title>
    <url>/2024/09/15/dataset/</url>
    <content><![CDATA[<p>Bangdream Game scripts <a href="https://huggingface.co/datasets/Mahiruoshi/Bangdori">Dataset</a> is extracted from <a href="https://bestdori.com/">Bestdori</a>. It have emotional labels for ZH, JP, EN, KR and TW corpus.<br>You can obtain wav files for each characters from the voice <a href="https://huggingface.co/spaces/Mahiruoshi/BangDream-Bert-VITS2/tree/main/%E7%88%AC%E8%99%AB">datas</a> extracted from Game.</p>
]]></content>
      <tags>
        <tag>Dataset</tag>
      </tags>
  </entry>
  <entry>
    <title>Fork from [Awesome-domain-adaptation](https://github.com/zhaoxin94/awesome-domain-adaptation)</title>
    <url>/2024/10/07/domain-adaptation/</url>
    <content><![CDATA[<h1 id="awesome-domain-adaptation"><a href="#awesome-domain-adaptation" class="headerlink" title="awesome-domain-adaptation"></a>awesome-domain-adaptation</h1><p><a href="https://opensource.org/licenses/MIT"><img src="https://img.shields.io/badge/license-MIT-green.svg" alt="MIT License"></a> </p>
<p>This repo is a collection of AWESOME things about domain adaptation, including papers, code, etc. Feel free to star and fork.</p>
<h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><ul>
<li><a href="#awesome-domain-adaptation">awesome-domain-adaptation</a></li>
<li><a href="#contents">Contents</a></li>
<li><a href="#papers">Papers</a><ul>
<li><a href="#survey">Survey</a></li>
<li><a href="#theory">Theory</a></li>
<li><a href="#explainable">Explainable</a></li>
<li><a href="#unsupervised-da">Unsupervised DA</a><ul>
<li><a href="#adversarial-methods">Adversarial Methods</a></li>
<li><a href="#distance-based-methods">Distance-based Methods</a></li>
<li><a href="#information-based-methods">Information-based Methods</a></li>
<li><a href="#optimal-transport">Optimal Transport</a></li>
<li><a href="#incremental-methods">Incremental Methods</a></li>
<li><a href="#semi-supervised-learning-based-methods">Semi-Supervised-Learning-Based Methods</a></li>
<li><a href="#self-training-based-methods">Self-training-Based Methods</a></li>
<li><a href="#self-supervised-methods">Self-Supervised Methods</a></li>
<li><a href="#transformer-based-methods">Transformer-based Methods</a></li>
<li><a href="#other-methods">Other Methods</a></li>
</ul>
</li>
<li><a href="#semi-supervised-da">Semi-supervised DA</a></li>
<li><a href="#weakly-supervised-da">Weakly-Supervised DA</a></li>
<li><a href="#zero-shot-da">Zero-shot DA</a></li>
<li><a href="#one-shot-da">One-shot DA</a></li>
<li><a href="#few-shot-uda">Few-shot UDA</a></li>
<li><a href="#few-shot-da">Few-shot DA</a></li>
<li><a href="#partial-da">Partial DA</a></li>
<li><a href="#open-set-da">Open Set DA</a></li>
<li><a href="#universal-da">Universal DA</a></li>
<li><a href="#open-compound-da">Open Compound DA</a></li>
<li><a href="#multi-source-da">Multi Source DA</a></li>
<li><a href="#multi-target-da">Multi Target DA</a></li>
<li><a href="#incremental-da">Incremental DA</a></li>
<li><a href="#multi-step-da">Multi Step DA</a></li>
<li><a href="#heterogeneous-da">Heterogeneous DA</a></li>
<li><a href="#target-agnostic-da">Target-agnostic DA</a></li>
<li><a href="#federated-da">Federated DA</a></li>
<li><a href="#continuously-indexed-da">Continuously Indexed DA</a></li>
<li><a href="#source-free-da">Source Free DA</a></li>
<li><a href="#active-da">Active DA</a></li>
<li><a href="#generalized-domain-adaptation">Generalized Domain Adaptation</a></li>
<li><a href="#model-selection">Model Selection</a></li>
<li><a href="#other-transfer-learning-paradigms">Other Transfer Learning Paradigms</a><ul>
<li><a href="#domain-generalization">Domain Generalization</a></li>
<li><a href="#domain-randomization">Domain Randomization</a></li>
<li><a href="#transfer-metric-learning">Transfer Metric Learning</a></li>
<li><a href="#knowledge-transfer">Knowledge Transfer</a></li>
<li><a href="#others">Others</a></li>
</ul>
</li>
<li><a href="#applications">Applications</a><ul>
<li><a href="#object-detection">Object Detection</a></li>
<li><a href="#semantic-segmentation">Semantic Segmentation</a></li>
<li><a href="#person-re-identification">Person Re-identification</a></li>
<li><a href="#sim-to-real-transfer">Sim-to-Real Transfer</a></li>
<li><a href="#video-domain-adaptation">Video Domain Adaptation</a></li>
<li><a href="#medical-related">Medical Related</a></li>
<li><a href="#monocular-depth-estimation">Monocular Depth Estimation</a></li>
<li><a href="#3d">3D</a></li>
<li><a href="#fine-grained-domain">Fine-Grained Domain</a></li>
<li><a href="#lidar">LiDAR</a></li>
<li><a href="#remote-sensing">Remote Sensing</a></li>
<li><a href="#others-1">Others</a></li>
</ul>
</li>
<li><a href="#benchmarks">Benchmarks</a></li>
</ul>
</li>
<li><a href="#library">Library</a></li>
<li><a href="#lectures-and-tutorials">Lectures and Tutorials</a></li>
<li><a href="#other-resources">Other Resources</a></li>
</ul>
<h1 id="Papers"><a href="#Papers" class="headerlink" title="Papers"></a>Papers</h1><h2 id="Survey"><a href="#Survey" class="headerlink" title="Survey"></a>Survey</h2><p><strong>Arxiv</strong></p>
<ul>
<li>Video Unsupervised Domain Adaptation with Deep Learning: A Comprehensive Survey <a href="https://arxiv.org/abs/2211.10412">[17 Nov 2022]</a> <a href="https://github.com/xuyu0010/awesome-video-domain-adaptation">[project]</a></li>
<li>A Survey on Deep Domain Adaptation for LiDAR Perception <a href="https://arxiv.org/abs/2106.02377">[7 Jun 2021]</a></li>
<li>A Comprehensive Survey on Transfer Learning <a href="https://arxiv.org/abs/1911.02685">[7 Nov 2019]</a></li>
<li>Transfer Adaptation Learning: A Decade Survey <a href="https://arxiv.org/abs/1903.04687">[12 Mar 2019]</a></li>
<li>A review of single-source unsupervised domain adaptation <a href="https://arxiv.org/abs/1901.05335">[16 Jan 2019]</a></li>
<li>An introduction to domain adaptation and transfer learning <a href="https://arxiv.org/abs/1812.11806v2">[31 Dec 2018]</a></li>
<li>A Survey of Unsupervised Deep Domain Adaptation <a href="https://arxiv.org/abs/1812.02849v2">[6 Dec 2018]</a></li>
<li>Transfer Learning for Cross-Dataset Recognition: A Survey <a href="https://sci-hub.tw/https://arxiv.org/abs/1705.04396">[2017]</a></li>
<li>Domain Adaptation for Visual Applications: A Comprehensive Survey  <a href="https://arxiv.org/abs/1702.05374">[2017]</a></li>
</ul>
<p><strong>Journal</strong></p>
<ul>
<li>Survey on Unsupervised Domain Adaptation for Semantic Segmentation for Visual Perception in Automated Driving <a href="https://ieeexplore.ieee.org/document/10128983">[IEEE Access 2023]</a></li>
<li>A Review of Single-Source Deep Unsupervised Visual Domain Adaptation <a href="https://arxiv.org/pdf/2009.00155.pdf">[TNNLS 2020]</a></li>
<li>Deep Visual Domain Adaptation: A Survey <a href="https://arxiv.org/abs/1802.03601v4">[Neurocomputing 2018]</a></li>
<li>A Survey on Deep Transfer Learning <a href="https://arxiv.org/abs/1808.01974v1">[ICANN2018]</a></li>
<li>Visual domain adaptation: A survey of recent advances <a href="https://sci-hub.tw/10.1109/msp.2014.2347059">[2015]</a></li>
</ul>
<h2 id="Theory"><a href="#Theory" class="headerlink" title="Theory"></a>Theory</h2><p><strong>Arxiv</strong></p>
<ul>
<li>A Theory of Label Propagation for Subpopulation Shift <a href="https://arxiv.org/abs/2102.11203">[22 Feb 2021]</a></li>
<li>A General Upper Bound for Unsupervised Domain Adaptation <a href="https://arxiv.org/abs/1910.01409">[3 Oct 2019]</a></li>
<li>On Deep Domain Adaptation: Some Theoretical Understandings <a href="https://arxiv.org/abs/1811.06199">[arXiv 15 Nov 2018]</a></li>
</ul>
<p><strong>Conference</strong></p>
<ul>
<li>Domain Adaptation with Conditional Distribution Matching and Generalized Label Shift <a href="https://arxiv.org/abs/2003.04475">[NeurIPS 2020]</a></li>
<li>Bridging Theory and Algorithm for Domain Adaptation <a href="http://proceedings.mlr.press/v97/zhang19i/zhang19i.pdf">[ICML2019]</a> <a href="https://github.com/thuml/MDD">[Pytorch]</a></li>
<li>On Learning Invariant Representation for Domain Adaptation <a href="https://arxiv.org/abs/1901.09453v1">[ICML2019]</a> <a href="https://github.com/KeiraZhao/On-Learning-Invariant-Representations-for-Domain-Adaptation">[code]</a></li>
<li>Unsupervised Domain Adaptation Based on Source-guided Discrepancy <a href="https://arxiv.org/abs/1809.03839">[AAAI2019]</a></li>
<li>Learning Bounds for Domain Adaptation <a href="http://papers.nips.cc/paper/3212-learning-bounds-for-domain-adaptation">[NIPS2007]</a></li>
<li>Analysis of Representations for Domain Adaptation <a href="https://papers.nips.cc/paper/2983-analysis-of-representations-for-domain-adaptation">[NIPS2006]</a></li>
</ul>
<p><strong>Journal</strong></p>
<ul>
<li>On a Regularization of Unsupervised Domain Adaptation in RKHS <a href="https://www.sciencedirect.com/science/article/abs/pii/S1063520321001032?via=ihub">[ACHA2021]</a></li>
<li>Unsupervised Multi-Class Domain Adaptation: Theory, Algorithms, and Practice <a href="https://arxiv.org/abs/2002.08681">[TPAMI2020]</a> <a href="https://github.com/YBZh/MultiClassDA">[PyTroch]</a></li>
<li>On generalization in moment-based domain adaptation <a href="https://link.springer.com/article/10.1007/s10472-020-09719-x">[AMAI2020]</a></li>
<li>A theory of learning from different domains <a href="https://link.springer.com/content/pdf/10.1007%2Fs10994-009-5152-4.pdf">[ML2010]</a></li>
</ul>
<h2 id="Explainable"><a href="#Explainable" class="headerlink" title="Explainable"></a>Explainable</h2><p><strong>Conference</strong></p>
<ul>
<li>Visualizing Adapted Knowledge in Domain Transfer <a href="https://arxiv.org/abs/2104.10602">[CVPR2021]</a> <a href="https://github.com/hou-yz/DA_visualization">[Pytorch]</a></li>
</ul>
<h2 id="Unsupervised-DA"><a href="#Unsupervised-DA" class="headerlink" title="Unsupervised DA"></a>Unsupervised DA</h2><h3 id="Adversarial-Methods"><a href="#Adversarial-Methods" class="headerlink" title="Adversarial Methods"></a>Adversarial Methods</h3><p><strong>Conference</strong></p>
<ul>
<li>SPA: A Graph Spectral Alignment Perspective for Domain Adaptation <a href="https://arxiv.org/abs/2310.17594">[NeurIPS 2023]</a> <a href="https://github.com/CrownX/SPA">[Pytorch]</a></li>
<li>Reusing the Task-specific Classifier as a Discriminator: Discriminator-free Adversarial Domain Adaptation <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Reusing_the_Task-Specific_Classifier_as_a_Discriminator_Discriminator-Free_Adversarial_Domain_CVPR_2022_paper.pdf">[CVPR2022]</a> <a href="https://github.com/xiaoachen98/DALN">[Pytorch]</a></li>
<li>A Closer Look at Smoothness in Domain Adversarial Training <a href="https://arxiv.org/abs/2206.08213">[ICML2022]</a> <a href="https://github.com/val-iisc/SDAT">[Pytorch]</a></li>
<li>ToAlign: Task-oriented Alignment for Unsupervised Domain Adaptation <a href="https://arxiv.org/abs/2004.01888">[NeurIPS2021]</a> <a href="https://github.com/microsoft/UDA">[Pytorch]</a></li>
<li>Adversarial Unsupervised Domain Adaptation With Conditional and Label Shift: Infer, Align and Iterate <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Adversarial_Unsupervised_Domain_Adaptation_With_Conditional_and_Label_Shift_Infer_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Gradient Distribution Alignment Certificates Better Adversarial Domain Adaptation <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Gao_Gradient_Distribution_Alignment_Certificates_Better_Adversarial_Domain_Adaptation_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Re-energizing Domain Discriminator with Sample Relabeling<br>for Adversarial Domain Adaptation <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Jin_Re-Energizing_Domain_Discriminator_With_Sample_Relabeling_for_Adversarial_Domain_Adaptation_ICCV_2021_paper.pdf">[ICCV2021]</a></li>
<li>Cross-Domain Gradient Discrepancy Minimization for Unsupervised Domain Adaptation <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Du_Cross-Domain_Gradient_Discrepancy_Minimization_for_Unsupervised_Domain_Adaptation_CVPR_2021_paper.pdf">[CVPR2021]</a> <a href="https://github.com/lijin118/CGDM">[Pytorch]</a></li>
<li>MetaAlign: Coordinating Domain Alignment and Classification for Unsupervised Domain Adaptation <a href="https://arxiv.org/abs/2103.13575">[CVPR2021]</a> <a href="https://github.com/microsoft/UDA">[Pytorch]</a></li>
<li>Self-adaptive Re-weighted Adversarial Domain Adaptation <a href="https://www.ijcai.org/Proceedings/2020/0440.pdf">[IJCAI2020]</a></li>
<li>DIRL: Domain-Invariant Reperesentation Learning Approach for Sim-to-Real Transfer <a href="https://arxiv.org/abs/2011.07589">[CoRL2020]</a> <a href="https://www.sites.google.com/view/dirl">[Project]</a></li>
<li>SSA-DA: Bi-dimensional feature alignment for cross-domain object detection <a href="https://arxiv.org/pdf/2011.07205.pdf">[ECCV Workshop 2020]</a></li>
<li>Classes Matter: A Fine-grained Adversarial Approach to Cross-domain Semantic Segmentation <a href="https://arxiv.org/abs/2007.09222">[ECCV2020]</a> <a href="https://github.com/JDAI-CV/FADA">[PyTorch]</a></li>
<li>MCAR: Adaptive object detection with dual multi-label prediction <a href="https://arxiv.org/pdf/2003.12943.pdf">[ECCV2020]</a></li>
<li>Gradually Vanishing Bridge for Adversarial Domain Adaptation <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Cui_Gradually_Vanishing_Bridge_for_Adversarial_Domain_Adaptation_CVPR_2020_paper.pdf">[CVPR2020]</a> <a href="https://github.com/cuishuhao/GVB">[Pytorch]</a></li>
<li>Implicit Class-Conditioned Domain Alignment for Unsupervised Domain Adaptation <a href="https://arxiv.org/abs/2006.04996">[ICML2020]</a> <a href="https://github.com/xiangdal/implicit_alignment">[Pytorch]</a></li>
<li>Adversarial-Learned Loss for Domain Adaptation <a href="https://arxiv.org/abs/2001.01046v1">[AAAI2020]</a></li>
<li>Structure-Aware Feature Fusion for Unsupervised Domain Adaptation <a href="https://aaai.org/Papers/AAAI/2020GB/AAAI-ChenQ.8923.pdf">[AAAI2020]</a></li>
<li>Adversarial Domain Adaptation with Domain Mixup <a href="https://arxiv.org/abs/1912.01805v1">[AAAI2020]</a> <a href="https://github.com/ChrisAllenMing/Mixup_for_UDA">[Pytorch]</a></li>
<li>Discriminative Adversarial Domain Adaptation <a href="https://arxiv.org/abs/1911.12036v1">[AAAI2020]</a> <a href="https://github.com/huitangtang/DADA-AAAI2020">[Pytorch]</a></li>
<li>Bi-Directional Generation for Unsupervised Domain Adaptation <a href="https://www.aaai.org/Papers/AAAI/2020GB/AAAI-YangG.1084.pdf">[AAAI2020]</a></li>
<li>Cross-stained Segmentation from Renal Biopsy Images Using Multi-level Adversarial Learning <a href="https://arxiv.org/abs/2002.08587">[ICASSP 2020]</a></li>
<li>Curriculum based Dropout Discriminator for Domain Adaptation <a href="https://arxiv.org/pdf/1907.10628.pdf">[BMVC2019]</a> <a href="https://delta-lab-iitk.github.io/CD3A/">[Project]</a></li>
<li>Unifying Unsupervised Domain Adaptation and Zero-Shot Visual Recognition <a href="https://arxiv.org/abs/1903.10601">[IJCNN2019]</a> <a href="https://github.com/hellowangqian/domain-adaptation-capls">[Matlab]</a></li>
<li>Transfer Learning with Dynamic Adversarial Adaptation Network <a href="https://arxiv.org/abs/1909.08184">[ICDM2019]</a></li>
<li>Joint Adversarial Domain Adaptation <a href="https://dl.acm.org/citation.cfm?id=3351070">[ACM MM2019]</a></li>
<li>Cycle-consistent Conditional Adversarial Transfer Networks <a href="https://dl.acm.org/citation.cfm?id=3350902">[ACM MM2019]</a> <a href="https://github.com/lijin118/3CATN">[Pytorch]</a></li>
<li>Learning Disentangled Semantic Representation for Domain Adaptation <a href="https://www.ijcai.org/proceedings/2019/0285.pdf">[IJCAI2019]</a> <a href="https://github.com/DMIRLAB-Group/DSR">[Tensorflow]</a></li>
<li>Transferability vs. Discriminability: Batch Spectral Penalization for Adversarial Domain Adaptation <a href="http://proceedings.mlr.press/v97/chen19i/chen19i.pdf">[ICML2019]</a> <a href="https://github.com/thuml/Batch-Spectral-Penalization">[Pytorch]</a></li>
<li>Transferable Adversarial Training: A General Approach to Adapting Deep Classifiers <a href="http://proceedings.mlr.press/v97/liu19b/liu19b.pdf">[ICML2019]</a> <a href="https://github.com/thuml/Transferable-Adversarial-Training">[Pytorch]</a></li>
<li>Drop to Adapt: Learning Discriminative Features for Unsupervised Domain Adaptation <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Lee_Drop_to_Adapt_Learning_Discriminative_Features_for_Unsupervised_Domain_Adaptation_ICCV_2019_paper.pdf">[ICCV2019]</a> <a href="https://github.com/postBG/DTA.pytorch">[PyTorch]</a></li>
<li>Cluster Alignment with a Teacher for Unsupervised Domain Adaptation <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Deng_Cluster_Alignment_With_a_Teacher_for_Unsupervised_Domain_Adaptation_ICCV_2019_paper.pdf">[ICCV2019]</a> <a href="https://github.com/thudzj/CAT">[Tensorflow]</a></li>
<li>Unsupervised Domain Adaptation via Regularized Conditional Alignment <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Cicek_Unsupervised_Domain_Adaptation_via_Regularized_Conditional_Alignment_ICCV_2019_paper.pdf">[ICCV2019]</a></li>
<li>Attending to Discriminative Certainty for Domain Adaptation <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Kurmi_Attending_to_Discriminative_Certainty_for_Domain_Adaptation_CVPR_2019_paper.pdf">[CVPR2019]</a> <a href="https://delta-lab-iitk.github.io/CADA/">[Project]</a></li>
<li>GCAN: Graph Convolutional Adversarial Network for Unsupervised Domain Adaptation <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Ma_GCAN_Graph_Convolutional_Adversarial_Network_for_Unsupervised_Domain_Adaptation_CVPR_2019_paper.pdf">[CVPR2019]</a></li>
<li>Domain-Symmetric Networks for Adversarial Domain Adaptation <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Domain-Symmetric_Networks_for_Adversarial_Domain_Adaptation_CVPR_2019_paper.pdf">[CVPR2019]</a> <a href="https://github.com/YBZh/SymNets">[Pytorch]</a></li>
<li>DLOW: Domain Flow for Adaptation and Generalization <a href="https://arxiv.org/pdf/1812.05418.pdf">[CVPR2019 Oral]</a></li>
<li>Progressive Feature Alignment for Unsupervised Domain Adaptation <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Progressive_Feature_Alignment_for_Unsupervised_Domain_Adaptation_CVPR_2019_paper.pdf">[CVPR2019]</a> <a href="https://github.com/Xiewp/PFAN">[Tensorflow]</a></li>
<li>Gotta Adapt ’Em All: Joint Pixel and Feature-Level Domain Adaptation for Recognition in the Wild <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Tran_Gotta_Adapt_Em_All_Joint_Pixel_and_Feature-Level_Domain_Adaptation_CVPR_2019_paper.pdf">[CVPR2019]</a> </li>
<li>Looking back at Labels: A Class based Domain Adaptation Technique <a href="https://arxiv.org/abs/1904.01341">[IJCNN2019]</a> <a href="https://vinodkkurmi.github.io/DiscriminatorDomainAdaptation/">[Project]</a></li>
<li>Consensus Adversarial Domain Adaptation <a href="https://aaai.org/ojs/index.php/AAAI/article/view/4552">[AAAI2019]</a></li>
<li>Transferable Attention for Domain Adaptation <a href="http://ise.thss.tsinghua.edu.cn/~mlong/doc/transferable-attention-aaai19.pdf">[AAAI2019]</a></li>
<li>Exploiting Local Feature Patterns for Unsupervised Domain Adaptation <a href="https://arxiv.org/abs/1811.05042v2">[AAAI2019]</a></li>
<li>Augmented Cyclic Adversarial Learning for Low Resource Domain Adaptation <a href="https://openreview.net/forum?id=B1G9doA9F7">[ICLR2019]</a></li>
<li>Conditional Adversarial Domain Adaptation <a href="http://papers.nips.cc/paper/7436-conditional-adversarial-domain-adaptation">[NIPS2018]</a> <a href="https://github.com/thuml/CDAN">[Pytorch(official)]</a>  <a href="https://github.com/thuml/CDAN">[Pytorch(third party)]</a></li>
<li>Semi-supervised Adversarial Learning to Generate Photorealistic Face Images of New Identities from 3D Morphable Model <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Baris_Gecer_Semi-supervised_Adversarial_Learning_ECCV_2018_paper.pdf">[ECCV2018]</a></li>
<li>Deep Adversarial Attention Alignment for Unsupervised Domain Adaptation: the Benefit of Target Expectation Maximization <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Guoliang_Kang_Deep_Adversarial_Attention_ECCV_2018_paper.pdf">[ECCV2018]</a></li>
<li>Learning Semantic Representations for Unsupervised Domain Adaptation <a href="http://proceedings.mlr.press/v80/xie18c.html">[ICML2018]</a> <a href="https://github.com/Mid-Push/Moving-Semantic-Transfer-Network">[TensorFlow(Official)]</a></li>
<li>CyCADA: Cycle-Consistent Adversarial Domain Adaptation <a href="http://proceedings.mlr.press/v80/hoffman18a.html">[ICML2018]</a> <a href="https://github.com/jhoffman/cycada_release">[Pytorch(official)]</a></li>
<li>From source to target and back: Symmetric Bi-Directional Adaptive GAN <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Russo_From_Source_to_CVPR_2018_paper.pdf">[CVPR2018]</a> <a href="https://github.com/engharat/SBADAGAN">[Keras(Official)]</a> <a href="https://github.com/naoto0804/pytorch-SBADA-GAN">[Pytorch]</a></li>
<li>Detach and Adapt: Learning Cross-Domain Disentangled Deep Representation <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Detach_and_Adapt_CVPR_2018_paper.pdf">[CVPR2018]</a> <a href="https://github.com/ycliu93/CDRD">[Tensorflow]</a></li>
<li>Maximum Classifier Discrepancy for Unsupervised Domain Adaptation <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Saito_Maximum_Classifier_Discrepancy_CVPR_2018_paper.pdf">[CVPR2018]</a> <a href="https://github.com/mil-tokyo/MCD_DA">[Pytorch(Official)]</a></li>
<li>Adversarial Feature Augmentation for Unsupervised Domain Adaptation <a href="https://arxiv.org/abs/1711.08561">[CVPR2018]</a> <a href="https://github.com/ricvolpi/adversarial-feature-augmentation">[TensorFlow(Official)]</a></li>
<li>Duplex Generative Adversarial Network for Unsupervised Domain Adaptation <a href="http://vipl.ict.ac.cn/uploadfile/upload/2018041610083083.pdf">[CVPR2018]</a> <a href="http://vipl.ict.ac.cn/view_database.php?id=6">[Pytorch(Official)]</a></li>
<li>Generate To Adapt: Aligning Domains using Generative Adversarial Networks <a href="https://arxiv.org/abs/1704.01705">[CVPR2018]</a> <a href="https://github.com/yogeshbalaji/Generate_To_Adapt">[Pytorch(Official)]</a></li>
<li>Image to Image Translation for Domain Adaptation <a href="https://arxiv.org/abs/1712.00479">[CVPR2018]</a></li>
<li>Unsupervised Domain Adaptation with Similarity Learning <a href="https://arxiv.org/abs/1711.08995">[CVPR2018]</a></li>
<li>Conditional Generative Adversarial Network for Structured Domain Adaptation <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Hong_Conditional_Generative_Adversarial_CVPR_2018_paper.pdf">[CVPR2018]</a> </li>
<li>Collaborative and Adversarial Network for Unsupervised Domain Adaptation <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Collaborative_and_Adversarial_CVPR_2018_paper.pdf">[CVPR2018]</a> <a href="https://github.com/zhangweichen2006/iCAN">[Pytorch]</a></li>
<li>Re-Weighted Adversarial Adaptation Network for Unsupervised Domain Adaptation <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Re-Weighted_Adversarial_Adaptation_CVPR_2018_paper.pdf">[CVPR2018]</a></li>
<li>Multi-Adversarial Domain Adaptation <a href="http://ise.thss.tsinghua.edu.cn/~mlong/doc/multi-adversarial-domain-adaptation-aaai18.pdf">[AAAI2018]</a> <a href="https://github.com/thuml/MADA">[Caffe(Official)]</a></li>
<li>Wasserstein Distance Guided Representation Learning for Domain Adaptation <a href="https://arxiv.org/abs/1707.01217">[AAAI2018]</a> <a href="https://github.com/RockySJ/WDGRL">[TensorFlow(official)]</a> <a href="https://github.com/jvanvugt/pytorch-domain-adaptation">[Pytorch]</a></li>
<li>Incremental Adversarial Domain Adaptation for Continually Changing Environments <a href="https://arxiv.org/abs/1712.07436">[ICRA2018]</a></li>
<li>Adversarial Dropout Regularization <a href="https://openreview.net/forum?id=HJIoJWZCZ">[ICLR2018]</a></li>
<li>A DIRT-T Approach to Unsupervised Domain Adaptation <a href="https://openreview.net/forum?id=H1q-TM-AW">[ICLR2018 Poster]</a> <a href="https://github.com/RuiShu/dirt-t">[Tensorflow(Official)]</a></li>
<li>Label Efficient Learning of Transferable Representations acrosss Domains and Tasks <a href="http://vision.stanford.edu/pdf/luo2017nips.pdf">[NIPS2017]</a> <a href="http://alan.vision/nips17_website/">[Project]</a></li>
<li>Adversarial Discriminative Domain Adaptation <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper.pdf">[CVPR2017]</a>  <a href="https://github.com/erictzeng/adda">[Tensorflow(Official)]</a> <a href="https://github.com/corenel/pytorch-adda">[Pytorch]</a></li>
<li>Unsupervised Pixel–Level Domain Adaptation with Generative Adversarial Networks <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Bousmalis_Unsupervised_Pixel-Level_Domain_CVPR_2017_paper.pdf">[CVPR2017]</a> <a href="https://github.com/tensorflow/models/tree/master/research/domain_adaptation">[Tensorflow(Official)]</a> <a href="https://github.com/vaibhavnaagar/pixelDA_GAN">[Pytorch]</a></li>
<li>Domain Separation Networks <a href="http://papers.nips.cc/paper/6254-domain-separation-networks">[NIPS2016]</a></li>
<li>Deep Reconstruction-Classification Networks for Unsupervised Domain Adaptation <a href="https://arxiv.org/abs/1607.03516">[ECCV2016]</a></li>
<li>Domain-Adversarial Training of Neural Networks <a href="http://www.jmlr.org/papers/volume17/15-239/15-239.pdf">[JMLR2016]</a></li>
<li>Unsupervised Domain Adaptation by Backpropagation <a href="http://proceedings.mlr.press/v37/ganin15.pdf">[ICML2015]</a> <a href="https://github.com/ddtm/caffe/tree/grl">[Caffe(Official)]</a> <a href="https://github.com/shucunt/domain_adaptation">[Tensorflow]</a> <a href="https://github.com/fungtion/DANN">[Pytorch]</a></li>
</ul>
<p><strong>Journal</strong></p>
<ul>
<li>Incremental Unsupervised Domain-Adversarial Training of Neural Networks <a href="https://ieeexplore.ieee.org/document/9216604">[TNNLS 2020]</a></li>
<li>Unsupervised Multi-Class Domain Adaptation: Theory, Algorithms, and Practice <a href="https://arxiv.org/abs/2002.08681">[TPAMI2020]</a> <a href="https://github.com/YBZh/MultiClassDA">[PyTroch]</a></li>
<li>Adversarial Learning and Interpolation Consistency for Unsupervised Domain Adaptation <a href="https://ieeexplore.ieee.org/document/8913529">[IEEE ACCESS]</a></li>
<li>TarGAN: Generating target data with class labels for unsupervised domain adaptation <a href="">[Knowledge-Based Systems]</a></li>
</ul>
<p><strong>Arxiv</strong></p>
<ul>
<li>Bi-Directional Generation for Unsupervised Domain Adaptation <a href="https://arxiv.org/abs/2002.04869v1">[12 Feb 2020]</a></li>
<li>Enlarging Discriminative Power by Adding an Extra Class in Unsupervised Domain Adaptation <a href="https://arxiv.org/abs/2002.08041v1">[19 Feb 2020]</a> <a href="https://github.com/haitran14/gada">[Tensorflow]</a></li>
<li>Learning Domain Adaptive Features with Unlabeled Domain Bridges <a href="https://arxiv.org/abs/1912.05004v1">[10 Dec 2019]</a></li>
<li>Reducing Domain Gap via Style-Agnostic Networks <a href="https://arxiv.org/abs/1910.11645">[25 Oct 2019]</a></li>
<li>Generalized Domain Adaptation with Covariate and<br>Label Shift CO-ALignment <a href="https://arxiv.org/abs/1910.10320">[23 Oct 2019]</a></li>
<li>Adversarial Variational Domain Adaptation <a href="https://arxiv.org/abs/1909.11651">[25 Sep 2019]</a></li>
<li>Contrastively Smoothed Class Alignment for Unsupervised Domain Adaptation <a href="https://arxiv.org/abs/1909.05288">[arXiv 13 Sep 2019]</a></li>
<li>SALT: Subspace Alignment as an Auxiliary Learning Task for Domain Adaptation <a href="https://arxiv.org/abs/1906.04338v1">[arXiv 11 Jun 2019]</a></li>
<li>Joint Semantic Domain Alignment and Target Classifier Learning for Unsupervised Domain Adaptation <a href="https://arxiv.org/abs/1906.04053v1">[arXiv 10 Jun 2019]</a></li>
<li>Adversarial Domain Adaptation Being Aware of Class Relationships <a href="https://arxiv.org/abs/1905.11931v1">[arXiv 28 May 2019]</a></li>
<li>Domain-Invariant Adversarial Learning for Unsupervised Domain Adaption <a href="https://arxiv.org/abs/1811.12751">[arXiv 30 Nov 2018]</a></li>
<li>Unsupervised Domain Adaptation using Deep Networks with Cross-Grafted Stacks <a href="https://arxiv.org/abs/1902.06328v1">[arXiv 17 Feb 2019]</a></li>
<li>DART: Domain-Adversarial Residual-Transfer Networks for Unsupervised Cross-Domain Image Classification <a href="https://arxiv.org/abs/1812.11478">[arXiv 30 Dec 2018]</a></li>
<li>Unsupervised Domain Adaptation using Generative Models and Self-ensembling <a href="https://arxiv.org/abs/1812.00479">[arXiv 2 Dec 2018]</a></li>
<li>Domain Confusion with Self Ensembling for Unsupervised Adaptation <a href="https://arxiv.org/abs/1810.04472">[arXiv 10 Oct 2018]</a></li>
<li>Improving Adversarial Discriminative Domain Adaptation <a href="https://arxiv.org/abs/1809.03625">[arXiv 10 Sep 2018]</a></li>
<li>M-ADDA: Unsupervised Domain Adaptation with Deep Metric Learning <a href="https://arxiv.org/abs/1807.02552v1">[arXiv 6 Jul 2018]</a> <a href="https://github.com/IssamLaradji/M-ADDA">[Pytorch(official)]</a></li>
<li>Factorized Adversarial Networks for Unsupervised Domain Adaptation <a href="https://arxiv.org/abs/1806.01376v1">[arXiv 4 Jun 2018]</a></li>
<li>DiDA: Disentangled Synthesis for Domain Adaptation <a href="https://arxiv.org/abs/1805.08019v1">[arXiv 21 May 2018]</a></li>
<li>Unsupervised Domain Adaptation with Adversarial Residual Transform Networks <a href="https://arxiv.org/abs/1804.09578">[arXiv 25 Apr 2018]</a></li>
<li>Causal Generative Domain Adaptation Networks <a href="https://arxiv.org/abs/1804.04333v3">[arXiv 28 Jun 2018]</a></li>
</ul>
<h3 id="Distance-based-Methods"><a href="#Distance-based-Methods" class="headerlink" title="Distance-based Methods"></a>Distance-based Methods</h3><p><strong>Journal</strong></p>
<ul>
<li>Transferable Representation Learning with Deep Adaptation Networks <a href="https://ieeexplore.ieee.org/document/8454781">[TPAMI]</a></li>
<li>Robust unsupervised domain adaptation for neural networks via moment alignment <a href="https://www.sciencedirect.com/science/article/abs/pii/S0020025519300301">[InfSc2019]</a></li>
</ul>
<p><strong>Conference</strong></p>
<ul>
<li>Domain Conditioned Adaptation Network <a href="https://arxiv.org/abs/2005.06717">[AAAI2020]</a> <a href="https://github.com/BIT-DA/DCAN">[Pytorch]</a></li>
<li>HoMM: Higher-order Moment Matching for Unsupervised Domain Adaptation <a href="https://arxiv.org/abs/1912.11976">[AAAI2020]</a> <a href="https://github.com/chenchao666/HoMM-Master">[Tensorflow]</a></li>
<li>Normalized Wasserstein for Mixture Distributions With Applications in Adversarial Learning and Domain Adaptation <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Balaji_Normalized_Wasserstein_for_Mixture_Distributions_With_Applications_in_Adversarial_Learning_ICCV_2019_paper.pdf">[ICCV2019]</a></li>
<li>Joint Domain Alignment and Discriminative Feature Learning for Unsupervised Deep Domain Adaptation <a href="https://arxiv.org/abs/1808.09347v2">[AAAI2019]</a></li>
<li>Residual Parameter Transfer for Deep Domain Adaptation <a href="https://arxiv.org/abs/1711.07714">[CVPR2018]</a></li>
<li>Deep Asymmetric Transfer Network for Unbalanced Domain Adaptation <a href="http://media.cs.tsinghua.edu.cn/~multimedia/cuipeng/papers/DATN.pdf">[AAAI2018]</a></li>
<li>Central Moment Discrepancy for Unsupervised Domain Adaptation <a href="https://openreview.net/pdf?id=SkB-_mcel">[ICLR2017]</a>, <a href="https://arxiv.org/pdf/1711.06114.pdf">[InfSc2019]</a>, <a href="https://github.com/wzell/cmd">[code]</a></li>
<li>Deep CORAL: Correlation Alignment for Deep Domain Adaptation <a href="https://arxiv.org/abs/1607.01719">[ECCV2016]</a></li>
<li>Learning Transferable Features with Deep Adaptation Networks <a href="http://ise.thss.tsinghua.edu.cn/~mlong/doc/deep-adaptation-networks-icml15.pdf">[ICML2015]</a><a href="https://github.com/thuml/DAN">[code]</a></li>
<li>Unsupervised Domain Adaptation with Residual Transfer Networks <a href="http://ise.thss.tsinghua.edu.cn/~mlong/doc/residual-transfer-network-nips16.pdf">[NIPS2016]</a> <a href="https://github.com/thuml/Xlearn">[code]</a></li>
<li>Deep Transfer Learning with Joint Adaptation Networks <a href="http://ise.thss.tsinghua.edu.cn/~mlong/doc/joint-adaptation-networks-icml17.pdf">[ICML2017]</a> <a href="https://github.com/thuml/Xlearn">[code]</a></li>
</ul>
<p><strong>Arxiv</strong></p>
<ul>
<li>Deep Domain Confusion: Maximizing for Domain Invariance <a href="https://arxiv.org/abs/1412.3474">[Arxiv 2014]</a></li>
</ul>
<h3 id="Information-based-Methods"><a href="#Information-based-Methods" class="headerlink" title="Information-based Methods"></a>Information-based Methods</h3><ul>
<li>Hypothesis Disparity Regularized Mutual Information Maximization <a href="https://arxiv.org/abs/2012.08072">[AAAI2021]</a></li>
</ul>
<h3 id="Optimal-Transport"><a href="#Optimal-Transport" class="headerlink" title="Optimal Transport"></a>Optimal Transport</h3><p><strong>Conference</strong></p>
<ul>
<li>Global-Local Regularization Via Distributional Robustness <a href="https://arxiv.org/abs/2203.00553">[AISTATS2023]</a> <a href="https://github.com/VietHoang1512/GLOT/">[Pytorch]</a></li>
<li>MOST: Multi-Source Domain Adaptation via Optimal Transport for Student-Teacher Learning <a href="https://auai.org/uai2021/pdf/uai2021.106.pdf">[UAI2021]</a></li>
<li>LAMDA: Label Matching Deep Domain Adaptation <a href="http://proceedings.mlr.press/v139/le21a.html">[ICML2021]</a></li>
<li>TIDOT: A Teacher Imitation Learning Approach for Domain Adaptation with Optimal Transport <a href="https://www.ijcai.org/proceedings/2021/0394.pdf">[IJCAI2021]</a> </li>
<li>Unbalanced minibatch Optimal Transport; applications to Domain Adaptation <a href="https://arxiv.org/abs/2103.03606">[ICML2021]</a> <a href="https://github.com/kilianFatras/JUMBOT">[Pytorch]</a></li>
<li>Graph Optimal Transport for Cross-Domain Alignment <a href="https://proceedings.icml.cc/static/paper_files/icml/2020/971-Paper.pdf">[ICML2020]</a></li>
<li>Margin-aware Adversarial Domain Adaptation with Optimal Transport <a href="https://proceedings.icml.cc/static/paper_files/icml/2020/2666-Paper.pdf">[ICML2020]</a> <a href="https://github.com/sofiendhouib/MADAOT">[code]</a></li>
<li>Metric Learning in Optimal Transport for Domain Adaptation <a href="https://www.ijcai.org/Proceedings/2020/0299.pdf">[IJCAI2020]</a></li>
<li>Reliable Weighted Optimal Transport for Unsupervised Domain Adaptation <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_Reliable_Weighted_Optimal_Transport_for_Unsupervised_Domain_Adaptation_CVPR_2020_paper.pdf">[CVPR2020]</a></li>
<li>Enhanced Transport Distance for Unsupervised Domain Adaptation <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Enhanced_Transport_Distance_for_Unsupervised_Domain_Adaptation_CVPR_2020_paper.pdf">[CVPR2020]</a> <a href="https://github.com/yimzhai3/ETD">[Pytorch]</a></li>
<li>Differentially Private Optimal Transport: Application to Domain Adaptation <a href="https://www.ijcai.org/proceedings/2019/0395.pdf">[IJCAI2019]</a></li>
<li>DeepJDOT: Deep Joint distribution optimal transport for unsupervised domain adaptation <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Bharath_Bhushan_Damodaran_DeepJDOT_Deep_Joint_ECCV_2018_paper.pdf">[ECCV2018]</a> <a href="https://github.com/bbdamodaran/deepJDOT">[Keras]</a></li>
<li>Joint Distribution Optimal Transportation for Domain Adaptation <a href="http://papers.nips.cc/paper/6963-joint-distribution-optimal-transportation-for-domain-adaptation.pdf">[NIPS2017]</a> <a href="https://github.com/rflamary/JDOT">[python]</a> <a href="https://github.com/rflamary/POT">[Python Optimal Transport Library]</a></li>
</ul>
<p><strong>Arxiv</strong></p>
<ul>
<li>CDOT: Continuous Domain Adaptation using Optimal Transport <a href="https://arxiv.org/abs/1909.11448">[20 Sep 2019]</a></li>
</ul>
<h3 id="Incremental-Methods"><a href="#Incremental-Methods" class="headerlink" title="Incremental Methods"></a>Incremental Methods</h3><ul>
<li>Incremental Unsupervised Domain-Adversarial Training of Neural Networks <a href="https://ieeexplore.ieee.org/document/9216604">[TNNLS 2020]</a></li>
</ul>
<h3 id="Semi-Supervised-Learning-Based-Methods"><a href="#Semi-Supervised-Learning-Based-Methods" class="headerlink" title="Semi-Supervised-Learning-Based Methods"></a>Semi-Supervised-Learning-Based Methods</h3><ul>
<li>Label Propagation with Augmented Anchors: A Simple Semi-Supervised Learning baseline for Unsupervised Domain Adaptation <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490749.pdf">[ECCV2020]</a></li>
<li>Semi-supervised Models are Strong Unsupervised Domain Adaptation Learners <a href="(https://arxiv.org/pdf/2106.00417.pdf)">[arXiv 2021]</a><a href="https://github.com/YBZh/Bridging_UDA_SSL">[Pytorch]</a></li>
</ul>
<h3 id="Self-training-Based-Methods"><a href="#Self-training-Based-Methods" class="headerlink" title="Self-training-Based Methods"></a>Self-training-Based Methods</h3><ul>
<li>Cycle Self-Training for Domain Adaptation <a href="https://proceedings.neurips.cc/paper/2021/hash/c1fea270c48e8079d8ddf7d06d26ab52-Abstract.html">[NeurIPS2021]</a></li>
<li>Meta Self-Learning for Multi-Source Domain Adaptation: A Benchmark <a href="https://arxiv.org/abs/2108.10840">[ICCV Workshop 2021]</a> <a href="https://github.com/bupt-ai-cz/Meta-SelfLearning">[Pytorch]</a></li>
<li>Instance Adaptive Self-Training for Unsupervised Domain Adaptation <a href="https://arxiv.org/abs/2008.12197">[ECCV 2020]</a> <a href="https://github.com/bupt-ai-cz/IAST-ECCV2020">[Pytorch]</a></li>
<li>Self-training Avoids Using Spurious Features Under Domain Shift <a href="https://arxiv.org/abs/2006.10032">[NeurIPS 2020]</a></li>
<li>Two-phase Pseudo Label Densification for Self-training based Domain Adaptation <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580528.pdf">[ECCV2020]</a></li>
</ul>
<p><strong>Arxiv</strong></p>
<ul>
<li>Probabilistic Contrastive Learning for Domain Adaptation <a href="https://arxiv.org/abs/2111.06021">[arXiv 20211]</a> <a href="https://github.com/ljjcoder/Probabilistic-Contrastive-Learning">[Pytorch]</a></li>
<li>Gradual Domain Adaptation via Self-Training of Auxiliary Models<a href="https://arxiv.org/pdf/2106.09890.pdf">[arXiv 2021]</a><a href="https://github.com/YBZh/AuxSelfTrain">[Pytorch]</a></li>
</ul>
<h3 id="Self-Supervised-Methods"><a href="#Self-Supervised-Methods" class="headerlink" title="Self-Supervised Methods"></a>Self-Supervised Methods</h3><p><strong>Conference</strong></p>
<ul>
<li>Self-Supervised CycleGAN for Object-Preserving Image-to-Image Domain Adaptation <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123650494.pdf">[ECCV2020]</a></li>
</ul>
<p><strong>Arxiv</strong></p>
<ul>
<li>Unsupervised Domain Adaptation through Self-Supervision <a href="https://arxiv.org/abs/1909.11825">[arXiv 26 Sep 2019]</a></li>
</ul>
<h3 id="Transformer-based-Methods"><a href="#Transformer-based-Methods" class="headerlink" title="Transformer-based Methods"></a>Transformer-based Methods</h3><p><strong>Conference</strong></p>
<ul>
<li>Safe Self-Refinement for Transformer-Based Domain Adaptation <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_Safe_Self-Refinement_for_Transformer-Based_Domain_Adaptation_CVPR_2022_paper.pdf">[CVPR2022]</a> [<a href="https://github.com/tsun/SSRT">Pytorch</a></li>
</ul>
<h3 id="Other-Methods"><a href="#Other-Methods" class="headerlink" title="Other Methods"></a>Other Methods</h3><p><strong>Conference</strong></p>
<ul>
<li>Prior Knowledge Guided Unsupervised Domain Adaptation <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136930628.pdf">[ECCV2022]</a> <a href="https://github.com/tsun/KUDA">[Pytorch]</a></li>
<li>Revisiting Unsupervised Domain Adaptation Models: a Smoothness Perspective <a href="https://openaccess.thecvf.com/content/ACCV2022/html/Wang_Revisiting_Unsupervised_Domain_Adaptation_Models_a_Smoothness_Perspective_ACCV_2022_paper.html">[ACCV2022]</a> <a href="https://github.com/Wang-Xiaodong1899/LeCo_UDA">[Pytorch]</a></li>
<li>Reducing the Covariate Shift by Mirror Samples in Cross Domain Alignment <a href="https://proceedings.neurips.cc/paper/2021/hash/4f284803bd0966cc24fa8683a34afc6e-Abstract.html">[NeurIPS2021]</a></li>
<li>Pareto Domain Adaptation <a href="https://proceedings.neurips.cc/paper/2021/hash/6ba3af5d7b2790e73f0de32e5c8c1798-Abstract.html">[NeurIPS2021]</a></li>
<li>ToAlign: Task-Oriented Alignment for Unsupervised Domain Adaptation <a href="https://proceedings.neurips.cc/paper/2021/hash/731c83db8d2ff01bdc000083fd3c3740-Abstract.html">[NeurIPS2021]</a></li>
<li>A Prototype-Oriented Framework for Unsupervised Domain Adaptation <a href="https://proceedings.neurips.cc/paper/2021/hash/8edd72158ccd2a879f79cb2538568fdc-Abstract.html">[NeurIPS2021]</a></li>
<li>Understanding the Limits of Unsupervised Domain Adaptation via Data Poisoning <a href="https://proceedings.neurips.cc/paper/2021/hash/90cc440b1b8caa520c562ac4e4bbcb51-Abstract.html">[NeurIPS2021]</a></li>
<li>SENTRY: Selective Entropy Optimization via Committee Consistency for Unsupervised Domain Adaptation <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Prabhu_SENTRY_Selective_Entropy_Optimization_via_Committee_Consistency_for_Unsupervised_Domain_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Transporting Causal Mechanisms for Unsupervised Domain Adaptation <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Yue_Transporting_Causal_Mechanisms_for_Unsupervised_Domain_Adaptation_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Semantic Concentration for Domain Adaptation <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Li_Semantic_Concentration_for_Domain_Adaptation_ICCV_2021_paper.html">[ICCV2021]</a> </li>
<li>FixBi: Bridging Domain Spaces for Unsupervised Domain Adaptation <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Na_FixBi_Bridging_Domain_Spaces_for_Unsupervised_Domain_Adaptation_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>Domain Adaptation With Auxiliary Target Domain-Oriented Classifier <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Liang_Domain_Adaptation_With_Auxiliary_Target_Domain-Oriented_Classifier_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>Conditional Bures Metric for Domain Adaptation <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Luo_Conditional_Bures_Metric_for_Domain_Adaptation_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>DRANet: Disentangling Representation and Adaptation Networks for Unsupervised Cross-Domain Adaptation <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Lee_DRANet_Disentangling_Representation_and_Adaptation_Networks_for_Unsupervised_Cross-Domain_Adaptation_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>Visualizing Adapted Knowledge in Domain Transfer <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Hou_Visualizing_Adapted_Knowledge_in_Domain_Transfer_CVPR_2021_paper.pdf">[CVPR2021]</a> <a href="https://github.com/hou-yz/DA_visualization">[Pytorch]</a></li>
<li>Instance Level Affinity-Based Transfer for Unsupervised Domain Adaptation <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Sharma_Instance_Level_Affinity-Based_Transfer_for_Unsupervised_Domain_Adaptation_CVPR_2021_paper.pdf">[CVPR2021]</a> <a href="https://github.com/astuti/ILA-DA">[code coming soon]</a></li>
<li>Dynamic Domain Adaptation for Efficient Inference <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Dynamic_Domain_Adaptation_for_Efficient_Inference_CVPR_2021_paper.pdf">[CVPR2021]</a> <a href="https://github.com/BIT-DA/DDA">[Pytorch]</a></li>
<li>Transferable Semantic Augmentation for Domain Adaptation <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Transferable_Semantic_Augmentation_for_Domain_Adaptation_CVPR_2021_paper.pdf">[CVPR2021]</a> <a href="https://github.com/BIT-DA/TSA">[Pytorch]</a></li>
<li>MetaAlign: Coordinating Domain Alignment and Classification for Unsupervised Domain Adaptation <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wei_MetaAlign_Coordinating_Domain_Alignment_and_Classification_for_Unsupervised_Domain_Adaptation_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>DRANet: Disentangling Representation and Adaptation Networks for Unsupervised Cross-Domain Adaptation <a href="https://arxiv.org/abs/2103.13447v1">[CVPR2021]</a></li>
<li>Dynamic Weighted Learning for Unsupervised Domain Adaptation <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Xiao_Dynamic_Weighted_Learning_for_Unsupervised_Domain_Adaptation_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>Domain Adaptation with Conditional Distribution Matching and Generalized Label Shift <a href="https://arxiv.org/abs/2003.04475">[NeurIPS 2020]</a></li>
<li>Transferable Calibration with Lower Bias and Variance in Domain Adaptation <a href="https://arxiv.org/abs/2007.08259">[NeurIPS 2020]</a></li>
<li>A Dictionary Approach to Domain-Invariant Learning in Deep Networks <a href="https://arxiv.org/abs/1909.11285">[NeurIPS 2020]</a></li>
<li>Heuristic Domain Adaptation <a href="https://arxiv.org/abs/2011.14540">[NeurIPS2020]</a> <a href="https://github.com/cuishuhao/HDA">[Pytorch]</a></li>
<li>Unsupervised Domain Adaptation for Semantic Segmentation of NIR Images through Generative Latent Search <a href="https://arxiv.org/abs/2006.08696">[ECCV2020]</a><a href="https://github.com/ambekarsameer96/GLSS">[code]</a></li>
<li>Mind the Discriminability: Asymmetric Adversarial Domain Adaptation <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123690579.pdf">[ECCV2020]</a></li>
<li>Domain2Vec: Domain Embedding for Unsupervised Domain Adaptation <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123510749.pdf">[ECCV2020]</a></li>
<li>CSCL: Critical Semantic-Consistent Learning for Unsupervised Domain Adaptation <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123530732.pdf">[ECCV2020]</a></li>
<li>Minimum Class Confusion for Versatile Domain Adaptation <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660460.pdf">[ECCV2020]</a></li>
<li>Partially-Shared Variational Auto-encoders for Unsupervised Domain Adaptation with Target Shift <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2472_ECCV_2020_paper.php">[ECCV2020]</a> <a href="https://github.com/iiyama-lab/PS-VAEs">[Pytorch]</a></li>
<li>Label Propagation with Augmented Anchors: A Simple Semi-Supervised Learning baseline for Unsupervised Domain Adaptation <a href="https://arxiv.org/pdf/2007.07695.pdf">[ECCV2020]</a> <a href="https://github.com/YBZh/Label-Propagation-with-Augmented-Anchors">[PyTorch]</a></li>
<li>Unsupervised Domain Adaptation via Structurally Regularized Deep Clustering <a href="http://arxiv.org/abs/2003.08607">[CVPR2020 Oral]</a> <a href="https://github.com/huitangtang/SRDC-CVPR2020">[Pytorch]</a></li>
<li>Towards Discriminability and Diversity: Batch Nuclear-norm Maximization under Label Insufficient Situations <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Cui_Towards_Discriminability_and_Diversity_Batch_Nuclear-Norm_Maximization_Under_Label_Insufficient_CVPR_2020_paper.pdf">[CVPR2020 Oral]</a> <a href="https://github.com/cuishuhao/BNM">[Pytorch]</a></li>
<li>Unsupervised Domain Adaptation With Hierarchical Gradient Synchronization <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Hu_Unsupervised_Domain_Adaptation_With_Hierarchical_Gradient_Synchronization_CVPR_2020_paper.pdf">[CVPR2020]</a></li>
<li>Spherical Space Domain Adaptation With Robust Pseudo-Label Loss <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Gu_Spherical_Space_Domain_Adaptation_With_Robust_Pseudo-Label_Loss_CVPR_2020_paper.pdf">[CVPR2020]</a> <a href="https://github.com/XJTU-XGU/RSDA">[Pytorch]</a></li>
<li>Stochastic Classifiers for Unsupervised Domain Adaptation <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Lu_Stochastic_Classifiers_for_Unsupervised_Domain_Adaptation_CVPR_2020_paper.pdf">[CVPR2020]</a></li>
<li>Structure Preserving Generative Cross-Domain Learning <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Xia_Structure_Preserving_Generative_Cross-Domain_Learning_CVPR_2020_paper.pdf">[CVPR2020]</a></li>
<li>Light-weight Calibrator: A Separable Component for Unsupervised Domain Adaptation <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Ye_Light-weight_Calibrator_A_Separable_Component_for_Unsupervised_Domain_Adaptation_CVPR_2020_paper.pdf">[CVPR2020]</a> <a href="https://github.com/yeshaokai/Calibrator-Domain-Adaptation">[code]</a></li>
<li>Domain Adaptive Multiflow Networks <a href="https://openreview.net/forum?id=rJxycxHKDS">[ICLR2020]</a></li>
<li>Unsupervised Domain Adaptation via Discriminative Manifold Embedding and Alignment <a href="https://arxiv.org/abs/2002.08675v1">[AAAI2020]</a></li>
<li>Visual Domain Adaptation by Consensus-based Transfer to Intermediate Domain <a href="https://aaai.org/Papers/AAAI/2020GB/AAAI-ChoiJ.3612.pdf">[Paper]</a></li>
<li>Unsupervised Domain Adaptation via Structured Prediction Based Selective Pseudo-Labeling <a href="https://arxiv.org/abs/1911.07982">[AAAI2020]</a> <a href="https://github.com/hellowangqian/domain-adaptation-capls">[Matlab]</a></li>
<li>CUDA: Contradistinguisher for Unsupervised Domain Adaptation <a href="https://arxiv.org/abs/1909.03442">[ICDM2019]</a></li>
<li>Domain Adaptation with Asymmetrically-Relaxed Distribution Alignment <a href="http://proceedings.mlr.press/v97/wu19f/wu19f.pdf">[ICML2019]</a></li>
<li>Batch Weight for Domain Adaptation With Mass Shift <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Binkowski_Batch_Weight_for_Domain_Adaptation_With_Mass_Shift_ICCV_2019_paper.pdf">[ICCV2019]</a></li>
<li>Switchable Whitening for Deep Representation Learning <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Pan_Switchable_Whitening_for_Deep_Representation_Learning_ICCV_2019_paper.pdf">[ICCV2019]</a> <a href="https://github.com/XingangPan/Switchable-Whitening">[pytorch]</a></li>
<li>Confidence Regularized Self-Training <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Zou_Confidence_Regularized_Self-Training_ICCV_2019_paper.pdf">[ICCV2019 Oral]</a> <a href="https://github.com/yzou2/CRST">[Pytorch]</a></li>
<li>Larger Norm More Transferable: An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Larger_Norm_More_Transferable_An_Adaptive_Feature_Norm_Approach_for_ICCV_2019_paper.pdf">[ICCV2019]</a> <a href="https://github.com/jihanyang/AFN">[Pytorch(official)]</a></li>
<li>Transferrable Prototypical Networks for Unsupervised Domain Adaptation <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Pan_Transferrable_Prototypical_Networks_for_Unsupervised_Domain_Adaptation_CVPR_2019_paper.pdf">[CVPR2019(Oral)]</a></li>
<li>Sliced Wasserstein Discrepancy for Unsupervised Domain Adaptation <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Lee_Sliced_Wasserstein_Discrepancy_for_Unsupervised_Domain_Adaptation_CVPR_2019_paper.pdf">[CVPR2019]</a></li>
<li>Unsupervised Domain Adaptation using Feature-Whitening and Consensus Loss <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Roy_Unsupervised_Domain_Adaptation_Using_Feature-Whitening_and_Consensus_Loss_CVPR_2019_paper.pdf">[CVPR 2019]</a>  <a href="https://github.com/roysubhankar/dwt-domain-adaptation">[Pytorch]</a></li>
<li>Domain Specific Batch Normalization for Unsupervised Domain Adaptation <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Chang_Domain-Specific_Batch_Normalization_for_Unsupervised_Domain_Adaptation_CVPR_2019_paper.pdf">[CVPR2019]</a> <a href="https://github.com/wgchang/DSBN">[Pytorch]</a></li>
<li>AdaGraph: Unifying Predictive and Continuous Domain Adaptation through Graphs <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Mancini_AdaGraph_Unifying_Predictive_and_Continuous_Domain_Adaptation_Through_Graphs_CVPR_2019_paper.pdf">[CVPR2019]</a> <a href="https://github.com/mancinimassimiliano/adagraph">[Pytorch]</a></li>
<li>Unsupervised Visual Domain Adaptation: A Deep Max-Margin Gaussian Process Approach <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Kim_Unsupervised_Visual_Domain_Adaptation_A_Deep_Max-Margin_Gaussian_Process_Approach_CVPR_2019_paper.pdf">[CVPR2019]</a> <a href="https://seqam-lab.github.io/GPDA/">[Project]</a></li>
<li>Contrastive Adaptation Network for Unsupervised Domain Adaptation <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Kang_Contrastive_Adaptation_Network_for_Unsupervised_Domain_Adaptation_CVPR_2019_paper.pdf">[CVPR2019]</a> <a href="https://github.com/kgl-prml/Contrastive-Adaptation-Network-for-Unsupervised-Domain-Adaptation">[Pytorch]</a></li>
<li>Distant Supervised Centroid Shift: A Simple and Efficient Approach to Visual Domain Adaptation <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Liang_Distant_Supervised_Centroid_Shift_A_Simple_and_Efficient_Approach_to_CVPR_2019_paper.pdf">[CVPR2019]</a></li>
<li>Unsupervised Domain Adaptation via Calibrating Uncertainties <a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/Uncertainty%20and%20Robustness%20in%20Deep%20Visual%20Learning/Han_Unsupervised_Domain_Adaptation_via_Calibrating_Uncertainties_CVPRW_2019_paper.pdf">[CVPRW2019]</a></li>
<li>Bayesian Uncertainty Matching for Unsupervised Domain Adaptation <a href="https://arxiv.org/abs/1906.09693v1">[IJCAI2019]</a></li>
<li>Unsupervised Domain Adaptation for Distance Metric Learning <a href="https://openreview.net/forum?id=BklhAj09K7">[ICLR2019]</a></li>
<li>Co-regularized Alignment for Unsupervised Domain Adaptation <a href="http://papers.nips.cc/paper/8146-co-regularized-alignment-for-unsupervised-domain-adaptation">[NIPS2018]</a></li>
<li>Domain Invariant and Class Discriminative Feature Learning for Visual Domain Adaptation <a href="https://ieeexplore.ieee.org/document/8362753/">[TIP 2018]</a></li>
<li>Graph Adaptive Knowledge Transfer for Unsupervised Domain Adaptation <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Zhengming_Ding_Graph_Adaptive_Knowledge_ECCV_2018_paper.pdf">[ECCV2018]</a></li>
<li>Aligning Infinite-Dimensional Covariance Matrices in Reproducing Kernel Hilbert Spaces for Domain Adaptation <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Aligning_Infinite-Dimensional_Covariance_CVPR_2018_paper.pdf">[CVPR2018]</a></li>
<li>Unsupervised Domain Adaptation with Distribution Matching Machines <a href="http://ise.thss.tsinghua.edu.cn/~mlong/doc/distribution-matching-machines-aaai18.pdf">[AAAI2018]</a></li>
<li>Learning to cluster in order to transfer across domains and tasks <a href="https://openreview.net/forum?id=ByRWCqvT-">[ICLR2018]</a> <a href="https://mlatgt.blog/2018/04/29/learning-to-cluster/">[Bolg]</a> <a href="https://github.com/GT-RIPL/L2C">[Pytorch]</a></li>
<li>Self-Ensembling for Visual Domain Adaptation <a href="https://openreview.net/forum?id=rkpoTaxA-">[ICLR2018]</a></li>
<li>Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation <a href="https://openreview.net/forum?id=rJWechg0Z">[ICLR2018]</a> <a href="https://github.com/pmorerio/minimal-entropy-correlation-alignment">[TensorFlow]</a></li>
<li>Associative Domain Adaptation <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Haeusser_Associative_Domain_Adaptation_ICCV_2017_paper.pdf">[ICCV2017]</a> <a href="https://github.com/haeusser/learning_by_association">[TensorFlow]</a> <a href="https://github.com/corenel/pytorch-atda">[Pytorch]</a></li>
<li>AutoDIAL: Automatic DomaIn Alignment Layers <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Carlucci_AutoDIAL_Automatic_DomaIn_ICCV_2017_paper.pdf">[ICCV2017]</a></li>
<li>Asymmetric Tri-training for Unsupervised Domain Adaptation <a href="http://proceedings.mlr.press/v70/saito17a.html">[ICML2017]</a> <a href="https://github.com/ksaito-ut/atda">[TensorFlow]</a></li>
<li>Learning Transferrable Representations for Unsupervised Domain Adaptation <a href="http://papers.nips.cc/paper/6360-learning-transferrable-representations-for-unsupervised-domain-adaptation">[NIPS2016]</a></li>
</ul>
<p><strong>Journal</strong></p>
<ul>
<li>Target-Independent Domain Adaptation for WBC Classification using Generative Latent Search <a href="https://ieeexplore.ieee.org/document/9139471">[IEEE TMI 2020]</a><a href="https://github.com/prinshul/WBC-Classification-UDA">[code]</a></li>
<li>Adaptive Batch Normalization for practical domain adaptation <a href="https://www.sciencedirect.com/science/article/pii/S003132031830092X">[Pattern Recognition(2018)]</a></li>
<li>Unsupervised Domain Adaptation by Mapped Correlation Alignment <a href="https://ieeexplore.ieee.org/abstract/document/8434290/">[IEEE ACCESS]</a></li>
</ul>
<p><strong>Arxiv</strong></p>
<ul>
<li>Low-confidence Samples Matter for Domain Adaptation <a href="https://arxiv.org/abs/2202.02802">[6 Feb 2022]</a> <a href="https://github.com/zhyx12/MixLRCo">[Pytorch]</a></li>
<li>Improving Unsupervised Domain Adaptation with Variational Information Bottleneck <a href="https://arxiv.org/abs/1911.09310v1">[21 Nov 2019]</a></li>
<li>Deep causal representation learning for unsupervised domain adaptation <a href="https://arxiv.org/abs/1910.12417">[28 Oct 2019]</a></li>
<li>Domain-invariant Learning using Adaptive Filter<br>Decomposition <a href="https://arxiv.org/abs/1909.11285">[25 Sep 2019]</a></li>
<li>Discriminative Clustering for Robust Unsupervised Domain Adaptation <a href="https://arxiv.org/abs/1905.13331">[arXiv 30 May 2019]</a></li>
<li>Virtual Mixup Training for Unsupervised Domain Adaptation <a href="https://arxiv.org/abs/1905.04215">[arXiv on 24 May 2019]</a> <a href="https://github.com/xudonmao/VMT">[Tensorflow]</a></li>
<li>Learning Smooth Representation for Unsupervised Domain Adaptation <a href="https://arxiv.org/abs/1905.10748v1">[arXiv 26 May 2019]</a></li>
<li>Towards Self-similarity Consistency and Feature Discrimination for Unsupervised Domain Adaptation <a href="https://arxiv.org/abs/1904.06490v1">[arXiv 13 Apr 2019]</a></li>
<li>Easy Transfer Learning By Exploiting Intra-domain Structures <a href="https://arxiv.org/abs/1904.01376v1">[arXiv 2 Apr 2019]</a> </li>
<li>Domain Discrepancy Measure Using Complex Models in Unsupervised Domain Adaptation <a href="https://arxiv.org/abs/1901.10654v1">[arXiv 30 Jan 2019]</a></li>
<li>Domain Alignment with Triplets <a href="https://arxiv.org/abs/1812.00893v2">[arXiv 22 Jan 2019]</a></li>
<li>Deep Discriminative Learning for Unsupervised Domain Adaptation <a href="https://arxiv.org/abs/1811.07134v1">[arXiv 17 Nov 2018]</a></li>
</ul>
<h2 id="Foundation-Models-based-DA"><a href="#Foundation-Models-based-DA" class="headerlink" title="Foundation-Models based DA"></a>Foundation-Models based DA</h2><p><strong>Conference</strong></p>
<ul>
<li>POUF: Prompt-oriented unsupervised fine-tuning for large pre-trained models <a href="https://arxiv.org/abs/2305.00350">[ICML2023]</a> <a href="https://github.com/korawat-tanwisuth/POUF">[Pytorch]</a></li>
</ul>
<h2 id="Semi-supervised-DA"><a href="#Semi-supervised-DA" class="headerlink" title="Semi-supervised DA"></a>Semi-supervised DA</h2><p><strong>Conference</strong></p>
<ul>
<li>Semi-Supervised Domain Adaptation With Source Label Adaptation <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yu_Semi-Supervised_Domain_Adaptation_With_Source_Label_Adaptation_CVPR_2023_paper.html">[CVPR 2023]</a></li>
<li>Multi-level Consistency Learning for Semi-supervised Domain Adaptation <a href="https://arxiv.org/abs/2205.04066">[IJCAI 2022]</a></li>
<li>AdaMatch: A Unified Approach to Semi-Supervised Learning and Domain Adaptation <a href="https://openreview.net/pdf?id=Q5uh1Nvv5dm">[ICLR 2022]</a></li>
<li>CLDA: Contrastive Learning for Semi-Supervised Domain Adaptation <a href="https://proceedings.neurips.cc/paper/2021/hash/288cd2567953f06e460a33951f55daaf-Abstract.html">[NeurIPS]</a></li>
<li>Deep Co-Training With Task Decomposition for Semi-Supervised Domain Adaptation <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Yang_Deep_Co-Training_With_Task_Decomposition_for_Semi-Supervised_Domain_Adaptation_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>ECACL: A Holistic Framework for Semi-Supervised Domain Adaptation <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Li_ECACL_A_Holistic_Framework_for_Semi-Supervised_Domain_Adaptation_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Cross-Domain Adaptive Clustering for Semi-Supervised Domain Adaptation <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Cross-Domain_Adaptive_Clustering_for_Semi-Supervised_Domain_Adaptation_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>Semi-supervised Domain Adaptation based on Dual-level Domain Mixing for Semantic Segmentation <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Semi-Supervised_Domain_Adaptation_Based_on_Dual-Level_Domain_Mixing_for_Semantic_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>Learning Invariant Representations and Risks for Semi-supervised Domain Adaptation <a href="https://arxiv.org/abs/2010.04647">[CVPR2021]</a></li>
<li>Improving Semi-Supervised Domain Adaptation Using Effective Target Selection and Semantics <a href="https://openaccess.thecvf.com/content/CVPR2021W/LLID/papers/Singh_Improving_Semi-Supervised_Domain_Adaptation_Using_Effective_Target_Selection_and_Semantics_CVPRW_2021_paper.pdf">[CVPRW2021]</a> <a href="https://github.com/Anurag14/STar-framework">[Code]</a></li>
<li>Attract, Perturb, and Explore: Learning a Feature Alignment Network for Semi-supervised Domain Adaptation <a href="https://arxiv.org/abs/2007.09375v1">[ECCV2020]</a></li>
<li>Online Meta-Learning for Multi-Source and Semi-Supervised Domain Adaptation <a href="https://arxiv.org/abs/2004.04398">[ECCV2020]</a></li>
<li>Bidirectional Adversarial Training for Semi-Supervised Domain Adaptation <a href="https://www.ijcai.org/Proceedings/2020/130">[IJCAI2020]</a></li>
<li>Semi-supervised Domain Adaptation via Minimax Entropy <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Saito_Semi-Supervised_Domain_Adaptation_via_Minimax_Entropy_ICCV_2019_paper.pdf">[ICCV2019]</a> <a href="https://github.com/VisionLearningGroup/SSDA_MME">[Pytorch]</a></li>
</ul>
<p><strong>Journal</strong></p>
<ul>
<li>Context-guided entropy minimization for semi-supervised domain adaptation <a href="https://doi.org/10.1016/j.neunet.2022.07.011">[Neural Networks]</a>  <a href="https://github.com/NingMa-AI/DEEM">[pytorch]</a></li>
</ul>
<p><strong>Arxiv</strong></p>
<ul>
<li>Pred&amp;Guide: Labeled Target Class Prediction for Guiding Semi-Supervised Domain Adaptation <a href="https://arxiv.org/abs/2211.11975">[22 Nov 2022]</a></li>
<li>MiCo: Mixup Co-Training for Semi-Supervised Domain Adaptation <a href="https://arxiv.org/abs/2007.12684">[ 24 Jul 2020]</a></li>
<li>Opposite Structure Learning for Semi-supervised Domain Adaptation <a href="https://arxiv.org/abs/2002.02545v1">[6 Feb 2020]</a></li>
<li>Reducing Domain Gap via Style-Agnostic Networks <a href="https://arxiv.org/abs/1910.11645">[25 Oct 2019]</a></li>
</ul>
<h2 id="Weakly-Supervised-DA"><a href="#Weakly-Supervised-DA" class="headerlink" title="Weakly-Supervised DA"></a>Weakly-Supervised DA</h2><p><strong>Conference</strong></p>
<ul>
<li>Towards Accurate and Robust Domain Adaptation under Noisy Environments <a href="https://www.ijcai.org/Proceedings/2020/0314.pdf">[IJCAI2020]</a></li>
<li>Weakly Supervised Open-set Domain Adaptation by Dual-domain Collaboration <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Tan_Weakly_Supervised_Open-Set_Domain_Adaptation_by_Dual-Domain_Collaboration_CVPR_2019_paper.pdf">[CVPR2019]</a></li>
<li>Transferable Curriculum for Weakly-Supervised Domain Adaptation <a href="http://ise.thss.tsinghua.edu.cn/~mlong/doc/transferable-curriculum-aaai19.pdf">[AAAI2019]</a></li>
</ul>
<p><strong>Arxiv</strong></p>
<ul>
<li>Butterfly: Robust One-step Approach towards Wildly-unsupervised Domain Adaptation <a href="https://arxiv.org/abs/1905.07720v1">[arXiv on 19 May 2019]</a></li>
</ul>
<h2 id="Zero-shot-DA"><a href="#Zero-shot-DA" class="headerlink" title="Zero-shot DA"></a>Zero-shot DA</h2><p><strong>Conference</strong></p>
<ul>
<li>Collaborative Learning With Disentangled Features for Zero-Shot Domain Adaptation <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Jhoo_Collaborative_Learning_With_Disentangled_Features_for_Zero-Shot_Domain_Adaptation_ICCV_2021_paper.pdf">[ICCV2021]</a></li>
<li>Zero-Shot Day-Night Domain Adaptation with a Physics Prior <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Lengyel_Zero-Shot_Day-Night_Domain_Adaptation_With_a_Physics_Prior_ICCV_2021_paper.pdf">[ICCV2021]</a></li>
<li>High Resolution Zero-Shot Domain Adaptation of Synthetically Rendered Face Images <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730222.pdf">[ECCV2020]</a></li>
<li>Adversarial Learning for Zero-shot Domain Adaptation <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660324.pdf">[ECCV2020]</a></li>
<li>HGNet: Hybrid Generative Network for Zero-shot Domain Adaptation <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123720052.pdf">[ECCV2020]</a></li>
<li>Zero-shot Domain Adaptation Based on Attribute Information <a href="http://proceedings.mlr.press/v101/ishii19a.html">[ACML2019]</a></li>
<li>Conditional Coupled Generative Adversarial Networks for Zero-Shot Domain Adaptation <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Conditional_Coupled_Generative_Adversarial_Networks_for_Zero-Shot_Domain_Adaptation_ICCV_2019_paper.pdf">[ICCV2019]</a></li>
<li>Generalized Zero-Shot Learning with Deep Calibration Network <a href="http://ise.thss.tsinghua.edu.cn/~mlong/doc/deep-calibration-network-nips18.pdf">[NIPS2018]</a></li>
<li>Zero-Shot Deep Domain Adaptation <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Kuan-Chuan_Peng_Zero-Shot_Deep_Domain_ECCV_2018_paper.pdf">[ECCV2018]</a></li>
</ul>
<h2 id="One-shot-DA"><a href="#One-shot-DA" class="headerlink" title="One-shot DA"></a>One-shot DA</h2><p><strong>Conference</strong></p>
<ul>
<li>Adversarial Style Mining for One-Shot Unsupervised Domain Adaptation <a href="https://proceedings.neurips.cc/paper/2020/hash/ed265bc903a5a097f61d3ec064d96d2e-Abstract.html">[NeurIPS2020]</a> <a href="https://github.com/RoyalVane/ASM">[Pytorch]</a></li>
<li>One-Shot Adaptation of Supervised Deep Convolutional Models <a href="https://arxiv.org/abs/1312.6204">[ICLR Workshop 2014]</a></li>
</ul>
<p><strong>Arxiv</strong></p>
<ul>
<li>One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning <a href="https://arxiv.org/abs/1802.01557">[arxiv]</a></li>
</ul>
<h2 id="Few-shot-UDA"><a href="#Few-shot-UDA" class="headerlink" title="Few-shot UDA"></a>Few-shot UDA</h2><p><strong>Conference</strong></p>
<ul>
<li>Prototypical Cross-domain Self-supervised Learning for Few-shot Unsupervised Domain Adaptation<br><a href="https://openaccess.thecvf.com/content/CVPR2021/html/Yue_Prototypical_Cross-Domain_Self-Supervised_Learning_for_Few-Shot_Unsupervised_Domain_Adaptation_CVPR_2021_paper.html">[CVPR2021]</a> <a href="https://github.com/zhengzangw/PCS-FUDA">[Pytorch]</a> <a href="http://xyue.io/pcs-fuda/">[Project]</a></li>
</ul>
<p><strong>Arxiv</strong></p>
<ul>
<li>Cross-domain Self-supervised Learning for Domain Adaptation with Few Source Labels <a href="https://arxiv.org/pdf/2003.08264.pdf">[arXiv 18 Mar 2020]</a></li>
</ul>
<h2 id="Few-shot-DA"><a href="#Few-shot-DA" class="headerlink" title="Few-shot DA"></a>Few-shot DA</h2><p><strong>Conference</strong></p>
<ul>
<li>Domain-Adaptive Few-Shot Learning<a href="https://openaccess.thecvf.com/content/WACV2021/papers/Zhao_Domain-Adaptive_Few-Shot_Learning_WACV_2021_paper.pdf">[WACV2021]</a> <a href="https://github.com/dingmyu/DAPN">[Pytorch]</a></li>
<li>Few-shot Domain Adaptation by Causal Mechanism Transfer <a href="https://proceedings.icml.cc/static/paper_files/icml/2020/1121-Paper.pdf">[ICML2020]</a> <a href="https://github.com/takeshi-teshima/few-shot-domain-adaptation-by-causal-mechanism-transfer">[Pytorch]</a></li>
<li>Few-Shot Adaptive Faster R-CNN <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Few-Shot_Adaptive_Faster_R-CNN_CVPR_2019_paper.html">[CVPR2019]</a></li>
<li>d-SNE: Domain Adaptation using Stochastic Neighborhood Embedding <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Xu_d-SNE_Domain_Adaptation_Using_Stochastic_Neighborhood_Embedding_CVPR_2019_paper.pdf">[CVPR2019 Oral]</a></li>
<li>Few-Shot Adversarial Domain Adaptation <a href="http://papers.nips.cc/paper/7244-few-shot-adversarial-domain-adaptation">[NIPS2017]</a></li>
</ul>
<p><strong>Arxiv</strong></p>
<ul>
<li>Feature transformation ensemble model with batch spectral regularization for cross-domain few-shot classification <a href="https://arxiv.org/pdf/2005.08463.pdf">[arXiv 18 May 2020]</a> <a href="https://github.com/liubingyuu/FTEM_BSR_CDFSL">[Pytorch]</a></li>
<li>Ensemble model with batch spectral regularization and data blending for cross-domain few-shot learning with unlabeled data <a href="https://arxiv.org/pdf/2006.04323.pdf">[arXiv 8 June 2020]</a> <a href="https://github.com/123zhen123/BSDB-CDFSL_Track">[Pytorch]</a></li>
</ul>
<h2 id="Partial-DA"><a href="#Partial-DA" class="headerlink" title="Partial DA"></a>Partial DA</h2><p><strong>Conference</strong></p>
<ul>
<li>Implicit Semantic Response Alignment for Partial Domain Adaptation <a href="https://proceedings.neurips.cc/paper/2021/hash/731b03008e834f92a03085ef47061c4a-Abstract.html">[NeurIPS2021]</a> <a href="https://github.com/implicit-seman-align/Implicit-Semantic-Response-Alignment">[Pytorch]</a></li>
<li>Adversarial Reweighting for Partial Domain Adaptation <a href="https://proceedings.neurips.cc/paper/2021/hash/7ce3284b743aefde80ffd9aec500e085-Abstract.html">[NeurIPS2021]</a></li>
<li>A Balanced and Uncertainty-aware Approach for Partial Domain Adaptation <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123560120.pdf">[ECCV2020]</a> <a href="https://github.com/tim-learn/BA3US">[Pytorch]</a></li>
<li>Discriminative Partial Domain Adversarial Network <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123720630.pdf">[ECCV2020]</a></li>
<li>Selective Transfer With Reinforced Transfer Network for Partial Domain Adaptation <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Selective_Transfer_With_Reinforced_Transfer_Network_for_Partial_Domain_Adaptation_CVPR_2020_paper.pdf">[CVPR2020]</a></li>
<li>Adaptively-Accumulated Knowledge Transfer for Partial Domain Adaptation <a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413986">[ACM MM2020]</a></li>
<li>Multi-Weight Partial Domain Adaptation <a href="https://bmvc2019.org/wp-content/uploads/papers/0406-paper.pdf">[BMVC2019]</a></li>
<li>Learning to Transfer Examples for Partial Domain Adaptation <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Cao_Learning_to_Transfer_Examples_for_Partial_Domain_Adaptation_CVPR_2019_paper.pdf">[CVPR2019]</a> <a href="https://github.com/thuml/ETN">[Pytorch]</a></li>
<li>Partial Adversarial Domain Adaptation <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Zhangjie_Cao_Partial_Adversarial_Domain_ECCV_2018_paper.pdf">[ECCV2018]</a> <a href="https://github.com/thuml/PADA">[Pytorch(Official)]</a></li>
<li>Importance Weighted Adversarial Nets for Partial Domain Adaptation <a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Importance_Weighted_Adversarial_CVPR_2018_paper.html">[CVPR2018]</a> <a href="https://github.com/hellojing89/weightedGANpartialDA">[Caffe]</a></li>
<li>Partial Transfer Learning with Selective Adversarial Networks <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Cao_Partial_Transfer_Learning_CVPR_2018_paper.pdf">[CVPR2018]</a><a href="http://www.paperweekly.site/papers/1388">[paper weekly]</a> <a href="https://github.com/thuml/SAN">[Pytorch(Official) &amp; Caffe(official)]</a></li>
</ul>
<p><strong>Journal</strong></p>
<ul>
<li>Unsupervised Multi-Class Domain Adaptation: Theory, Algorithms, and Practice <a href="https://arxiv.org/abs/2002.08681">[TPAMI2020]</a> <a href="https://github.com/YBZh/MultiClassDA">[PyTroch]</a></li>
</ul>
<p><strong>Arxiv</strong></p>
<ul>
<li>Select, Label, and Mix: Learning Discriminative Invariant Feature Representations for Partial Domain Adaptation <a href="https://arxiv.org/abs/2012.03358">[arXiv 06 Dec 2020]</a></li>
<li>Unsupervised Multi-Class Domain Adaptation: Theory, Algorithms, and Practice <a href="https://arxiv.org/pdf/2002.08681.pdf">[20 Feb 2020]</a> <a href="https://github.com/YBZh/MultiClassDA">[PyTroch]</a></li>
<li>Tackling Partial Domain Adaptation with Self-Supervision <a href="https://arxiv.org/abs/1906.05199v1">[arXiv 12 Jun 2019]</a></li>
<li>Domain Adversarial Reinforcement Learning for Partial Domain Adaptation <a href="https://arxiv.org/abs/1905.04094v1">[arXiv 10 May 2019]</a></li>
</ul>
<h2 id="Open-Set-DA"><a href="#Open-Set-DA" class="headerlink" title="Open Set DA"></a>Open Set DA</h2><p><strong>Conference</strong></p>
<ul>
<li>Towards Novel Target Discovery Through Open-Set Domain Adaptation <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Jing_Towards_Novel_Target_Discovery_Through_Open-Set_Domain_Adaptation_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>On the Effectiveness of Image Rotation for Open Set Domain Adaptation <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123610409.pdf">[ECCV2020]</a> <a href="https://github.com/silvia1993/ROS">[Pytorch]</a></li>
<li>Multi-Source Open-Set Deep Adversarial Domain Adaptation <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123710732.pdf">[ECCV2020]</a></li>
<li>Progressive Graph Learning for Open-Set Domain Adaptation <a href="https://proceedings.icml.cc/static/paper_files/icml/2020/136-Paper.pdf">[ICML2020]</a> <a href="https://github.com/BUserName/PGL">[Pytorch]</a></li>
<li>Joint Partial Optimal Transport for Open Set Domain Adaptation <a href="https://www.ijcai.org/Proceedings/2020/0352.pdf">[IJCAI2020]</a></li>
<li>Exploring Category-Agnostic Clusters for Open-Set Domain Adaptation <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Pan_Exploring_Category-Agnostic_Clusters_for_Open-Set_Domain_Adaptation_CVPR_2020_paper.pdf">[CVPR2020]</a></li>
<li>Towards Inheritable Models for Open-Set Domain Adaptation <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Kundu_Towards_Inheritable_Models_for_Open-Set_Domain_Adaptation_CVPR_2020_paper.pdf">[CVPR 2020]</a> <a href="https://sites.google.com/view/inheritune">[Project]</a></li>
<li>Attract or Distract: Exploit the Margin of Open Set <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Feng_Attract_or_Distract_Exploit_the_Margin_of_Open_Set_ICCV_2019_paper.pdf">[ICCV2019]</a> <a href="https://github.com/qy-feng/margin-openset">[code]</a></li>
<li>Separate to Adapt: Open Set Domain Adaptation via Progressive Separation <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Separate_to_Adapt_Open_Set_Domain_Adaptation_via_Progressive_Separation_CVPR_2019_paper.pdf">[CVPR2019]</a> <a href="https://github.com/thuml/Separate_to_Adapt">[Pytorch]</a></li>
<li>Weakly Supervised Open-set Domain Adaptation by Dual-domain Collaboration <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Tan_Weakly_Supervised_Open-Set_Domain_Adaptation_by_Dual-Domain_Collaboration_CVPR_2019_paper.pdf">[CVPR2019]</a></li>
<li>Learning Factorized Representations for Open-set Domain Adaptation <a href="https://openreview.net/pdf?id=SJe3HiC5KX">[ICLR2019]</a></li>
<li>Open Set Domain Adaptation by Backpropagation <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Kuniaki_Saito_Adversarial_Open_Set_ECCV_2018_paper.pdf">[ECCV2018]</a> <a href="https://github.com/ksaito-ut/OPDA_BP">[Pytorch(Official)]</a> <a href="https://github.com/Mid-Push/Open_set_domain_adaptation">[Tensorflow]</a> <a href="https://github.com/YU1ut/openset-DA">[Pytorch]</a></li>
<li>Open Set Domain Adaptation <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Busto_Open_Set_Domain_ICCV_2017_paper.pdf">[ICCV2017]</a></li>
</ul>
<p><strong>Journal</strong></p>
<ul>
<li>Open-set domain adaptation by deconfounding domain gaps <a href="https://link.springer.com/article/10.1007/s10489-022-03805-9">[Applied Intelligence 2022]</a></li>
<li>Unsupervised Multi-Class Domain Adaptation: Theory, Algorithms, and Practice <a href="https://arxiv.org/abs/2002.08681">[TPAMI2020]</a> <a href="https://github.com/YBZh/MultiClassDA">[PyTroch]</a></li>
<li>Adversarial Network with Multiple Classifiers for Open Set Domain Adaptation <a href="https://arxiv.org/abs/2007.00384">[IEEE TMM]</a> <a href="https://github.com/tasfia/DAMC">[Pytorch]</a></li>
</ul>
<p><strong>Arxiv</strong></p>
<ul>
<li>Collaborative Training of Balanced Random Forests for Open Set Domain Adaptation <a href="https://arxiv.org/abs/2002.03642v1">[10 Feb 2020]</a></li>
<li>Known-class Aware Self-ensemble for Open Set Domain Adaptation <a href="https://arxiv.org/abs/1905.01068v1">[3 May 2019]</a></li>
</ul>
<h2 id="Universal-DA"><a href="#Universal-DA" class="headerlink" title="Universal DA"></a>Universal DA</h2><p><strong>Conference</strong></p>
<ul>
<li>Universal Domain Adaptation for Robust Handling of Distributional Shifts in NLP <a href="https://aclanthology.org/2023.findings-emnlp.392/">[EMNLP 2023 Findings]</a></li>
<li>Subsidiary Prototype Alignment for Universal Domain Adaptation <a href="https://openreview.net/forum?id=5kThooa07pf">[NeurIPS2022]</a> <a href="https://sites.google.com/view/spa-unida">[Project Page]</a></li>
<li>OVANet: One-vs-All Network for Universal Domain Adaptation <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Saito_OVANet_One-vs-All_Network_for_Universal_Domain_Adaptation_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Active Universal Domain Adaptation <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Ma_Active_Universal_Domain_Adaptation_ICCV_2021_paper.pdf">[ICCV 2021]</a></li>
<li>Domain Consensus Clustering for Universal Domain Adaptation <a href="http://reler.net/papers/guangrui_cvpr2021.pdf">[CVPR 2021]</a> <a href="https://github.com/Solacex/Domain-Consensus-Clustering">[Pytorch]</a></li>
<li>Divergence Optimization for Noisy Universal Domain Adaptation <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yu_Divergence_Optimization_for_Noisy_Universal_Domain_Adaptation_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>Universal Domain Adaptation through Self Supervision <a href="https://papers.nips.cc/paper/2020/hash/bb7946e7d85c81a9e69fee1cea4a087c-Abstract.html">[NeurIPS 2020]</a> <a href="https://github.com/VisionLearningGroup/DANCE">[Pytorch]</a></li>
<li>Learning to Detect Open Classes for Universal Domain Adaptation <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123600562.pdf">[ECCV2020]</a> <a href="https://github.com/thuml/Calibrated-Multiple-Uncertainties">[code]</a></li>
<li>Universal Source-Free Domain Adaptation <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Kundu_Universal_Source-Free_Domain_Adaptation_CVPR_2020_paper.pdf">[CVPR2020]</a> <a href="https://sites.google.com/view/usfda-cvpr2020">[Project]</a></li>
<li>Universal Domain Adaptation <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/You_Universal_Domain_Adaptation_CVPR_2019_paper.pdf">[CVPR2019]</a>  <a href="https://github.com/thuml/Universal-Domain-Adaptation">[Pytorch]</a></li>
</ul>
<p><strong>Journal</strong></p>
<ul>
<li>Universal Model Adaptation by Style Augmented Open-set Consistency <a href="https://link.springer.com/article/10.1007/s10489-023-04731-0">[Applied Intelligence 2023]</a></li>
</ul>
<p><strong>Arxiv</strong></p>
<ul>
<li>Universal Multi-Source Domain Adaptation <a href="https://arxiv.org/abs/2011.02594">[5 Nov 2020]</a></li>
<li>A Sample Selection Approach for Universal Domain Adaptation <a href="https://arxiv.org/abs/2001.05071v1">[14 Jan 2020]</a></li>
</ul>
<h2 id="Open-Compound-DA"><a href="#Open-Compound-DA" class="headerlink" title="Open Compound DA"></a>Open Compound DA</h2><p><strong>Conference</strong></p>
<ul>
<li>Discover, Hallucinate, and Adapt: Open Compound Domain Adaptation for Semantic Segmentation <a href="https://proceedings.neurips.cc/paper/2020/file/7a9a322cbe0d06a98667fdc5160dc6f8-Paper.pdf">[NeurIPS2020]</a></li>
<li>Open Compound Domain Adaptation <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Open_Compound_Domain_Adaptation_CVPR_2020_paper.pdf">[CVRP2020 Oral]</a> <a href="https://github.com/zhmiao/OpenCompoundDomainAdaptation-OCDA">[Pytorch]</a></li>
</ul>
<p><strong>Journal</strong></p>
<ul>
<li>Source-Free Open Compound Domain Adaptation in Semantic Segmentation <a href="https://ieeexplore.ieee.org/document/9785619">[TCSVT 2022]</a></li>
</ul>
<h2 id="Multi-Source-DA"><a href="#Multi-Source-DA" class="headerlink" title="Multi Source DA"></a>Multi Source DA</h2><p><strong>Conference</strong></p>
<ul>
<li>Confident Anchor-Induced Multi-Source Free Domain Adaptation <a href="https://proceedings.neurips.cc/paper/2021/hash/168908dd3227b8358eababa07fcaf091-Abstract.html">[NeurIPS2021]</a> <a href="https://github.com/Learning-group123/CAiDA">[code is coming soon]</a></li>
<li>mDALU: Multi-Source Domain Adaptation and Label Unification With Partial Datasets <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Gong_mDALU_Multi-Source_Domain_Adaptation_and_Label_Unification_With_Partial_Datasets_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>STEM: An Approach to Multi-Source Domain Adaptation With Guarantees <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Nguyen_STEM_An_Approach_to_Multi-Source_Domain_Adaptation_With_Guarantees_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>T-SVDNet: Exploring High-Order Prototypical Correlations for Multi-Source Domain Adaptation <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Li_T-SVDNet_Exploring_High-Order_Prototypical_Correlations_for_Multi-Source_Domain_Adaptation_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Multi-Source Domain Adaptation for Object Detection <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Yao_Multi-Source_Domain_Adaptation_for_Object_Detection_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Information-Theoretic Regularization for Multi-Source Domain Adaptation <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Park_Information-Theoretic_Regularization_for_Multi-Source_Domain_Adaptation_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Partial Feature Selection and Alignment for Multi-Source Domain Adaptation <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Fu_Partial_Feature_Selection_and_Alignment_for_Multi-Source_Domain_Adaptation_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>Wasserstein Barycenter for Multi-Source Domain Adaptation <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Montesuma_Wasserstein_Barycenter_for_Multi-Source_Domain_Adaptation_CVPR_2021_paper.pdf">[CVPR2021]</a> <a href="https://github.com/eddardd/WBTransport">[Code]</a></li>
<li>Unsupervised Multi-source Domain Adaptation Without Access to Source Data <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ahmed_Unsupervised_Multi-Source_Domain_Adaptation_Without_Access_to_Source_Data_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>Dynamic Transfer for Multi-Source Domain Adaptation <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Dynamic_Transfer_for_Multi-Source_Domain_Adaptation_CVPR_2021_paper.pdf">[CVPR2021]</a> <a href="https://github.com/liyunsheng13/DRT">[Pytorch]</a></li>
<li>Multi-Source Domain Adaptation with Collaborative Learning for Semantic Segmentation <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/He_Multi-Source_Domain_Adaptation_With_Collaborative_Learning_for_Semantic_Segmentation_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>MOST: Multi-Source Domain Adaptation via Optimal Transport for Student-Teacher Learning <a href="https://auai.org/uai2021/pdf/uai2021.106.pdf">[UAI2021]</a></li>
<li>Meta Self-Learning for Multi-Source Domain Adaptation: A Benchmark <a href="https://arxiv.org/abs/2108.10840">[ICCV Workshop 2021]</a> <a href="https://github.com/bupt-ai-cz/Meta-SelfLearning">[Pytorch]</a></li>
<li>Your Classifier can Secretly Suffice Multi-Source Domain Adaptation <a href="https://papers.nips.cc/paper/2020/file/3181d59d19e76e902666df5c7821259a-Paper.pdf">[NeurIPS 2020]</a> <a href="https://sites.google.com/view/simpal">[Project]</a></li>
<li>Multi-Source Open-Set Deep Adversarial Domain Adaptation <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123710732.pdf">[ECCV2020]</a></li>
<li>Online Meta-Learning for Multi-Source and Semi-Supervised Domain Adaptation <a href="https://arxiv.org/abs/2004.04398">[ECCV2020]</a></li>
<li>Multi-Source Open-Set Deep Adversarial Domain Adaptation <a href="https://dipeshtamboli.github.io/blog/2020/Multi-Source-Open-Set-Deep-Adversarial-Domain-Adaptation/">[ECCV2020]</a></li>
<li>Curriculum Manager for Source Selection in Multi-Source Domain Adaptation <a href="https://arxiv.org/abs/2007.01261v1">[ECCV2020]</a></li>
<li>Domain Aggregation Networks for Multi-Source Domain Adaptation <a href="https://proceedings.icml.cc/static/paper_files/icml/2020/6292-Paper.pdf">[ICML2020]</a></li>
<li>Learning to Combine: Knowledge Aggregation for Multi-Source Domain Adaptation <a href="https://github.com/ChrisAllenMing/LtC-MSDA">[ECCV2020]</a> <a href="https://github.com/ChrisAllenMing/LtC-MSDA">[Pytorch]</a></li>
<li>Multi-Source Domain Adaptation for Text Classification via DistanceNet-Bandits <a href="https://arxiv.org/abs/2001.04362v2">[AAAI2020]</a></li>
<li>Adversarial Training Based Multi-Source Unsupervised Domain Adaptation for Sentiment Analysis <a href="https://arxiv.org/pdf/2006.05602.pdf">[AAAI2020]</a></li>
<li>Multi-source Domain Adaptation for Visual Sentiment Classification <a href="https://arxiv.org/abs/2001.03886v1">[AAAI2020]</a></li>
<li>Multi-source Distilling Domain Adaptation <a href="https://arxiv.org/abs/1911.11554v1">[AAAI2020]</a> <a href="https://github.com/daoyuan98/MDDA">[code]</a></li>
<li>Multi-source Domain Adaptation for Semantic Segmentation <a href="https://arxiv.org/abs/1910.12181">[NeurlPS2019]</a> <a href="https://github.com/Luodian/MADAN">[Pytorch]</a></li>
<li>Moment Matching for Multi-Source Domain Adaptation <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Peng_Moment_Matching_for_Multi-Source_Domain_Adaptation_ICCV_2019_paper.pdf">[ICCV2019]</a> <a href="http://ai.bu.edu/M3SDA/">[Pytorch]</a></li>
<li>Multi-Domain Adversarial Learning <a href="https://openreview.net/forum?id=Sklv5iRqYX">[ICLR2019]</a> <a href="https://github.com/AltschulerWu-Lab/MuLANN">[Torch]</a></li>
<li>Algorithms and Theory for Multiple-Source Adaptation <a href="https://papers.nips.cc/paper/8046-algorithms-and-theory-for-multiple-source-adaptation">[NIPS2018]</a></li>
<li>Adversarial Multiple Source Domain Adaptation <a href="http://papers.nips.cc/paper/8075-adversarial-multiple-source-domain-adaptation">[NIPS2018]</a> <a href="https://github.com/KeiraZhao/MDAN">[Pytorch]</a></li>
<li>Boosting Domain Adaptation by Discovering Latent Domains <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Mancini_Boosting_Domain_Adaptation_CVPR_2018_paper.pdf">[CVPR2018]</a> <a href="https://github.com/mancinimassimiliano/latent_domains_DA">[Caffe]</a> <a href="https://github.com/mancinimassimiliano/pytorch_wbn">[Pytorch]</a></li>
<li>Deep Cocktail Network: Multi-source Unsupervised Domain Adaptation with Category Shift <a href="https://arxiv.org/abs/1803.00830">[CVPR2018]</a> <a href="https://github.com/HCPLab-SYSU/MSDA">[Pytorch]</a></li>
</ul>
<p><strong>Journal</strong></p>
<ul>
<li>Graphical Modeling for Multi-Source Domain Adaptation <a href="https://ieeexplore.ieee.org/abstract/document/9767755">[TPAMI 2022]</a> <a href="https://github.com/Francis0625/Graphical-Modeling-for-Multi-Source-Domain-Adaptation">[Pytorch]</a></li>
<li>Unsupervised sentiment analysis by transferring multi-source knowledge<a href="https://arxiv.org/pdf/2105.11902.pdf">[Cognitive Computation]</a></li>
<li>A survey of multi-source domain adaptation <a href="https://www.sciencedirect.com/science/article/pii/S1566253514001316">[Information Fusion]</a></li>
</ul>
<p><strong>Arxiv</strong></p>
<ul>
<li>Mutual learning network for multi-source domain adaptation <a href="https://arxiv.org/pdf/2003.12944">[arXiv]</a></li>
<li>Domain Adaptive Ensemble Learning <a href="https://arxiv.org/abs/2003.07325">[arXiv]</a></li>
<li>Multi-Source Domain Adaptation and Semi-Supervised Domain Adaptation with Focus on Visual Domain Adaptation Challenge 2019 <a href="https://arxiv.org/abs/1910.03548">[14 Oct 2019]</a></li>
</ul>
<h2 id="Multi-Target-DA"><a href="#Multi-Target-DA" class="headerlink" title="Multi Target DA"></a>Multi Target DA</h2><p><strong>Conference</strong></p>
<ul>
<li>CoNMix for Source-free Single and Multi-target Domain Adaptation <a href="https://openaccess.thecvf.com/content/WACV2023/html/Kumar_CoNMix_for_Source-Free_Single_and_Multi-Target_Domain_Adaptation_WACV_2023_paper.html">[WACV2022]</a> <a href="https://github.com/vcl-iisc/CoNMix">[Pytorch]</a></li>
<li>Curriculum Graph Co-Teaching for Multi-Target Domain Adaptation <a href="https://arxiv.org/abs/2104.00808v1">[CVPR2021]</a> <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Roy_Curriculum_Graph_Co-Teaching_for_Multi-Target_Domain_Adaptation_CVPR_2021_paper.pdf">[Pytorch]</a></li>
<li>Multi-Target Domain Adaptation with Collaborative Consistency Learning <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Isobe_Multi-Target_Domain_Adaptation_With_Collaborative_Consistency_Learning_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
</ul>
<p><strong>Arxiv</strong></p>
<ul>
<li>Unsupervised Multi-Target Domain Adaptation: An Information Theoretic Approach <a href="https://arxiv.org/abs/1810.11547v1">[arXiv]</a></li>
</ul>
<h2 id="Incremental-DA"><a href="#Incremental-DA" class="headerlink" title="Incremental DA"></a>Incremental DA</h2><p><strong>Conference</strong></p>
<ul>
<li>Lifelong Domain Adaptation via Consolidated Internal Distribution <a href="https://proceedings.neurips.cc/paper/2021/hash/5caf41d62364d5b41a893adc1a9dd5d4-Abstract.html">[NeurIPS2021]</a></li>
<li>Continual Adaptation of Visual Representations via Domain Randomization and Meta-learning <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Volpi_Continual_Adaptation_of_Visual_Representations_via_Domain_Randomization_and_Meta-Learning_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>ConDA: Continual Unsupervised Domain Adaptation <a href="https://arxiv.org/abs/2103.11056v1">[CVPR2021]</a></li>
<li>Gradient Regularized Contrastive Learning for Continual Domain Adaptation <a href="https://arxiv.org/abs/2103.12294v1">[AAAI2021]</a></li>
<li>Gradual Domain Adaptation without Indexed Intermediate Domains <a href="https://proceedings.neurips.cc/paper/2021/hash/45017f6511f91be700fda3d118034994-Abstract.html">[NeurIPS2021]</a></li>
<li>Learning to Adapt to Evolving Domains <a href="https://proceedings.neurips.cc/paper/2020/file/fd69dbe29f156a7ef876a40a94f65599-Paper.pdf">[NeurIPS 2020]</a> <a href="https://github.com/Liuhong99/EAML">[Pytorch]</a></li>
<li>Class-Incremental Domain Adaptation <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580052.pdf">[ECCV2020]</a></li>
<li>Incremental Adversarial Domain Adaptation for Continually Changing Environments <a href="https://arxiv.org/abs/1712.07436">[ICRA2018]</a></li>
<li>Continuous Manifold based Adaptation for Evolving Visual Domains <a href="https://people.eecs.berkeley.edu/~jhoffman/papers/Hoffman_CVPR2014.pdf">[CVPR2014]</a></li>
</ul>
<h2 id="Multi-Step-DA"><a href="#Multi-Step-DA" class="headerlink" title="Multi Step DA"></a>Multi Step DA</h2><p><strong>Arxiv</strong></p>
<ul>
<li>Adversarial Domain Adaptation for Stance Detection <a href="https://arxiv.org/abs/1902.02401">[arXiv]</a></li>
<li>Ensemble Adversarial Training: Attacks and Defenses <a href="https://arxiv.org/abs/1705.07204">[arXiv]</a></li>
</ul>
<p><strong>Conference</strong></p>
<ul>
<li>Distant domain transfer learning <a href="http://www.ntu.edu.sg/home/sinnopan/publications/[AAAI17]Distant%20Domain%20Transfer%20Learning.pdf">[AAAI2017]</a></li>
</ul>
<h2 id="Heterogeneous-DA"><a href="#Heterogeneous-DA" class="headerlink" title="Heterogeneous DA"></a>Heterogeneous DA</h2><p><strong>Conference</strong></p>
<ul>
<li>Domain Adaptive Classification on Heterogeneous Information Networks <a href="https://www.ijcai.org/Proceedings/2020/0196.pdf">[IJCAI2020]</a></li>
<li>Heterogeneous Domain Adaptation via Soft Transfer Network <a href="https://arxiv.org/abs/1908.10552v1">[ACM MM2019]</a></li>
</ul>
<h2 id="Target-agnostic-DA"><a href="#Target-agnostic-DA" class="headerlink" title="Target-agnostic DA"></a>Target-agnostic DA</h2><p><strong>Arxiv</strong></p>
<ul>
<li>Compound Domain Adaptation in an Open World <a href="https://arxiv.org/abs/1909.03403">[8 Sep 2019]</a></li>
</ul>
<p><strong>Conference</strong></p>
<ul>
<li>Domain Agnostic Learning with Disentangled Representations <a href="http://proceedings.mlr.press/v97/peng19b/peng19b.pdf">[ICML2019]</a> <a href="https://github.com/VisionLearningGroup/DAL">[Pytorch]</a></li>
<li>Blending-target Domain Adaptation by Adversarial Meta-Adaptation Networks <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Blending-Target_Domain_Adaptation_by_Adversarial_Meta-Adaptation_Networks_CVPR_2019_paper.pdf">[CVPR2019]</a> <a href="https://github.com/zjy526223908/BTDA">[Pytorch]</a></li>
</ul>
<h2 id="Federated-DA"><a href="#Federated-DA" class="headerlink" title="Federated DA"></a>Federated DA</h2><p><strong>Arxiv</strong></p>
<ul>
<li>Federated Adversarial Domain Adaptation <a href="https://arxiv.org/abs/1911.02054v1">[5 Nov 2019]</a></li>
</ul>
<h2 id="Continuously-Indexed-DA"><a href="#Continuously-Indexed-DA" class="headerlink" title="Continuously Indexed DA"></a>Continuously Indexed DA</h2><p><strong>Conference</strong></p>
<ul>
<li>Continuously Indexed Domain Adaptation <a href="http://wanghao.in/paper/ICML20_CIDA.pdf">[ICML 2020]</a> <a href="https://github.com/hehaodele/CIDA">[Pytorch]</a> <a href="https://github.com/hehaodele/CIDA/blob/master/README.md">[Project Page]</a> <a href="https://www.youtube.com/watch?v=KtZPSCD-WhQ">[Video]</a></li>
</ul>
<h2 id="Source-Free-DA"><a href="#Source-Free-DA" class="headerlink" title="Source Free DA"></a>Source Free DA</h2><p><strong>Conference</strong></p>
<ul>
<li>Domain Adaptation with Adversarial Training on Penultimate Activations <a href="https://ojs.aaai.org/index.php/AAAI/article/view/26185">[AAAI2023]</a> <a href="https://github.com/tsun/APA">[Pytorch]</a></li>
<li>Source-free Domain Adaptive Human Pose Estimation <a href="https://arxiv.org/abs/2308.03202">[ICCV2023]</a><a href="https://github.com/davidpengucf/SFDAHPE">[Pytorch]</a></li>
<li>RAIN: RegulArization on Input and Network for Black-Box Domain Adaptation <a href="https://www.ijcai.org/proceedings/2023/458">[IJCAI2023]</a> <a href="https://github.com/davidpengucf/RAIN">[Pytorch]</a></li>
<li>CoNMix for Source-free Single and Multi-target Domain Adaptation <a href="https://openaccess.thecvf.com/content/WACV2023/html/Kumar_CoNMix_for_Source-Free_Single_and_Multi-Target_Domain_Adaptation_WACV_2023_paper.html">[WACV2022]</a> <a href="https://github.com/vcl-iisc/CoNMix">[Pytorch]</a></li>
<li>Source-free Video Domain Adaptation by Learning Temporal Consistency for Action Recognition <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136940144.pdf">[ECCV2022]</a> <a href="https://github.com/xuyu0010/ATCoN">[Pytorch]</a> <a href="https://xuyu0010.github.io/sfvda.html">[Project]</a></li>
<li>Concurrent Subsidiary Supervision for Unsupervised Source-Free Domain Adaptation <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/912_ECCV_2022_paper.php">[ECCV2022]</a> <a href="https://sites.google.com/view/sticker-sfda">[Project Page]</a></li>
<li>Balancing Discriminability and Transferability for Source-Free Domain Adaptation <a href="https://proceedings.mlr.press/v162/kundu22a.html">[ICML2022]</a> <a href="https://sites.google.com/view/mixup-sfda">[Project Page]</a></li>
<li>Source-free Domain Adaptation via Avatar Prototype Generation and Adaptation <a href="https://arxiv.org/abs/2106.15326">[IJCAI2021]</a> <a href="https://github.com/SCUT-AILab/CPGA">[Pytorch]</a></li>
<li>Confident Anchor-Induced Multi-Source Free Domain Adaptation <a href="https://proceedings.neurips.cc/paper/2021/hash/168908dd3227b8358eababa07fcaf091-Abstract.html">[NeurIPS2021]</a> <a href="https://github.com/Learning-group123/CAiDA">[Pytorch]</a></li>
<li>Model Adaptation: Historical Contrastive Learning for Unsupervised Domain Adaptation without Source Data <a href="https://proceedings.neurips.cc/paper/2021/hash/1dba5eed8838571e1c80af145184e515-Abstract.html">[NeurIPS2021]</a> <a href="https://github.com/jxhuang0508/HCL">[Pytorch]</a></li>
<li>Exploiting the Intrinsic Neighborhood Structure for Source-free Domain Adaptation <a href="https://proceedings.neurips.cc/paper/2021/hash/f5deaeeae1538fb6c45901d524ee2f98-Abstract.html">[NeurIPS2021]</a> <a href="https://github.com/Albert0147/SFDA_neighbors">[Pytorch]</a></li>
<li>Unsupervised Domain Adaptation of Black-Box Source Models <a href="https://www.bmvc2021-virtualconference.com/assets/papers/0404.pdf">[BMVC2021]</a><a href="https://github.com/zhjscut/IterLNL">[Pytorch]</a></li>
<li>Generalize Then Adapt: Source-Free Domain Adaptive Semantic Segmentation <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Kundu_Generalize_Then_Adapt_Source-Free_Domain_Adaptive_Semantic_Segmentation_ICCV_2021_paper.html">[ICCV2021]</a> <a href="https://sites.google.com/view/sfdaseg">[Project]</a></li>
<li>Generalized Source-free Domain Adaptation <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_Generalized_Source-Free_Domain_Adaptation_ICCV_2021_paper.pdf">[ICCV2021]</a> <a href="https://github.com/Albert0147/G-SFDA">[Pytorch]</a></li>
<li>Adaptive Adversarial Network for Source-free Domain Adaptation <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Xia_Adaptive_Adversarial_Network_for_Source-Free_Domain_Adaptation_ICCV_2021_paper.pdf">[ICCV2021]</a> <a href="https://github.com/HaifengXia/SFDA">[Pytorch]</a></li>
<li>Visualizing Adapted Knowledge in Domain Transfer <a href="https://arxiv.org/abs/2104.10602">[CVPR2021]</a> <a href="https://github.com/hou-yz/DA_visualization">[Pytorch]</a></li>
<li>Unsupervised Multi-source Domain Adaptation Without Access to Source Data <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ahmed_Unsupervised_Multi-Source_Domain_Adaptation_Without_Access_to_Source_Data_CVPR_2021_paper.pdf">[CVPR2021]</a> <a href="https://github.com/driptaRC/DECISION">[Pytorch]</a></li>
<li>Source-Free Domain Adaptation for Semantic Segmentation <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Source-Free_Domain_Adaptation_for_Semantic_Segmentation_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>Domain Impression: A Source Data Free Domain Adaptation Method <a href="https://openaccess.thecvf.com/content/WACV2021/papers/Kurmi_Domain_Impression_A_Source_Data_Free_Domain_Adaptation_Method_WACV_2021_paper.pdf">[WACV2021]</a> <a href="https://delta-lab-iitk.github.io/SFDA/">[Project]</a></li>
<li>Model Adaptation: Unsupervised Domain Adaptation Without Source Data <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Model_Adaptation_Unsupervised_Domain_Adaptation_Without_Source_Data_CVPR_2020_paper.pdf">[CVPR2020]</a></li>
<li>Universal Source-Free Domain Adaptation <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Kundu_Universal_Source-Free_Domain_Adaptation_CVPR_2020_paper.pdf">[CVPR2020]</a> <a href="https://sites.google.com/view/usfda-cvpr2020">[Project]</a></li>
<li>Towards Inheritable Models for Open-Set Domain Adaptation <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Kundu_Towards_Inheritable_Models_for_Open-Set_Domain_Adaptation_CVPR_2020_paper.pdf">[CVPR2020]</a> <a href="https://sites.google.com/view/inheritune">[Project]</a></li>
<li>Do We Really Need to Access the Source Data? Source Hypothesis Transfer for Unsupervised Domain Adaptation <a href="http://proceedings.mlr.press/v119/ishida20a.html">[ICML2020]</a> <a href="https://github.com/tim-learn/SHOT">[Pytorch]</a></li>
</ul>
<p><strong>Arxiv</strong></p>
<ul>
<li>Learning Invariant Representation with Consistency and Diversity for Semi-supervised Source Hypothesis Transfer<a href="https://arxiv.org/abs/2107.03008">[7 Jul 2021]</a><a href="https://github.com/Wang-xd1899/SSHT">[Pytorch]</a></li>
<li>Source Data-absent Unsupervised Domain Adaptation through Hypothesis Transfer and Labeling Transfer <a href="https://arxiv.org/abs/2012.07297">[14 Dec 2020]</a> <a href="https://github.com/tim-learn/SHOT-plus">[Pytorch]</a></li>
</ul>
<h2 id="Active-DA"><a href="#Active-DA" class="headerlink" title="Active DA"></a>Active DA</h2><p><strong>Conference</strong></p>
<ul>
<li>Local Context-Aware Active Domain Adaptation <a href="https://arxiv.org/abs/2208.12856">[ICCV2023]</a> <a href="https://github.com/tsun/LADA">[Pytorch]</a></li>
<li>Reducing Annotation Effort by Identifying and Labeling Contextually Diverse Classes for Semantic Segmentation Under Domain Shift <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Agarwal_Reducing_Annotation_Effort_by_Identifying_and_Labeling_Contextually_Diverse_Classes_WACV_2023_paper.pdf">[WACV2023]</a></li>
<li>Towards Fewer Annotations: Active Learning via Region Impurity and Prediction Uncertainty for Domain Adaptive Semantic Segmentation <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xie_Towards_Fewer_Annotations_Active_Learning_via_Region_Impurity_and_Prediction_CVPR_2022_paper.pdf">[CVPR2022]</a><a href="https://github.com/BIT-DA/RIPU">[Pytorch]</a></li>
<li>Active Learning for Domain Adaptation: An Energy-based Approach <a href="ttps://arxiv.org/abs/2112.01406">[AAAI2022]</a><a href="https://github.com/BIT-DA/EADA">[Pytorch]</a></li>
<li>Multi-Anchor Active Domain Adaptation for Semantic Segmentation <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Ning_Multi-Anchor_Active_Domain_Adaptation_for_Semantic_Segmentation_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Active Domain Adaptation via Clustering Uncertainty-Weighted Embeddings <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Prabhu_Active_Domain_Adaptation_via_Clustering_Uncertainty-Weighted_Embeddings_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Active Universal Domain Adaptation <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Ma_Active_Universal_Domain_Adaptation_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>S3VAADA: Submodular Subset Selection for Virtual Adversarial Active Domain Adaptation <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Rangwani_S3VAADA_Submodular_Subset_Selection_for_Virtual_Adversarial_Active_Domain_Adaptation_ICCV_2021_paper.pdf">[ICCV2021]</a></li>
<li>Transferable Query Selection for Active Domain Adaptation <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Fu_Transferable_Query_Selection_for_Active_Domain_Adaptation_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
</ul>
<h2 id="Generalized-Domain-Adaptation"><a href="#Generalized-Domain-Adaptation" class="headerlink" title="Generalized Domain Adaptation"></a>Generalized Domain Adaptation</h2><p><strong>Conference</strong></p>
<ul>
<li>Generalized Domain Adaptation <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Mitsuzumi_Generalized_Domain_Adaptation_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
</ul>
<h2 id="Model-Selection"><a href="#Model-Selection" class="headerlink" title="Model Selection"></a>Model Selection</h2><ul>
<li>Addressing Parameter Choice Issues in Unsupervised Domain Adaptation by Aggregation <a href="https://openreview.net/forum?id=M95oDwJXayG">[ICLR2023ORAL]</a> <a href="https://github.com/Xpitfire/iwa">[Pytorch]</a></li>
<li>The Balancing Principle for Parameter Choice in Distance-Regularized Domain Adaptation <a href="https://proceedings.neurips.cc/paper/2021/hash/ae0909a324fb2530e205e52d40266418-Abstract.html">[NeurIPS2021]</a> <a href="https://github.com/xpitfire/bpda">[Pytorch]</a></li>
<li>Towards Accurate Model Selection in Deep Unsupervised Domain Adaptation <a href="http://proceedings.mlr.press/v97/you19a/you19a.pdf">[ICML2019]</a> <a href="https://github.com/thuml/Deep-Embedded-Validation">[Pytorch]</a></li>
</ul>
<h2 id="Other-Transfer-Learning-Paradigms"><a href="#Other-Transfer-Learning-Paradigms" class="headerlink" title="Other Transfer Learning Paradigms"></a>Other Transfer Learning Paradigms</h2><h3 id="Domain-Generalization"><a href="#Domain-Generalization" class="headerlink" title="Domain Generalization"></a>Domain Generalization</h3><p><strong>Conference</strong></p>
<ul>
<li>Adapting to Distribution Shift by Visual Domain Prompt Generation <a href="https://arxiv.org/pdf/2405.02797">[ICLR2024]</a> <a href="https://github.com/Guliisgreat/VDPG">[Pytorch]</a></li>
<li>Test-Time Domain Adaptation by Learning Domain-Aware Batch Normalization <a href="https://arxiv.org/pdf/2312.10165">[AAAI2024 (Oral)]</a></li>
<li>A Dual-Augmentor Framework for Domain Generalization in 3D Human Pose Estimation <a href="https://arxiv.org/abs/2403.11310">[CVPR 2024]</a> <a href="https://github.com/davidpengucf/DAF-DG">[Pytorch]</a></li>
<li>Improving Single Domain-Generalized Object Detection: A Focus on Diversification and Alignment <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Danish_Improving_Single_Domain-Generalized_Object_Detection_A_Focus_on_Diversification_and_CVPR_2024_paper.pdf">[CVPR 2024]</a> <a href="https://github.com/msohaildanish/DivAlign">[Pytorch]</a></li>
<li>Generalization by Adaptation: Diffusion-Based Domain Extension for Domain-Generalized Semantic Segmentation <a href="https://arxiv.org/abs/2312.01850">[WACV 2024]</a> <a href="https://github.com/JNiemeijer/DIDEX">[Pytorch]</a></li>
<li>Topology-aware Robust Optimization for Out-of-Distribution Generalization <a href="https://arxiv.org/pdf/2307.13943">[ICLR 2023]</a> <a href="https://github.com/joffery/TRO">[Pytorch]</a></li>
<li>A Re-Parameterized Vision Transformer (ReVT) for Domain-Generalized Semantic Segmentation <a href="https://arxiv.org/abs/2308.13331">[ICCV Workshop 2023]</a> <a href="https://github.com/ifnspaml/revt-domain-generalization">[Pytorch]</a></li>
<li>Weight Averaging Improves Knowledge Distillation under Domain Shift <a href="https://arxiv.org/abs/2309.11446">[ICCV Workshop 2023]</a> <a href="https://github.com/vorobeevich/distillation-in-dg">[Pytorch]</a></li>
<li>Adaptive Texture Filtering for Single-Domain Generalized Segmentation <a href="https://arxiv.org/abs/2303.02943">[AAAI2023 oral]</a> <a href="https://github.com/leelxh/Adaptive-Texture-Filtering-for-Single-Domain-Generalized-Segmentation">[Pytorch]</a></li>
<li>PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization <a href="https://arxiv.org/abs/2307.15199">[ICCV2023]</a> <a href="https://promptstyler.github.io/">[Project]</a></li>
<li>Sparse Mixture-of-Experts are Domain Generalizable Learners <a href="https://openreview.net/forum?id=RecZ9nB9Q4">[ICLR2023(Oral)]</a> <a href="https://github.com/Luodian/Generalizable-Mixture-of-Experts">[Pytorch]</a></li>
<li>Meta-DMoE: Adapting to Domain Shift by Meta-Distillation from Mixture-of-Experts <a href="https://arxiv.org/pdf/2210.03885.pdf">[NeruIPS2022]</a> <a href="https://github.com/n3il666/Meta-DMoE">[Pytorch]</a></li>
<li>Style-Hallucinated Dual Consistency Learning for Domain Generalized Semantic Segmentation <a href="https://arxiv.org/pdf/2204.02548.pdf">[ECCV 2022]</a> <a href="https://github.com/HeliosZhao/SHADE">[Pytorch]</a></li>
<li>Learning to Generalize Unseen Domains via Memory-based Multi-Source Meta-Learning for Person Re-Identification <a href="https://arxiv.org/pdf/2012.00417.pdf">[CVPR 2021]</a> <a href="https://github.com/HeliosZhao/M3L">[Pytorch]</a></li>
<li>Domain Generalization via Inference-time Label-Preserving Target Projections <a href="https://arxiv.org/abs/2103.01134">[CVPR2021]</a> <a href="https://github.com/peterDan8/InferenceTimeDG">[Pytorch]</a></li>
<li>Domain Generalization via Entropy Regularization <a href="https://papers.nips.cc/paper/2020/file/b98249b38337c5088bbc660d8f872d6a-Paper.pdf">[NeurIPS2020]</a> <a href="https://github.com/sshan-zhao/DG_via_ER">[Pytorch]</a></li>
<li>Domain Generalization for Medical Imaging Classification with Linear-Dependency Regularization <a href="https://arxiv.org/abs/2009.12829">[NeurIPS2020]</a></li>
<li>Learning to Learn with Variational Information Bottleneck for Domain Generalization <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123550205.pdf">[ECCV2020]</a></li>
<li>Self-Challenging Improves Cross-Domain Generalization <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123470120.pdf">[ECCV2020]</a> <a href="https://github.com/DeLightCMU/RSC">[Pytorch]</a></li>
<li>Learning from Extrinsic and Intrinsic Supervisions for Domain Generalization <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123540154.pdf">[ECCV2020]</a> <a href="https://github.com/emma-sjwang/EISNet">[Pytorch]</a></li>
<li>Learning to Balance Specificity and Invariance for In and Out of Domain Generalization <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123540290.pdf">[ECCV2020]</a> <a href="https://github.com/prithv1/DMG">[Pytorch]</a></li>
<li>Learning to Generate Novel Domains for Domain Generalization <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123610545.pdf">[ECCV2020]</a></li>
<li>Learning to Optimize Domain Specific Normalization for Domain Generalization <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123670069.pdf">[ECCV2020]</a></li>
<li>Towards Recognizing Unseen Categories in Unseen Domains <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123680460.pdf">[ECCV2020]</a> <a href="https://github.com/mancinimassimiliano/CuMix">[Pytorch]</a></li>
<li>Efficient Domain Generalization via Common-Specific Low-Rank Decomposition <a href="https://proceedings.icml.cc/static/paper_files/icml/2020/4649-Paper.pdf">[ICML2020]</a> <a href="https://github.com/vihari/csd">[Pytorch]</a></li>
<li>Learning to Learn Single Domain Generalization <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Qiao_Learning_to_Learn_Single_Domain_Generalization_CVPR_2020_paper.pdf">[CVPR2020]</a> <a href="https://github.com/joffery/M-ADA">[Pytorch]</a></li>
<li>Generalized Convolutional Forest Networks for Domain Generalization and Visual Recognition <a href="https://openreview.net/forum?id=H1lxVyStPH">[ICLR2020]</a></li>
<li>Cross-Domain Few-Shot Classification via Learned Feature-Wise Transformation <a href="https://openreview.net/forum?id=SJl5Np4tPr">[ICLR2020]</a></li>
<li>Domain Generalization Using a Mixture of Multiple Latent Domains <a href="https://arxiv.org/abs/1911.07661v1">[AAAI2020]</a> <a href="https://github.com/mil-tokyo/dg_mmld">[Pytorch]</a></li>
<li>Deep Domain-Adversarial Image Generation for Domain Generalisation <a href="https://www.aaai.org/Papers/AAAI/2020GB/AAAI-ZhouK.2138.pdf">[Paper]</a> <a href="https://github.com/KaiyangZhou/Dassl.pytorch">[Pytorch]</a></li>
<li>Domain Generalization via Model-Agnostic Learning of Semantic Features <a href="https://papers.nips.cc/paper/8873-domain-generalization-via-model-agnostic-learning-of-semantic-features">[NeurIPS2019]</a> <a href="https://github.com/biomedia-mira/masf">[Tensorflow]</a></li>
<li>Episodic Training for Domain Generalization <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Episodic_Training_for_Domain_Generalization_ICCV_2019_paper.pdf">[ICCV2019 Oral]</a> [Pytorch]](<a href="https://github.com/HAHA-DL/Episodic-DG">https://github.com/HAHA-DL/Episodic-DG</a>)</li>
<li>Feature-Critic Networks for Heterogeneous Domain Generalization <a href="http://proceedings.mlr.press/v97/li19l/li19l.pdf">[ICML2019]</a> <a href="https://github.com/liyiying/Feature_Critic">[Pytorch]</a></li>
<li>Domain Generalization by Solving Jigsaw Puzzles <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Carlucci_Domain_Generalization_by_Solving_Jigsaw_Puzzles_CVPR_2019_paper.pdf">[CVPR2019 Oral]</a> <a href="https://github.com/fmcarlucci/JigenDG">[Pytorch]</a></li>
<li>MetaReg: Towards Domain Generalization using Meta-Regularization <a href="https://papers.nips.cc/paper/7378-metareg-towards-domain-generalization-using-meta-regularization">[NIPS2018]</a></li>
<li>Deep Domain Generalization via Conditional Invariant Adversarial Networks <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Ya_Li_Deep_Domain_Generalization_ECCV_2018_paper.pdf">[ECCV2018]</a></li>
<li>Domain Generalization with Adversarial Feature Learning <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Domain_Generalization_With_CVPR_2018_paper.pdf">[CVPR2018]</a></li>
</ul>
<p><strong>Journal</strong></p>
<ul>
<li>Domain Generalization for Regression <a href="https://link.springer.com/article/10.1007/s10845-019-01499-4">[IntellManuf2020]</a></li>
<li>Correlation-aware Adversarial Domain Adaptation and Generalization <a href="https://arxiv.org/abs/1911.12983v1">[Pattern Recognition(2019)]</a> <a href="https://github.com/mahfujur1/CA-DA-DG">[code]</a></li>
</ul>
<p><strong>Arxiv</strong></p>
<ul>
<li>Adversarial Pyramid Network for Video Domain Generalization <a href="https://arxiv.org/abs/1912.03716">[8 Dec 2019]</a></li>
<li>Towards Shape Biased Unsupervised Representation Learning for Domain Generalization <a href="https://arxiv.org/abs/1909.08245v1">[18 Sep 2019]</a></li>
<li>A Generalization Error Bound for Multi-class Domain Generalization <a href="https://arxiv.org/abs/1905.10392v1">[24 May 2019]</a></li>
<li>Adversarial Invariant Feature Learning with Accuracy Constraint for Domain Generalization <a href="https://arxiv.org/abs/1904.12543v1">[29 Apr 2019]</a></li>
<li>Beyond Domain Adaptation: Unseen Domain Encapsulation via Universal Non-volume Preserving Models <a href="https://arxiv.org/abs/1812.03407v1">[9 Dec 2018]</a></li>
</ul>
<h3 id="Domain-Randomization"><a href="#Domain-Randomization" class="headerlink" title="Domain Randomization"></a>Domain Randomization</h3><p><strong>Conference</strong></p>
<ul>
<li>DeceptionNet: Network-Driven Domain Randomization <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Zakharov_DeceptionNet_Network-Driven_Domain_Randomization_ICCV_2019_paper.pdf">[ICCV2019]</a></li>
<li>Domain Randomization and Pyramid Consistency: Simulation-to-Real Generalization Without Accessing Target Domain Data <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Yue_Domain_Randomization_and_Pyramid_Consistency_Simulation-to-Real_Generalization_Without_Accessing_Target_ICCV_2019_paper.pdf">[ICCV2019]</a></li>
</ul>
<h3 id="Transfer-Metric-Learning"><a href="#Transfer-Metric-Learning" class="headerlink" title="Transfer Metric Learning"></a>Transfer Metric Learning</h3><ul>
<li>Transfer Metric Learning: Algorithms, Applications and Outlooks <a href="https://arxiv.org/abs/1810.03944">[arXiv]</a></li>
</ul>
<h3 id="Knowledge-Transfer"><a href="#Knowledge-Transfer" class="headerlink" title="Knowledge Transfer"></a>Knowledge Transfer</h3><p><strong>Conference</strong></p>
<ul>
<li>Attention Bridging Network for Knowledge Transfer <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Attention_Bridging_Network_for_Knowledge_Transfer_ICCV_2019_paper.pdf">[ICCV2019]</a></li>
<li>Few-Shot Image Recognition with Knowledge Transfer <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Peng_Few-Shot_Image_Recognition_With_Knowledge_Transfer_ICCV_2019_paper.pdf">[ICCV2019]</a></li>
</ul>
<h3 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h3><p><strong>Conference</strong></p>
<ul>
<li>Learning Across Tasks and Domains <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Ramirez_Learning_Across_Tasks_and_Domains_ICCV_2019_paper.pdf">[ICCV2019]</a></li>
<li>UM-Adapt: Unsupervised Multi-Task Adaptation Using Adversarial Cross-Task Distillation <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Kundu_UM-Adapt_Unsupervised_Multi-Task_Adaptation_Using_Adversarial_Cross-Task_Distillation_ICCV_2019_paper.pdf">[ICCV2019]</a></li>
<li>Domain Agnostic Learning with Disentangled Representations <a href="https://arxiv.org/abs/1904.12347v1">[ICML2019]</a></li>
<li>Unsupervised Open Domain Recognition by Semantic Discrepancy Minimization <a href="https://arxiv.org/abs/1904.08631">[CVPR2019]</a> <a href="https://github.com/junbaoZHUO/UODTN">[Pytorch]</a></li>
</ul>
<p><strong>Arxiv</strong></p>
<ul>
<li>GradMix: Multi-source Transfer across Domains and Tasks [[9 Feb 2020]](GradMix: Multi-source Transfer across Domains and Tasks)</li>
<li>When Semi-Supervised Learning Meets Transfer Learning: Training Strategies, Models and Datasets <a href="https://arxiv.org/abs/1812.05313">[arXiv 13 Dec 2018]</a></li>
</ul>
<h2 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h2><h3 id="Object-Detection"><a href="#Object-Detection" class="headerlink" title="Object Detection"></a>Object Detection</h3><p><strong>Survey</strong></p>
<ul>
<li>Unsupervised Domain Adaptation of Object Detectors: A Survey <a href="https://arxiv.org/abs/2105.13502">[Arxiv 27 May 2021]</a></li>
</ul>
<p><strong>Conference</strong></p>
<ul>
<li>Supervision Interpolation via LossMix: Generalizing Mixup for Object Detection and Beyond <a href="https://arxiv.org/abs/2303.10343">[AAAI2024]</a></li>
<li>Instance Relation Graph Guided Source-Free Domain Adaptive Object Detection <a href="https://arxiv.org/abs/2203.15793">[CVPR2023]</a> <a href="https://viudomain.github.io/irg-sfda-web/">[Project]</a></li>
<li>Towards Online Domain Adaptive Object Detection <a href="https://arxiv.org/abs/2204.05289">[WACV2023]</a> [[<a href="https://github.com/Vibashan/online-da]]">https://github.com/Vibashan/online-da]]</a></li>
<li>Mixture of Teacher Experts for Source-Free Domain Adaptive Object Detection <a href="https://ieeexplore.ieee.org/document/9897795">[ICIP2022]</a></li>
<li>Towards Robust Adaptive Object Detection under Noisy Annotations <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Towards_Robust_Adaptive_Object_Detection_Under_Noisy_Annotations_CVPR_2022_paper.pdf">[CVPR2022]</a> <a href="https://github.com/CityU-AIM-Group/NLTE">[PyTorch]</a></li>
<li>H<sup>2</sup>FA R-CNN: Holistic and Hierarchical Feature Alignment for Cross-Domain Weakly Supervised Object Detection <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_H2FA_R-CNN_Holistic_and_Hierarchical_Feature_Alignment_for_Cross-Domain_Weakly_CVPR_2022_paper.pdf">[CVPR2022]</a> <a href="https://github.com/XuYunqiu/H2FA_R-CNN">[PyTorch]</a> <a href="https://github.com/XuYunqiu/H2FA_R-CNN/tree/ppdet">[PaddlePaddle]</a></li>
<li>Cross-Domain Adaptive Teacher for Object Detection <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Cross-Domain_Adaptive_Teacher_for_Object_Detection_CVPR_2022_paper.pdf">[CVPR2022]</a> <a href="https://yujheli.github.io/projects/adaptiveteacher.html">[Project]</a> <a href="https://github.com/facebookresearch/adaptive_teacher">[PyTorch]</a></li>
<li>Task-specific Inconsistency Alignment for Domain Adaptive Object Detection <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhao_Task-Specific_Inconsistency_Alignment_for_Domain_Adaptive_Object_Detection_CVPR_2022_paper.pdf">[CVPR2022]</a> <a href="https://github.com/MCG-NJU/TIA">[PyTorch]</a></li>
<li>SIGMA: Semantic-complete Graph Matching for Domain Adaptive Object Detection <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_SIGMA_Semantic-Complete_Graph_Matching_for_Domain_Adaptive_Object_Detection_CVPR_2022_paper.pdf">[CVPR2022]</a> <a href="https://github.com/CityU-AIM-Group/SIGMA">[PyTorch]</a></li>
<li>Single-Domain Generalized Object Detection in Urban Scene via Cyclic-Disentangled Self-Distillation <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Single-Domain_Generalized_Object_Detection_in_Urban_Scene_via_Cyclic-Disentangled_Self-Distillation_CVPR_2022_paper.pdf">[CVPR2022]</a></li>
<li>Target-Relevant Knowledge Preservation for Multi-Source Domain Adaptive Object Detection <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Target-Relevant_Knowledge_Preservation_for_Multi-Source_Domain_Adaptive_Object_Detection_CVPR_2022_paper.pdf">[CVPR2022]</a></li>
<li>Cross Domain Object Detection by Target-Perceived Dual Branch Distillation <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/He_Cross_Domain_Object_Detection_by_Target-Perceived_Dual_Branch_Distillation_CVPR_2022_paper.pdf">[CVPR2022]</a></li>
<li>Decoupled Adaptation for Cross-Domain Object Detection <a href="https://openreview.net/pdf?id=VNqaB1g9393">[ICLR2022]</a> <a href="https://github.com/thuml/Decoupled-Adaptation-for-Cross-Domain-Object-Detection">[PyTorch]</a></li>
<li>SCAN: Cross Domain Object Detection with Semantic Conditioned Adaptation <a href="https://www.aaai.org/AAAI22Papers/AAAI-902.LiW.pdf">[AAAI2022]</a> <a href="https://github.com/CityU-AIM-Group/SCAN">[PyTorch]</a></li>
<li>SSAL: Synergizing between Self-Training and Adversarial Learning for Domain Adaptive Object Detection <a href="https://proceedings.neurips.cc/paper/2021/file/c0cccc24dd23ded67404f5e511c342b0-Paper.pdf">[NeurIPS2021]</a> <a href="http://im.itu.edu.pk/synergizing-domain-adaptation/">[Project]</a></li>
<li>Multi-Source Domain Adaptation for Object Detection <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Yao_Multi-Source_Domain_Adaptation_for_Object_Detection_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Knowledge Mining and Transferring for Domain Adaptive Object Detection <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Tian_Knowledge_Mining_and_Transferring_for_Domain_Adaptive_Object_Detection_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Dual Bipartite Graph Learning: A General Approach for Domain Adaptive Object Detection <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Dual_Bipartite_Graph_Learning_A_General_Approach_for_Domain_Adaptive_ICCV_2021_paper.pdf">[ICCV2021]</a></li>
<li>Seeking Similarities over Differences: Similarity-based Domain Alignment for Adaptive Object Detection <a href="https://arxiv.org/pdf/2110.01428.pdf">[ICCV2021]</a></li>
<li>Informative and Consistent Correspondence Mining for Cross-Domain Weakly Supervised Object Detection <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Hou_Informative_and_Consistent_Correspondence_Mining_for_Cross-Domain_Weakly_Supervised_Object_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>MeGA-CDA: Memory Guided Attention for Category-Aware Unsupervised Domain Adaptive Object Detection <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/VS_MeGA-CDA_Memory_Guided_Attention_for_Category-Aware_Unsupervised_Domain_Adaptive_Object_CVPR_2021_paper">[CVPR2021]</a></li>
<li>SRDAN: Scale-aware and Range-aware Domain Adaptation Network<br>for Cross-dataset 3D Object Detection <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_SRDAN_Scale-Aware_and_Range-Aware_Domain_Adaptation_Network_for_Cross-Dataset_3D_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>I3Net: Implicit Instance-Invariant Network for Adapting One-Stage Object Detectors <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_I3Net_Implicit_Instance-Invariant_Network_for_Adapting_One-Stage_Object_Detectors_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>RPN Prototype Alignment for Domain Adaptive Object Detector <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_RPN_Prototype_Alignment_for_Domain_Adaptive_Object_Detector_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>ST3D: Self-training for Unsupervised Domain Adaptation on 3D ObjectDetection <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_ST3D_Self-Training_for_Unsupervised_Domain_Adaptation_on_3D_Object_Detection_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>Domain-Specific Suppression for Adaptive Object Detection <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Domain-Specific_Suppression_for_Adaptive_Object_Detection_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>Unbiased Mean Teacher for Cross-Domain Object Detection <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Deng_Unbiased_Mean_Teacher_for_Cross-Domain_Object_Detection_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>YOLO in the Dark - Domain Adaptation Method for Merging Multiple Models <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660341.pdf">[ECCV2020]</a></li>
<li>Collaborative Training between Region Proposal Localization and Classification for Domain Adaptive Object Detection <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123630086.pdf">[ECCV2020]</a></li>
<li>One-Shot Unsupervised Cross-Domain Detection <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123610715.pdf">[ECCV2020]</a></li>
<li>Every Pixel Matters: Center-aware Feature Alignment for Domain Adaptive Object Detector <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123540698.pdf">[ECCV2020]</a></li>
<li>Adapting Object Detectors with Conditional Domain Normalization <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123560392.pdf">[ECCV2020]</a></li>
<li>Prior-based Domain Adaptive Object Detection for Hazy and Rainy Conditions <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590749.pdf">[ECCV2020]</a></li>
<li>Domain Adaptive Object Detection via Asymmetric Tri-way Faster-RCNN <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123690307.pdf">[ECCV2020]</a></li>
<li>Cross-domain Object Detection through Coarse-to-Fine Feature Adaptation <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Zheng_Cross-domain_Object_Detection_through_Coarse-to-Fine_Feature_Adaptation_CVPR_2020_paper.pdf">[CVPR2020]</a></li>
<li>Harmonizing Transferability and Discriminability for Adapting Object Detectors <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Harmonizing_Transferability_and_Discriminability_for_Adapting_Object_Detectors_CVPR_2020_paper.pdf">[CVPR2020]</a> <a href="https://github.com/chaoqichen/HTCN">[code]</a></li>
<li>Exploring Categorical Regularization for Domain Adaptive Object Detection <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_Exploring_Categorical_Regularization_for_Domain_Adaptive_Object_Detection_CVPR_2020_paper.pdf">[CVPR2020]</a> <a href="https://github.com/Megvii-Nanjing/CR-DA-DET">[code]</a></li>
<li>Cross-domain Detection via Graph-induced Prototype Alignment <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_Cross-Domain_Detection_via_Graph-Induced_Prototype_Alignment_CVPR_2020_paper.pdf">[CVPR2020 Oral]</a> <a href="https://github.com/ChrisAllenMing/GPA-detection">[code]</a></li>
<li>Multi-spectral Salient Object Detection by Adversarial Domain Adaptation <a href="https://cse.sc.edu/~songwang/document/aaai20b.pdf">[Paper]</a></li>
<li>Deep Domain Adaptive Object Detection: a Survey <a href="https://arxiv.org/abs/2002.06797v1">[ICIP2020]</a></li>
<li>Progressive Domain Adaptation for Object Detection <a href="https://arxiv.org/abs/1910.11319">[WACV]</a></li>
<li>Cross-Domain Car Detection Using Unsupervised Image-to-Image Translation: From Day to Night <a href="https://ieeexplore.ieee.org/document/8852008">[IJCNN2019 Oral]</a> <a href="https://github.com/viniciusarruda/cross-domain-car-detection">[Project]</a></li>
<li>Self-Training and Adversarial Background Regularization for Unsupervised Domain Adaptive One-Stage Object Detection <a href="https://arxiv.org/abs/1909.00597v1">[ICCV2019 Oral]</a></li>
<li>A Robust Learning Approach to Domain Adaptive Object Detection <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Khodabandeh_A_Robust_Learning_Approach_to_Domain_Adaptive_Object_Detection_ICCV_2019_paper.pdf">[ICCV2019]</a> <a href="https://github.com/mkhodabandeh/robust_domain_adaptation">[code]</a></li>
<li>Multi-adversarial Faster-RCNN for Unrestricted Object Detection <a href="https://arxiv.org/abs/1907.10343">[ICCV2019]</a></li>
<li>Few-Shot Adaptive Faster R-CNN <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Few-Shot_Adaptive_Faster_R-CNN_CVPR_2019_paper.html">[CVPR2019]</a></li>
<li>Exploring Object Relation in Mean Teacher for Cross-Domain Detection <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Cai_Exploring_Object_Relation_in_Mean_Teacher_for_Cross-Domain_Detection_CVPR_2019_paper.pdf">[CVPR2019]</a></li>
<li>Adapting Object Detectors via Selective Cross-Domain Alignment <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhu_Adapting_Object_Detectors_via_Selective_Cross-Domain_Alignment_CVPR_2019_paper.pdf">[CVPR2019]</a> <a href="https://github.com/xinge008/SCDA">[Pytorch]</a></li>
<li>Automatic adaptation of object detectors to new domains using self-training <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/RoyChowdhury_Automatic_Adaptation_of_Object_Detectors_to_New_Domains_Using_Self-Training_CVPR_2019_paper.pdf">[CVPR2019]</a> <a href="http://vis-www.cs.umass.edu/unsupVideo/">[Project]</a></li>
<li>Towards Universal Object Detection by Domain Attention <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Towards_Universal_Object_Detection_by_Domain_Attention_CVPR_2019_paper.pdf">[CVPR2019]</a></li>
<li>Strong-Weak Distribution Alignment for Adaptive Object Detection <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Saito_Strong-Weak_Distribution_Alignment_for_Adaptive_Object_Detection_CVPR_2019_paper.pdf">[CVPR2019]</a> <a href="https://github.com/VisionLearningGroup/DA_Detection">[Pytorch]</a></li>
<li>Diversify and Match: A Domain Adaptive Representation Learning Paradigm for Object Detection <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Kim_Diversify_and_Match_A_Domain_Adaptive_Representation_Learning_Paradigm_for_CVPR_2019_paper.pdf">[CVPR2019]</a> <a href="https://github.com/TKKim93/DivMatch">[Pytorch]</a></li>
<li>Cross-Domain Weakly-Supervised Object Detection Through Progressive Domain Adaptation <a href="https://arxiv.org/abs/1803.11365">[CVPR2018]</a></li>
<li>Domain Adaptive Faster R-CNN for Object Detection in the Wild <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Domain_Adaptive_Faster_CVPR_2018_paper.pdf">[CVPR2018]</a> <a href="https://github.com/krumo/Detectron-DA-Faster-RCNN">[Caffe2]</a> <a href="https://github.com/yuhuayc/da-faster-rcnn">[Caffe]</a> <a href="">[Pytorch(under developing)]</a></li>
</ul>
<p><strong>Journal</strong></p>
<ul>
<li>Cross-domain object detection using unsupervised image translation <a href="https://www.sciencedirect.com/science/article/abs/pii/S0957417421016328">[ESWA]</a></li>
<li>Pixel and feature level based domain adaptation for object detection in autonomous driving <a href="https://www.sciencedirect.com/science/article/pii/S092523121931149X?via=ihub">[Neurocomputing]</a></li>
</ul>
<p><strong>Arxiv</strong></p>
<ul>
<li>See Eye to Eye: A Lidar-Agnostic 3D Detection Framework for Unsupervised Multi-Target Domain Adaptation <a href="https://arxiv.org/abs/2111.09450">[17 Nov 2021]</a></li>
<li>Unsupervised Domain Adaptive Object Detection using Forward-Backward Cyclic Adaptation <a href="https://arxiv.org/abs/2002.00575v1">[3 Feb 2020]</a></li>
<li>Prior-based Domain Adaptive Object Detection for Adverse Weather Conditions <a href="https://arxiv.org/abs/1912.00070v1">[29 Nov 2019]</a></li>
<li>Unsupervised Domain Adaptation for Object Detection via Cross-Domain Semi-Supervised Learning <a href="https://arxiv.org/abs/1911.07158v1">[17 Nov 2019]</a></li>
<li>Curriculum Self-Paced Learning for Cross-Domain Object Detection <a href="https://arxiv.org/abs/1911.06849v1">[15 Nov 2019]</a></li>
<li>SCL: Towards Accurate Domain Adaptive Object Detection via Gradient Detach Based Stacked Complementary Losses <a href="https://arxiv.org/abs/1911.02559v1">[6 Nov 2019]</a></li>
</ul>
<h3 id="Semantic-Segmentation"><a href="#Semantic-Segmentation" class="headerlink" title="Semantic Segmentation"></a>Semantic Segmentation</h3><p><strong>Conference</strong></p>
<ul>
<li>PiPa: Pixel- and Patch-wise Self-supervised Learning for Domain Adaptative Semantic Segmentation <a href="https://arxiv.org/abs/2211.07609">[ACM MM2023]</a> <a href="https://github.com/chen742/PiPa">[Pytorch]</a></li>
<li>Refign: Align and Refine for Adaptation of Semantic Segmentation to Adverse Conditions <a href="https://arxiv.org/pdf/2207.06825.pdf">[WACV 2023]</a> <a href="https://github.com/brdav/refign">[Pytorch]</a></li>
<li>Deliberated Domain Bridging for Domain Adaptive Semantic Segmentation <a href="https://arxiv.org/abs/2209.07695">[NeruIPS 2022]</a> <a href="https://github.com/xiaoachen98/DDB">[Pytorch]</a></li>
<li>DecoupleNet: Decoupled Network for Domain Adaptive Semantic Segmentation <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136930362.pdf">[ECCV 2022]</a> <a href="https://github.com/dvlab-research/DecoupleNet">[Pytorch]</a></li>
<li>HRDA: Context-Aware High-Resolution Domain-Adaptive Semantic Segmentation <a href="https://arxiv.org/pdf/2204.13132">[ECCV 2022]</a> <a href="https://github.com/lhoyer/HRDA">[Pytorch]</a></li>
<li>Style-Hallucinated Dual Consistency Learning for Domain Generalized Semantic Segmentation <a href="https://arxiv.org/pdf/2204.02548.pdf">[ECCV 2022]</a> <a href="https://github.com/HeliosZhao/SHADE">[Pytorch]</a></li>
<li>DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Hoyer_DAFormer_Improving_Network_Architectures_and_Training_Strategies_for_Domain-Adaptive_Semantic_CVPR_2022_paper.pdf">[CVPR 2022]</a> <a href="https://github.com/lhoyer/DAFormer">[Pytorch]</a></li>
<li>Plugging Self-Supervised Monocular Depth into Unsupervised Domain Adaptation for Semantic Segmentation <a href="https://openaccess.thecvf.com/content/WACV2022/papers/Cardace_Plugging_Self-Supervised_Monocular_Depth_Into_Unsupervised_Domain_Adaptation_for_Semantic_WACV_2022_paper.pdf">[WACV 2022]</a></li>
<li>Shallow Features Guide Unsupervised Domain Adaptation for Semantic Segmentation at Class Boundaries <a href="https://openaccess.thecvf.com/content/WACV2022/papers/Cardace_Shallow_Features_Guide_Unsupervised_Domain_Adaptation_for_Semantic_Segmentation_at_WACV_2022_paper.pdf">[WACV 2022]</a></li>
<li>Learning to Adapt via Latent Domains for Adaptive Semantic Segmentation <a href="https://proceedings.neurips.cc/paper/2021/hash/092cb13c22d51c22b9035a2b4fe76b00-Abstract.html">[NeurIPS2021]</a></li>
<li>Dual Path Learning for Domain Adaptation of Semantic Segmentation <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Cheng_Dual_Path_Learning_for_Domain_Adaptation_of_Semantic_Segmentation_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Exploring Robustness of Unsupervised Domain Adaptation in Semantic Segmentation <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Yang_Exploring_Robustness_of_Unsupervised_Domain_Adaptation_in_Semantic_Segmentation_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Multi-Anchor Active Domain Adaptation for Semantic Segmentation <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Ning_Multi-Anchor_Active_Domain_Adaptation_for_Semantic_Segmentation_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>LabOR: Labeling Only if Required for Domain Adaptive Semantic Segmentation <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Shin_LabOR_Labeling_Only_if_Required_for_Domain_Adaptive_Semantic_Segmentation_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Self-Mutating Network for Domain Adaptive Segmentation in Aerial Images <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Lee_Self-Mutating_Network_for_Domain_Adaptive_Segmentation_in_Aerial_Images_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Geometric Unsupervised Domain Adaptation for Semantic Segmentation <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Guizilini_Geometric_Unsupervised_Domain_Adaptation_for_Semantic_Segmentation_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Multi-Target Adversarial Frameworks for Domain Adaptation in Semantic Segmentation <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Saporta_Multi-Target_Adversarial_Frameworks_for_Domain_Adaptation_in_Semantic_Segmentation_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>BAPA-Net: Boundary Adaptation and Prototype Alignment for Cross-Domain Semantic Segmentation <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Liu_BAPA-Net_Boundary_Adaptation_and_Prototype_Alignment_for_Cross-Domain_Semantic_Segmentation_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>BiMaL: Bijective Maximum Likelihood Approach to Domain Adaptation in Semantic Scene Segmentation <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Truong_BiMaL_Bijective_Maximum_Likelihood_Approach_to_Domain_Adaptation_in_Semantic_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Uncertainty-Aware Pseudo Label Refinery for Domain Adaptive Semantic Segmentation <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Wang_Uncertainty-Aware_Pseudo_Label_Refinery_for_Domain_Adaptive_Semantic_Segmentation_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Domain Adaptive Semantic Segmentation With Self-Supervised Depth Estimation <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Wang_Domain_Adaptive_Semantic_Segmentation_With_Self-Supervised_Depth_Estimation_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Generalize Then Adapt: Source-Free Domain Adaptive Semantic Segmentation <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Kundu_Generalize_Then_Adapt_Source-Free_Domain_Adaptive_Semantic_Segmentation_ICCV_2021_paper.pdf">[ICCV2021]</a></li>
<li>DARCNN: Domain Adaptive Region-Based Convolutional Neural Network for Unsupervised Instance Segmentation in Biomedical Images <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Hsu_DARCNN_Domain_Adaptive_Region-Based_Convolutional_Neural_Network_for_Unsupervised_Instance_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>DANNet: A One-Stage Domain Adaptation Network for Unsupervised Nighttime Semantic Segmentation <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wu_DANNet_A_One-Stage_Domain_Adaptation_Network_for_Unsupervised_Nighttime_Semantic_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>Prototypical Pseudo Label Denoising and Target Structure Learning for Domain Adaptive Semantic Segmentation <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Prototypical_Pseudo_Label_Denoising_and_Target_Structure_Learning_for_Domain_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>Complete &amp; Label: A Domain Adaptation Approach to Semantic Segmentation of LiDAR Point Clouds <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yi_Complete__Label_A_Domain_Adaptation_Approach_to_Semantic_Segmentation_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>Cluster, Split, Fuse, and Update: Meta-Learning for Open Compound Domain Adaptive Semantic Segmentation <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Gong_Cluster_Split_Fuse_and_Update_Meta-Learning_for_Open_Compound_Domain_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>PixMatch: Unsupervised Domain Adaptation via Pixelwise Consistency Training <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Melas-Kyriazi_PixMatch_Unsupervised_Domain_Adaptation_via_Pixelwise_Consistency_Training_CVPR_2021_paper.pdf">[CVPR2021]</a> <a href="https://github.com/lukemelas/pixmatch">[Pytorch]</a></li>
<li>Learning to Relate Depth and Semantics for Unsupervised Domain Adaptation <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Saha_Learning_To_Relate_Depth_and_Semantics_for_Unsupervised_Domain_Adaptation_CVPR_2021_paper.pdf">[CVPR2021]</a> <a href="https://github.com/susaha/ctrl-uda">[Pytorch]</a></li>
<li>Cross-View Regularization for Domain Adaptive Panoptic Segmentation <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Huang_Cross-View_Regularization_for_Domain_Adaptive_Panoptic_Segmentation_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>Semi-supervised Domain Adaptation based on Dual-level Domain Mixing for Semantic Segmentation <a href="https://arxiv.org/abs/2103.04705v1">[CVPR2021]</a></li>
<li>MetaCorrection: Domain-aware Meta Loss Correction for Unsupervised Domain Adaptation in Semantic Segmentation <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Guo_MetaCorrection_Domain-Aware_Meta_Loss_Correction_for_Unsupervised_Domain_Adaptation_in_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>Coarse-to-Fine Domain Adaptive Semantic Segmentation with Photometric Alignment and Category-Center Regularization <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ma_Coarse-To-Fine_Domain_Adaptive_Semantic_Segmentation_With_Photometric_Alignment_and_Category-Center_CVPR_2021_paper.pdf">[CVPR2021]</a> </li>
<li>Multi-Source Domain Adaptation with Collaborative Learning for Semantic Segmentation <a href="https://arxiv.org/abs/2103.04717v2">[CVPR2021]</a></li>
<li>Source-Free Domain Adaptation for Semantic Segmentation <a href="https://arxiv.org/abs/2103.16372v1">[CVPR2021]</a></li>
<li>Instance Adaptive Self-Training for Unsupervised Domain Adaptation <a href="https://arxiv.org/abs/2008.12197">[ECCV 2020]</a> <a href="https://github.com/bupt-ai-cz/IAST-ECCV2020">[Pytorch]</a></li>
<li>Cross-stained Segmentation from Renal Biopsy Images Using Multi-level Adversarial Learning <a href="https://arxiv.org/abs/2002.08587">[ICASSP 2020]</a></li>
<li>Pixel-Level Cycle Association: A New Perspective for Domain Adaptive Semantic Segmentation <a href="https://arxiv.org/abs/2011.00147v1">[NeurlIPS 2020]</a> <a href="https://github.com/kgl-prml/Pixel-Level-Cycle-Association">[Pytorch]</a></li>
<li>Adversarial Style Mining for One-Shot Unsupervised Domain Adaptation <a href="https://proceedings.neurips.cc/paper/2020/hash/ed265bc903a5a097f61d3ec064d96d2e-Abstract.html">[NeurIPS2020]</a> <a href="https://github.com/RoyalVane/ASM">[Pytorch]</a></li>
<li>Semantically Adaptive Image-to-image Translation for Domain Adaptation of Semantic Segmentation <a href="https://arxiv.org/abs/2009.01166">[BMVC2020]</a></li>
<li>Contextual-Relation Consistent Domain Adaptation for Semantic Segmentation <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123600698.pdf">[ECCV2020]</a></li>
<li>Learning from Scale-Invariant Examples for Domain Adaptation in Semantic Segmentation <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123670290.pdf">[ECCV2020]</a></li>
<li>Label-Driven Reconstruction for Domain Adaptation in Semantic Segmentation <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123720477.pdf">[ECCV2020]</a></li>
<li>Unsupervised Domain Adaptation for Semantic Segmentation of NIR Images through Generative Latent Search <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123510409.pdf">[ECCV2020]</a></li>
<li>Domain Adaptive Semantic Segmentation Using Weak Labels <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123540545.pdf">[ECCV2020]</a>  </li>
<li>Content-Consistent Matching for Domain Adaptive Semantic Segmentation <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590426.pdf">[ECCV2020]</a> <a href="https://github.com/Solacex/CCM">[PyTorch]</a></li>
<li>Cross-Domain Semantic Segmentation via Domain-Invariant Interactive Relation Transfer <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Lv_Cross-Domain_Semantic_Segmentation_via_Domain-Invariant_Interactive_Relation_Transfer_CVPR_2020_paper.pdf">[CVPR2020]</a></li>
<li>Phase Consistent Ecological Domain Adaptation <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Phase_Consistent_Ecological_Domain_Adaptation_CVPR_2020_paper.pdf">[CVPR2020]</a> <a href="https://github.com/donglao/PCEDA">[Pytorch]</a></li>
<li>FDA: Fourier Domain Adaptation for Semantic Segmentation <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_FDA_Fourier_Domain_Adaptation_for_Semantic_Segmentation_CVPR_2020_paper.pdf">[CVPR2020]</a> <a href="https://github.com/YanchaoYang/FDA/blob/master/SStrain.py">[Pytorch]</a></li>
<li>Unsupervised Instance Segmentation in Microscopy Images via Panoptic Domain Adaptation and Task Re-weighting <a href="https://arxiv.org/abs/2005.02066v1">[CVPR2020]</a></li>
<li>Unsupervised Intra-domain Adaptation for Semantic Segmentation through Self-Supervision <a href="https://arxiv.org/abs/2004.07703v1">[CVPR2020 Oral]</a> <a href="https://github.com/feipan664/IntraDA">[Pytorch]</a></li>
<li>Differential Treatment for Stuff and Things: A Simple Unsupervised Domain Adaptation Method for Semantic Segmentation <a href="https://arxiv.org/abs/2003.08040v1">[CVPR2020]</a></li>
<li>Learning Texture Invariant Representation for Domain Adaptation of Semantic Segmentation <a href="https://arxiv.org/abs/2003.00867v2">[CVPR2020]</a> <a href="https://github.com/MyeongJin-Kim/Learning-Texture-Invariant-Representation">[Pytorch]</a></li>
<li>xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation <a href="https://arxiv.org/abs/1911.12676">[CVPR2020]</a> <a href="https://www.youtube.com/watch?v=WgvBBCEKQVE">[Demo]</a> <a href="https://github.com/valeoai/xmuda">[code]</a></li>
<li>Unsupervised Scene Adaptation with Memory Regularization in vivo <a href="https://arxiv.org/abs/1912.11164">[IJCAI2020]</a> <a href="https://github.com/layumi/Seg-Uncertainty">[code]</a></li>
<li>Joint Adversarial Learning for Domain Adaptation in Semantic Segmentation <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6169">[AAAI2020]</a></li>
<li>An Adversarial Perturbation Oriented Domain Adaptation Approach for Semantic Segmentation <a href="https://arxiv.org/abs/1912.08954v1">[AAAI2020]</a></li>
<li>Category Anchor-Guided Unsupervised Domain Adaptation for Semantic Segmentation <a href="https://arxiv.org/abs/1910.13049">[NeurIPS2019]</a> <a href="https://github.com/RogerZhangzz/CAG_UDA">[code]</a></li>
<li>MLSL: Multi-Level Self-Supervised Learning for Domain Adaptation with Spatially Independent and Semantically Consistent Labeling <a href="https://arxiv.org/abs/1909.13776">[WACV2020]</a></li>
<li>Domain Bridge for Unpaired Image-to-Image Translation and Unsupervised Domain Adaptation <a href="https://arxiv.org/abs/1910.10563">[WACV2020]</a></li>
<li>Guided Curriculum Model Adaptation and Uncertainty-Aware Evaluation for<br>Semantic Nighttime Image Segmentation <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Sakaridis_Guided_Curriculum_Model_Adaptation_and_Uncertainty-Aware_Evaluation_for_Semantic_Nighttime_ICCV_2019_paper.pdf">[ICCV2019]</a></li>
<li>Constructing Self-motivated Pyramid Curriculums for Cross-Domain Semantic<br>Segmentation: A Non-Adversarial Approach <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Lian_Constructing_Self-Motivated_Pyramid_Curriculums_for_Cross-Domain_Semantic_Segmentation_A_Non-Adversarial_ICCV_2019_paper.pdf">[ICCV2019]</a> <a href="https://github.com/lianqing11/pycda">[Pytorch]</a></li>
<li>SSF-DAN: Separated Semantic Feature Based Domain Adaptation Network for Semantic Segmentation <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Du_SSF-DAN_Separated_Semantic_Feature_Based_Domain_Adaptation_Network_for_Semantic_ICCV_2019_paper.pdf">[ICCV2019]</a></li>
<li>Significance-aware Information Bottleneck for Domain Adaptive Semantic Segmentation <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Luo_Significance-Aware_Information_Bottleneck_for_Domain_Adaptive_Semantic_Segmentation_ICCV_2019_paper.pdf">[ICCV2019]</a></li>
<li>Domain Adaptation for Semantic Segmentation with Maximum Squares Loss <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Domain_Adaptation_for_Semantic_Segmentation_With_Maximum_Squares_Loss_ICCV_2019_paper.pdf">[ICCV2019]</a> <a href="https://github.com/ZJULearning/MaxSquareLoss">[Pytorch]</a></li>
<li>Self-Ensembling with GAN-based Data Augmentation for Domain Adaptation in Semantic Segmentation <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Choi_Self-Ensembling_With_GAN-Based_Data_Augmentation_for_Domain_Adaptation_in_Semantic_ICCV_2019_paper.pdf">[ICCV2019]</a></li>
<li>DADA: Depth-aware Domain Adaptation in Semantic Segmentation <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Vu_DADA_Depth-Aware_Domain_Adaptation_in_Semantic_Segmentation_ICCV_2019_paper.pdf">[ICCV2019]</a> <a href="https://github.com/valeoai/DADA">[code]</a></li>
<li>Domain Adaptation for Structured Output via Discriminative Patch Representations <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Tsai_Domain_Adaptation_for_Structured_Output_via_Discriminative_Patch_Representations_ICCV_2019_paper.pdf">[ICCV2019 Oral]</a> <a href="https://sites.google.com/site/yihsuantsai/research/iccv19-adapt-seg">[Project]</a></li>
<li>Not All Areas Are Equal: Transfer Learning for Semantic Segmentation via Hierarchical Region Selection <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Sun_Not_All_Areas_Are_Equal_Transfer_Learning_for_Semantic_Segmentation_CVPR_2019_paper.pdf">[CVPR2019(Oral)]</a></li>
<li>CrDoCo: Pixel-level Domain Transfer with Cross-Domain Consistency <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_CrDoCo_Pixel-Level_Domain_Transfer_With_Cross-Domain_Consistency_CVPR_2019_paper.pdf">[CVPR2019]</a> <a href="https://yunchunchen.github.io/CrDoCo/">[Project]</a> <a href="https://github.com/YunChunChen/CrDoCo-pytorch">[Pytorch]</a></li>
<li>Bidirectional Learning for Domain Adaptation of Semantic Segmentation <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Bidirectional_Learning_for_Domain_Adaptation_of_Semantic_Segmentation_CVPR_2019_paper.pdf">[CVPR2019]</a> <a href="https://github.com/liyunsheng13/BDL">[Pytorch]</a></li>
<li>Learning Semantic Segmentation from Synthetic Data: A Geometrically Guided Input-Output Adaptation Approach <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Learning_Semantic_Segmentation_From_Synthetic_Data_A_Geometrically_Guided_Input-Output_CVPR_2019_paper.pdf">[CVPR2019]</a></li>
<li>All about Structure: Adapting Structural Information across Domains for Boosting Semantic Segmentation <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Chang_All_About_Structure_Adapting_Structural_Information_Across_Domains_for_Boosting_CVPR_2019_paper.pdf">[CVPR2019]</a> <a href="https://github.com/a514514772/DISE-Domain-Invariant-Structure-Extraction">[Pytorch]</a></li>
<li>DLOW: Domain Flow for Adaptation and Generalization <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Gong_DLOW_Domain_Flow_for_Adaptation_and_Generalization_CVPR_2019_paper.pdf">[CVPR2019 Oral]</a></li>
<li>Taking A Closer Look at Domain Shift: Category-level Adversaries for Semantics Consistent Domain Adaptation <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Luo_Taking_a_Closer_Look_at_Domain_Shift_Category-Level_Adversaries_for_CVPR_2019_paper.pdf">[CVPR2019 Oral]</a> <a href="https://github.com/RoyalVane/CLAN">[Pytorch]</a></li>
<li>ADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Vu_ADVENT_Adversarial_Entropy_Minimization_for_Domain_Adaptation_in_Semantic_Segmentation_CVPR_2019_paper.pdf">[CVPR2019 Oral]</a> <a href="https://github.com/valeoai/ADVENT">[Pytorch]</a></li>
<li>SPIGAN: Privileged Adversarial Learning from Simulation <a href="https://openreview.net/forum?id=rkxoNnC5FQ">[ICLR2019]</a></li>
<li>Penalizing Top Performers: Conservative Loss for Semantic Segmentation Adaptation <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Xinge_Zhu_Penalizing_Top_Performers_ECCV_2018_paper.pdf">[ECCV2018]</a></li>
<li>Domain transfer through deep activation matching <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Haoshuo_Huang_Domain_transfer_through_ECCV_2018_paper.pdf">[ECCV2018]</a></li>
<li>Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yang_Zou_Unsupervised_Domain_Adaptation_ECCV_2018_paper.pdf">[ECCV2018]</a> <a href="https://github.com/yzou2/CBST">[Pytorch]</a></li>
<li>DCAN: Dual channel-wise alignment networks for unsupervised scene adaptation <a href="https://eccv2018.org/openaccess/content_ECCV_2018/papers/Zuxuan_Wu_DCAN_Dual_Channel-wise_ECCV_2018_paper.pdf">[ECCV2018]</a> </li>
<li>Fully convolutional adaptation networks for semantic<br>segmentation <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Fully_Convolutional_Adaptation_CVPR_2018_paper.pdf">[CVPR2018]</a></li>
<li>Learning to Adapt Structured Output Space for Semantic Segmentation <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Tsai_Learning_to_Adapt_CVPR_2018_paper.pdf">[CVPR2018]</a> <a href="https://github.com/wasidennis/AdaptSegNet">[Pytorch]</a></li>
<li>Conditional Generative Adversarial Network for Structured Domain Adaptation <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Hong_Conditional_Generative_Adversarial_CVPR_2018_paper.pdf">[CVPR2018]</a></li>
<li>Learning From Synthetic Data: Addressing Domain Shift for Semantic Segmentation <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Sankaranarayanan_Learning_From_Synthetic_CVPR_2018_paper.pdf">[CVPR2018]</a> <a href="https://github.com/swamiviv/LSD-seg">[Pytorch]</a></li>
<li>Curriculum Domain Adaptation for Semantic Segmentation of Urban Scenes <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhang_Curriculum_Domain_Adaptation_ICCV_2017_paper.pdf">[ICCV2017]</a> <a href="https://arxiv.org/abs/1812.09953v3">[Journal Version]</a> <a href="https://github.com/YangZhang4065/AdaptationSeg">[Keras]</a></li>
<li>No more discrimination: Cross city adaptation of road scene segmenters <a href="http://openaccess.thecvf.com/content_ICCV_2017/supplemental/Chen_No_More_Discrimination_ICCV_2017_supplemental.pdf">[ICCV2017]</a></li>
</ul>
<p><strong>Journal</strong></p>
<ul>
<li>SePiCo: Semantic-Guided Pixel Contrast for Domain Adaptive Semantic Segmentation <a href="https://ieeexplore.ieee.org/document/10018569">[TPAMI2023]</a><a href="https://arxiv.org/abs/2204.08808">[arxiv version]</a><a href="https://github.com/BIT-DA/SePiCo">[Pytorch]</a></li>
<li>Adaptive Boosting for Domain Adaptation: Towards Robust Predictions in Scene Segmentation <a href="https://arxiv.org/abs/2103.15685">[TIP2022]</a><a href="https://github.com/layumi/AdaBoost_Seg">[Pytorch]</a></li>
<li>Rectifying Pseudo Label Learning via Uncertainty Estimation for Domain Adaptive Semantic Segmentation <a href="https://arxiv.org/abs/2003.03773">[IJCV2020]</a><a href="https://github.com/layumi/Seg-Uncertainty">[Pytorch]</a></li>
<li>Multi-level colonoscopy malignant tissue detection with adversarial CAC-UNet <a href="https://arxiv.org/abs/2006.15954">[Neurocomputing 2021]</a> <a href="https://github.com/bupt-ai-cz/CAC-UNet-DigestPath2019">[Pytorch]</a></li>
<li>Affinity Space Adaptation for Semantic Segmentation Across Domains <a href="https://arxiv.org/abs/2009.12559">[TIP2020]</a><a href="https://github.com/idealwei/ASANet">[Pytorch]</a></li>
<li>Semantic-aware short path adversarial training for cross-domain semantic segmentation <a href="https://www.sciencedirect.com/science/article/pii/S0925231219315656#fig0002">[Neurocomputing 2019]</a> </li>
<li>Weakly Supervised Adversarial Domain Adaptation for Semantic Segmentation in Urban Scenes <a href="https://arxiv.org/abs/1904.09092v1">[TIP]</a></li>
</ul>
<p><strong>Arxiv</strong></p>
<ul>
<li>Class-Conditional Domain Adaptation on Semantic Segmentation <a href="https://arxiv.org/abs/1911.11981v1">[27 Nov 2019]</a></li>
<li>Adversarial Learning and Self-Teaching Techniques for Domain Adaptation in Semantic Segmentation <a href="https://arxiv.org/abs/1909.00781v1">[2 Sep 2019]</a></li>
<li>FCNs in the Wild: Pixel-level Adversarial and Constraint-based Adaptation <a href="https://arxiv.org/abs/1612.02649">[8 Dec 2016]</a></li>
<li>BoMuDA: Boundless Multi-Source Domain Adaptive Segmentation in Unconstrained Environments <a href="https://arxiv.org/abs/2010.03523">[13 Oct 2020]</a><a href="https://github.com/divyakraman/BoMuDA-Boundless-Multi-Source-Domain-Adaptive-Segmentation-in-Unstructured-Environments">[Pytorch]</a></li>
<li>SAfE: Self-Attention Based Unsupervised Road Safety Classification in Hazardous Environments <a href="https://arxiv.org/abs/2012.08939">[27 Nov 2020]</a><a href="https://github.com/divyakraman/SAfE-Self-Attention-Based-Unsupervised-Road-Safety-Classification-in-Hazardous-Environments">[Pytorch]</a></li>
<li>Semantics-aware Multi-modal Domain Translation:From LiDAR Point Clouds to Panoramic Color Images <a href="https://arxiv.org/abs/2106.13974">[26 Jun 2021]</a> <a href="https://github.com/halmstad-University/TITAN-NET">[Pytorch]</a></li>
</ul>
<h3 id="Person-Re-identification"><a href="#Person-Re-identification" class="headerlink" title="Person Re-identification"></a>Person Re-identification</h3><p><strong>Conference</strong></p>
<ul>
<li>Learning to Generalize Unseen Domains via Memory-based Multi-Source Meta-Learning for Person Re-Identification <a href="https://arxiv.org/pdf/2012.00417.pdf">[CVPR 2021]</a> <a href="https://github.com/HeliosZhao/M3L">[Pytorch]</a></li>
<li>Group-aware Label Transfer for Domain Adaptive Person Re-identification <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_Group-aware_Label_Transfer_for_Domain_Adaptive_Person_Re-identification_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>Unsupervised Domain Adaptation in the Dissimilarity Space for Person Re-identification <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123720154.pdf">[ECCV2020]</a></li>
<li>Joint Visual and Temporal Consistency for Unsupervised Domain Adaptive Person Re-Identification <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123690477.pdf">[ECCV2020]</a></li>
<li>Joint Disentangling and Adaptation for Cross-Domain Person Re-Identification <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123470086.pdf">[ECV2020]</a></li>
<li>Multiple Expert Brainstorming for Domain Adaptive Person Re-identification <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520579.pdf">[ECCV2020]</a></li>
<li>Deep Credible Metric Learning for Unsupervised Domain Adaptation Person Re-identification <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123530630.pdf">[ECCV2020]</a></li>
<li>Unsupervised Domain Adaptation with Noise Resistible Mutual-Training for Person Re-identification <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123560511.pdf">[ECCV2020]</a></li>
<li>Generalizing Person Re-Identification by Camera-Aware Invariance Learning and Cross-Domain Mixup <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123600222.pdf">[ECCV2020]</a></li>
<li>AD-Cluster: Augmented Discriminative Clustering for Domain Adaptive Person Re-identification <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhai_AD-Cluster_Augmented_Discriminative_Clustering_for_Domain_Adaptive_Person_Re-Identification_CVPR_2020_paper.pdf">[CVPR2020]</a></li>
<li>Smoothing Adversarial Domain Attack and P-Memory Reconsolidation for Cross-Domain Person Re-Identification <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Smoothing_Adversarial_Domain_Attack_and_P-Memory_Reconsolidation_for_Cross-Domain_Person_CVPR_2020_paper.pdf">[CVPR2020]</a></li>
<li>Cross-Modal Cross-Domain Moment Alignment Network for Person Search <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Jing_Cross-Modal_Cross-Domain_Moment_Alignment_Network_for_Person_Search_CVPR_2020_paper.pdf">[CVPR2020]</a></li>
<li>Online Joint Multi-Metric Adaptation From Frequent Sharing-Subset Mining for Person Re-Identification <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_Online_Joint_Multi-Metric_Adaptation_From_Frequent_Sharing-Subset_Mining_for_Person_CVPR_2020_paper.pdf">[CVPR2020]</a></li>
<li>Mutual Mean-Teaching: Pseudo Label Refinery for Unsupervised Domain Adaptation on Person Re-identification <a href="https://openreview.net/forum?id=rJlnOhVYPS">[ICLR2020]</a> <a href="https://github.com/yxgeee/MMT">[Pytorch]</a></li>
<li>Self-similarity Grouping: A Simple Unsupervised Cross Domain Adaptation Approach for Person Re-identification <a href="https://arxiv.org/abs/1811.10144">[ICCV2019 Oral]</a> <a href="https://github.com/OasisYang/SSG">[Pytorch]</a></li>
<li>A Novel Unsupervised Camera-aware Domain Adaptation Framework for Person Re-identification <a href="https://arxiv.org/abs/1904.03425">[ICCV2019]</a></li>
<li>Invariance Matters: Exemplar Memory for Domain Adaptive Person Re-identification <a href="https://arxiv.org/abs/1904.01990v1">[CVPR2019]</a> <a href="https://github.com/zhunzhong07/ECN">[Pytorch]</a></li>
<li>Domain Adaptation through Synthesis for Unsupervised Person Re-identification <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Slawomir_Bak_Domain_Adaptation_through_ECCV_2018_paper.pdf">[ECCV2018]</a></li>
<li>Person Transfer GAN to Bridge Domain Gap for Person Re-Identification <a href="https://arxiv.org/abs/1711.08565v2">[CVPR2018]</a> </li>
<li>Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification <a href="https://arxiv.org/abs/1711.07027v3">[CVPR2018]</a></li>
</ul>
<p><strong>Arxiv</strong></p>
<ul>
<li>Domain Adaptation for Semantic Segmentation via Patch-Wise Contrastive Learning <a href="https://arxiv.org/abs/2104.11056">[arXiv 22 Apr 2021]</a></li>
<li>Structured Domain Adaptation for Unsupervised Person Re-identification <a href="https://arxiv.org/abs/2003.06650">[arXiv 14 Mar 2020]</a></li>
<li>Domain Adaptive Attention Model for Unsupervised Cross-Domain Person Re-Identification <a href="https://arxiv.org/abs/1905.10529">[arXiv 25 May 2019]</a></li>
<li>Camera Adversarial Transfer for Unsupervised Person Re-Identification <a href="https://arxiv.org/abs/1904.01308">[arXiv 2 Apr 2019]</a></li>
<li>EANet: Enhancing Alignment for Cross-Domain Person Re-identification <a href="https://arxiv.org/abs/1812.11369">[arXiv 29 Dec 2018]</a> <a href="https://github.com/huanghoujing/EANet">[Pytorch]</a></li>
<li>One Shot Domain Adaptation for Person Re-Identification <a href="https://arxiv.org/abs/1811.10144v1">[arXiv 26 Nov 2018]</a></li>
<li>Similarity-preserving Image-image Domain Adaptation for Person Re-identification <a href="https://arxiv.org/abs/1811.10551v1">[arXiv 26 Nov 2018]</a></li>
</ul>
<h3 id="Sim-to-Real-Transfer"><a href="#Sim-to-Real-Transfer" class="headerlink" title="Sim-to-Real Transfer"></a>Sim-to-Real Transfer</h3><p><strong>Conference</strong></p>
<ul>
<li>DIRL: Domain-Invariant Reperesentation Learning Approach for Sim-to-Real Transfer <a href="https://arxiv.org/abs/2011.07589">[CoRL2020]</a> <a href="https://www.sites.google.com/view/dirl">[Project]</a></li>
</ul>
<h3 id="Video-Domain-Adaptation"><a href="#Video-Domain-Adaptation" class="headerlink" title="Video Domain Adaptation"></a>Video Domain Adaptation</h3><p><strong>Conference</strong></p>
<ul>
<li>Overcoming Label Noise for Source-free Unsupervised Video Domain Adaptation [[ICVGIP’22]] (<a href="https://dl.acm.org/doi/abs/10.1145/3571600.3571621">https://dl.acm.org/doi/abs/10.1145/3571600.3571621</a>) [Pytorch]](<a href="https://github.com/avijit9/CleanAdapt">https://github.com/avijit9/CleanAdapt</a>) [[Project]] (<a href="https://avijit9.github.io/CleanAdapt/">https://avijit9.github.io/CleanAdapt/</a>) [[Extended Version]] (<a href="https://arxiv.org/abs/2311.18572">https://arxiv.org/abs/2311.18572</a>)</li>
<li>Source-free Video Domain Adaptation by Learning Temporal Consistency for Action Recognition <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136940144.pdf">[ECCV2022]</a> <a href="https://github.com/xuyu0010/ATCoN">[Pytorch]</a> <a href="https://xuyu0010.github.io/sfvda.html">[Project]</a></li>
<li>Contrast and Mix: Temporal Contrastive Video Domain Adaptation with Background Mixing <a href="https://arxiv.org/pdf/2110.15128.pdf">[NeurIPS2021]</a></li>
<li>Learning Cross-Modal Contrastive Features for Video Domain Adaptation <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Kim_Learning_Cross-Modal_Contrastive_Features_for_Video_Domain_Adaptation_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Partial Video Domain Adaptation With Partial Adversarial Temporal Attentive Network <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Xu_Partial_Video_Domain_Adaptation_With_Partial_Adversarial_Temporal_Attentive_Network_ICCV_2021_paper.html">[ICCV2021]</a> <a href="https://github.com/xuyu0010/PATAN">[Pytorch]</a></li>
<li>Domain Adaptive Video Segmentation via Temporal Consistency Regularization <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Guan_Domain_Adaptive_Video_Segmentation_via_Temporal_Consistency_Regularization_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Shuffle and Attend: Video Domain Adaptation <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123570664.pdf">[ECCV2020]</a></li>
<li>Transferring Cross-Domain Knowledge for Video Sign Language Recognition <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Transferring_Cross-Domain_Knowledge_for_Video_Sign_Language_Recognition_CVPR_2020_paper.pdf">[CVPR2020]</a></li>
<li>Action Segmentation with Joint Self-Supervised Temporal Domain Adaptation <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Action_Segmentation_With_Joint_Self-Supervised_Temporal_Domain_Adaptation_CVPR_2020_paper.pdf">[CVPR2020]</a> <a href="https://github.com/cmhungsteve/SSTDA">[Pytorch]</a></li>
<li>Transferring Cross-domain Knowledge for Video Sign Language Recognition <a href="https://arxiv.org/abs/2003.03703v2">[CVPR2020 Oral]</a></li>
<li>Multi-Modal Domain Adaptation for Fine-Grained Action Recognition <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Munro_Multi-Modal_Domain_Adaptation_for_Fine-Grained_Action_Recognition_CVPR_2020_paper.pdf">[CVPR2020 Oral]</a></li>
<li>Adversarial Cross-Domain Action Recognition with Co-Attention <a href="https://arxiv.org/abs/1912.10405v1">[AAAI2020]</a></li>
<li>Generative Adversarial Networks for Video-to-Video Domain Adaptation <a href="https://www.aaai.org/Papers/AAAI/2020GB/AAAI-ChenJ.1453.pdf">[Paper]</a></li>
<li>Temporal Attentive Alignment for Large-Scale Video Domain Adaptation <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Temporal_Attentive_Alignment_for_Large-Scale_Video_Domain_Adaptation_ICCV_2019_paper.pdf">[ICCV2019 Oral]</a> <a href="https://github.com/olivesgatech/TA3N">[Pytorch]</a></li>
<li>Temporal Attentive Alignment for Video Domain Adaptation <a href="https://arxiv.org/abs/1905.10861v5">[CVPRW 2019]</a> <a href="https://github.com/olivesgatech/TA3N">[Pytorch]</a></li>
</ul>
<p><strong>Arxiv</strong></p>
<ul>
<li>Video Unsupervised Domain Adaptation with Deep Learning: A Comprehensive Survey <a href="https://arxiv.org/abs/2211.10412">[17 Nov 2022]</a> <a href="https://github.com/xuyu0010/awesome-video-domain-adaptation">[project]</a></li>
<li>Unsupervised Video Domain Adaptation: A Disentanglement Perspective <a href="https://arxiv.org/abs/2208.07365">[15 Aug 2022]</a> <a href="https://github.com/ldkong1205/TranSVAE">[Pyotrch]</a> <a href="https://ldkong.com/TranSVAE">[Project]</a> <a href="https://huggingface.co/spaces/ldkong/TranSVAE">[Gradio Demo]</a></li>
<li>Image to Video Domain Adaptation Using Web Supervision <a href="https://arxiv.org/abs/1908.01449">[5 Aug 2019]</a></li>
</ul>
<h3 id="Medical-Related"><a href="#Medical-Related" class="headerlink" title="Medical Related"></a>Medical Related</h3><p><strong>Conference</strong></p>
<ul>
<li>PopGenAdapt: Semi-Supervised Domain Adaptation for Genotype-to-Phenotype Prediction in Underrepresented Populations <a href="https://psb.stanford.edu/psb-online/proceedings/psb24/comajoan.pdf">[PSB 2024]</a></li>
<li>Cross-stained Segmentation from Renal Biopsy Images Using Multi-level Adversarial Learning <a href="https://arxiv.org/abs/2002.08587">[ICASSP 2020]</a></li>
<li>What Can Be Transferred: Unsupervised Domain Adaptation for Endoscopic Lesions Segmentation <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Dong_What_Can_Be_Transferred_Unsupervised_Domain_Adaptation_for_Endoscopic_Lesions_CVPR_2020_paper.pdf">[Paper]</a></li>
<li>Semantic-Transferable Weakly-Supervised Endoscopic Lesions Segmentation <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Dong_Semantic-Transferable_Weakly-Supervised_Endoscopic_Lesions_Segmentation_ICCV_2019_paper.pdf">[ICCV2019]</a></li>
</ul>
<p><strong>Journal</strong></p>
<ul>
<li>Multi-level colonoscopy malignant tissue detection with adversarial CAC-UNet <a href="https://arxiv.org/abs/2006.15954">[Neurocomputing 2021]</a> <a href="https://github.com/bupt-ai-cz/CAC-UNet-DigestPath2019">[Pytorch]</a></li>
</ul>
<p><strong>Arxiv</strong></p>
<ul>
<li>On-the-Fly Test-time Adaptation for Medical Image Segmentation <a href="https://arxiv.org/abs/2203.05574">[10 Mar 2022]</a> <a href="https://github.com/jeya-maria-jose/On-The-Fly-Adaptation">[Pytorch]</a></li>
<li>Target and task specific source-free domain adaptive image segmentation <a href="https://arxiv.org/abs/2203.15792">[10 Mar 2023]</a> <a href="https://github.com/Vibashan/tt-sfuda">[Pytorch]</a></li>
<li>Unsupervised Domain Adaptation via Disentangled Representations: Application to Cross-Modality Liver Segmentation <a href="https://arxiv.org/abs/1907.13590">[arXiv 29 Aug 2019]</a></li>
<li>Synergistic Image and Feature Adaptation: Towards Cross-Modality Domain Adaptation for Medical Image Segmentation <a href="https://arxiv.org/abs/1901.08211v1">[arXiv on 24 Jan 2019]</a></li>
<li>Unsupervised domain adaptation for medical imaging segmentation with self-ensembling <a href="https://arxiv.org/abs/1811.06042v1">[arXiv 14 Nov 2018]</a></li>
</ul>
<h3 id="Monocular-Depth-Estimation"><a href="#Monocular-Depth-Estimation" class="headerlink" title="Monocular Depth Estimation"></a>Monocular Depth Estimation</h3><ul>
<li>Geometry-Aware Symmetric Domain Adaptation for Monocular Depth Estimation <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhao_Geometry-Aware_Symmetric_Domain_Adaptation_for_Monocular_Depth_Estimation_CVPR_2019_paper.pdf">[CVPR2019]</a></li>
<li>Real-Time Monocular Depth Estimation using Synthetic Data with Domain Adaptation via Image Style Transfer <a href="http://breckon.eu/toby/publications/papers/abarghouei18monocular.pdf">[CVPR2018]</a></li>
</ul>
<h3 id="3D"><a href="#3D" class="headerlink" title="3D"></a>3D</h3><p><strong>Conference</strong></p>
<ul>
<li>SPG: Unsupervised Domain Adaptation for 3D Object Detection via Semantic Point Generation <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Xu_SPG_Unsupervised_Domain_Adaptation_for_3D_Object_Detection_via_Semantic_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Sparse-to-Dense Feature Matching: Intra and Inter Domain Cross-Modal Learning in Domain Adaptation for 3D Semantic Segmentation <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Peng_Sparse-to-Dense_Feature_Matching_Intra_and_Inter_Domain_Cross-Modal_Learning_in_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Unsupervised Domain Adaptive 3D Detection With Multi-Level Consistency <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Luo_Unsupervised_Domain_Adaptive_3D_Detection_With_Multi-Level_Consistency_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Domain-Adaptive Single-View 3D Reconstruction <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Pinheiro_Domain-Adaptive_Single-View_3D_Reconstruction_ICCV_2019_paper.pdf">[ICCV2019]</a></li>
</ul>
<p><strong>Arxiv</strong></p>
<ul>
<li>MS3D++: Ensemble of Experts for Multi-Source Unsupervised Domain Adaptation in 3D Object Detection <a href="https://arxiv.org/abs/2308.05988">[Arxiv 11 Aug 2023]</a> <a href="https://github.com/darrenjkt/MS3D">[Pytorch]</a></li>
</ul>
<h3 id="Fine-Grained-Domain"><a href="#Fine-Grained-Domain" class="headerlink" title="Fine-Grained Domain"></a>Fine-Grained Domain</h3><p><strong>Conference</strong></p>
<ul>
<li>Progressive Adversarial Networks for Fine-Grained Domain Adaptation <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Progressive_Adversarial_Networks_for_Fine-Grained_Domain_Adaptation_CVPR_2020_paper.pdf">[CVPR2020]</a> <a href="https://github.com/thuml/PAN">[Pytorch]</a></li>
</ul>
<h3 id="LiDAR"><a href="#LiDAR" class="headerlink" title="LiDAR"></a>LiDAR</h3><p><strong>Conference</strong></p>
<ul>
<li>SALUDA: Surface-based Automotive Lidar Unsupervised Domain Adaptation <a href="https://arxiv.org/abs/2304.03251">[3DV 2024]</a> <a href="https://github.com/valeoai/SALUDA">[Pytorch]</a></li>
<li>GIPSO: Geometrically Informed Propagation for Online Adaptation in 3D LiDAR Segmentation <a href="https://arxiv.org/abs/2207.09763">[ECCV2022]</a>  <a href="https://github.com/saltoricristiano/gipso-sfouda">[Pytorch]</a></li>
<li>CoSMix: Compositional Semantic Mix for Domain Adaptation in 3D LiDAR Segmentation <a href="https://arxiv.org/abs/2207.09778">[ECCV2022]</a> <a href="https://github.com/saltoricristiano/cosmix-uda">[Pytorch]</a></li>
</ul>
<p><strong>ArXiv</strong></p>
<ul>
<li>ConDA: Unsupervised Domain Adaptation for LiDAR Segmentation via Regularized Domain Concatenation <a href="https://arxiv.org/abs/2111.15242">[13 Mar 2022]</a></li>
</ul>
<h3 id="Remote-Sensing"><a href="#Remote-Sensing" class="headerlink" title="Remote Sensing"></a>Remote Sensing</h3><p><strong>Journal</strong></p>
<ul>
<li>Open-Set Black-Box Domain Adaptation for Remote Sensing Image Scene Classification <a href="https://ieeexplore.ieee.org/abstract/document/10210386">[GRSL 2023]</a></li>
</ul>
<h3 id="Others-1"><a href="#Others-1" class="headerlink" title="Others"></a>Others</h3><p><strong>Conference</strong></p>
<ul>
<li>RefRec: Pseudo-labels Refinement via Shape Reconstruction for Unsupervised 3D Domain Adaptation <a href="https://ieeexplore.ieee.org/abstract/document/9665900">[3DV 2021 Oral]</a></li>
<li>Unsupervised Domain Adaptation in LiDAR Semantic Segmentation with Self-Supervision and Gated Adapters <a href="https://arxiv.org/abs/2107.09783">[ICRA2022]</a></li>
<li>RDA: Robust Domain Adaptation via Fourier Adversarial Attacking <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Huang_RDA_Robust_Domain_Adaptation_via_Fourier_Adversarial_Attacking_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Geometry-Aware Self-Training for Unsupervised Domain Adaptation on Object Point Clouds <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Zou_Geometry-Aware_Self-Training_for_Unsupervised_Domain_Adaptation_on_Object_Point_Clouds_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Tune It the Right Way: Unsupervised Validation of Domain Adaptation via Soft Neighborhood Density <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Saito_Tune_It_the_Right_Way_Unsupervised_Validation_of_Domain_Adaptation_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>PIT: Position-Invariant Transform for Cross-FoV Domain Adaptation <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Gu_PIT_Position-Invariant_Transform_for_Cross-FoV_Domain_Adaptation_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Self-Supervised Domain Adaptation for Forgery Localization of JPEG Compressed Images <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Rao_Self-Supervised_Domain_Adaptation_for_Forgery_Localization_of_JPEG_Compressed_Images_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Unsupervised Real-World Super-Resolution: A Domain Adaptation Perspective <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Wang_Unsupervised_Real-World_Super-Resolution_A_Domain_Adaptation_Perspective_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Adversarial Robustness for Unsupervised Domain Adaptation <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Awais_Adversarial_Robustness_for_Unsupervised_Domain_Adaptation_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Collaborative Optimization and Aggregation for Decentralized Domain Generalization and Adaptation <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Wu_Collaborative_Optimization_and_Aggregation_for_Decentralized_Domain_Generalization_and_Adaptation_ICCV_2021_paper.html">[ICCV2021]</a></li>
<li>Adaptive Cross-Modal Prototypes for Cross-Domain Visual-Language Retrieval <a href="https://openaccess.thecvf.com/CVPR2021?day=all">[CVPR2021]</a></li>
<li>Spatio-temporal Contrastive Domain Adaptation for Action Recognition <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Song_Spatio-temporal_Contrastive_Domain_Adaptation_for_Action_Recognition_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>Regressive Domain Adaptation for Unsupervised Keypoint Detection <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Jiang_Regressive_Domain_Adaptation_for_Unsupervised_Keypoint_Detection_CVPR_2021_paper.pdf">[CVPR2021]</a></li>
<li>From Synthetic to Real: Unsupervised Domain Adaptation for Animal Pose Estimation <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Li_From_Synthetic_to_Real_Unsupervised_Domain_Adaptation_for_Animal_Pose_CVPR_2021_paper.pdf">[CVPR2021]</a> <a href="https://github.com/chaneyddtt/UDA-Animal-Pose">[code coming soon]</a></li>
<li>Meta Self-Learning for Multi-Source Domain Adaptation: A Benchmark <a href="https://arxiv.org/abs/2108.10840">[ICCV Workshop 2021]</a> <a href="https://github.com/bupt-ai-cz/Meta-SelfLearning">[Pytorch]</a></li>
<li>Adapting Neural Architectures Between Domains <a href="https://github.com/liyxi/AdaptNAS">[NeurlPS 2020]</a></li>
<li>Unsupervised Domain Attention Adaptation Network for Caricature Attribute Recognition <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123530018.pdf">[ECCV2020]</a></li>
<li>A Broader Study of Cross-Domain Few-Shot Learning <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123720120.pdf">[ECCV2020]</a></li>
<li>Label-Noise Robust Domain Adaptation <a href="https://proceedings.icml.cc/static/paper_files/icml/2020/1942-Paper.pdf">[ICML2020]</a></li>
<li>Unsupervised Domain Adaptation of a Pretrained Cross-Lingual Language Model <a href="https://www.ijcai.org/Proceedings/2020/0508.pdf">[IJCAI2020]</a></li>
<li>Domain Adaptation for Semantic Parsing <a href="https://www.ijcai.org/Proceedings/2020/0515.pdf">[IJCAI2020]</a></li>
<li>Bridging Cross-Tasks Gap for Cognitive Assessment via Fine-Grained Domain Adaptation <a href="https://www.ijcai.org/Proceedings/2020/0597.pdf">[IJCAI2020]</a></li>
<li>Clarinet: A One-step Approach Towards Budget-friendly Unsupervised Domain Adaptation <a href="https://www.ijcai.org/Proceedings/2020/0350.pdf">[IJCAI2020]</a></li>
<li>Weakly-Supervised Domain Adaptation via GAN and Mesh Model for Estimating 3D Hand Poses Interacting Objects <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Baek_Weakly-Supervised_Domain_Adaptation_via_GAN_and_Mesh_Model_for_Estimating_CVPR_2020_paper.pdf">[CVPR2020]</a></li>
<li>One-Shot Domain Adaptation for Face Generation <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_One-Shot_Domain_Adaptation_for_Face_Generation_CVPR_2020_paper.pdf">[CVPR2020]</a></li>
<li>Learning Meta Face Recognition in Unseen Domains <a href="hhttp://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Learning_Meta_Face_Recognition_in_Unseen_Domains_CVPR_2020_paper.pdf">[CVPR2020 Oral]</a> <a href="https://github.com/cleardusk/MFR">[code]</a></li>
<li>Cross-Domain Document Object Detection: Benchmark Suite and Method <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Cross-Domain_Document_Object_Detection_Benchmark_Suite_and_Method_CVPR_2020_paper.pdf">[CVPR2020]</a> <a href="https://github.com/kailigo/cddod">[code]</a></li>
<li>StereoGAN: Bridging Synthetic-to-Real Domain Gap by Joint Optimization of Domain Translation and Stereo Matching <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_StereoGAN_Bridging_Synthetic-to-Real_Domain_Gap_by_Joint_Optimization_of_Domain_CVPR_2020_paper.pdf">[CVPR2020]</a></li>
<li>Domain Adaptation for Image Dehazing <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Shao_Domain_Adaptation_for_Image_Dehazing_CVPR_2020_paper.pdf">[CVPR2020]</a></li>
<li>Probability Weighted Compact Feature for Domain Adaptive Retrieval <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_Probability_Weighted_Compact_Feature_for_Domain_Adaptive_Retrieval_CVPR_2020_paper.pdf">[CVPR2020]</a> <a href="https://github.com/fuxianghuang1/PWCF">[code]</a></li>
<li>Disparity-Aware Domain Adaptation in Stereo Image Restoration <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Yan_Disparity-Aware_Domain_Adaptation_in_Stereo_Image_Restoration_CVPR_2020_paper.pdf">[CVPR2020]</a></li>
<li>Multi-Path Learning for Object Pose Estimation Across Domains <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Sundermeyer_Multi-Path_Learning_for_Object_Pose_Estimation_Across_Domains_CVPR_2020_paper.pdf">[CVPR2020]</a></li>
<li>Unsupervised Domain Adaptation for 3D Human Pose Estimation <a href="https://dl.acm.org/citation.cfm?id=3351052">[ACM MM2019]</a></li>
<li>PointDAN: A Multi-Scale 3D Domain Adaption Network for Point Cloud Representation <a href="https://arxiv.org/abs/1911.02744v1">[NeurIPS 2019]</a> <a href="https://github.com/canqin001/PointDAN">[code]</a></li>
<li>Deep Head Pose Estimation Using Synthetic Images and Partial Adversarial Domain Adaption for Continuous Label Spaces <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Kuhnke_Deep_Head_Pose_Estimation_Using_Synthetic_Images_and_Partial_Adversarial_ICCV_2019_paper.pdf">[ICCV2019]</a></li>
<li>Cross-Domain Adaptation for Animal Pose Estimation <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Cao_Cross-Domain_Adaptation_for_Animal_Pose_Estimation_ICCV_2019_paper.pdf">[ICCV2019]</a></li>
<li>GA-DAN: Geometry-Aware Domain Adaptation Network for Scene Text Detection and Recognition <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhan_GA-DAN_Geometry-Aware_Domain_Adaptation_Network_for_Scene_Text_Detection_and_ICCV_2019_paper.pdf">[ICCV2019]</a></li>
<li>Accelerating Deep Unsupervised Domain Adaptation with Transfer Channel Pruning <a href="https://arxiv.org/abs/1904.02654">[IJCNN]</a></li>
<li>Adversarial Adaptation of Scene Graph Models for Understanding Civic Issues <a href="https://arxiv.org/abs/1901.10124">[WWW2019]</a></li>
<li>Cross-Dataset Adaptation for Visual Question Answering <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Chao_Cross-Dataset_Adaptation_for_CVPR_2018_paper.pdf">[CVPR2018]</a></li>
<li>Cross-domain fault diagnosis through optimal transport for a CSTR process <a href="https://www.sciencedirect.com/science/article/pii/S2405896322009727">[DYCOPS2022]</a> <a href="https://github.com/eddardd/CrossDomainFaultDiagnosis">[Code]</a></li>
</ul>
<p><strong>Journal</strong></p>
<ul>
<li>DASGIL: Domain Adaptation for Semantic and Geometric-Aware Image-Based Localization <a href="https://ieeexplore.ieee.org/document/9296559">[TIP2020]</a> <a href="https://github.com/HanjiangHu/DASGIL">[Pytorch]</a> </li>
<li>An Unsupervised Domain Adaptation Scheme for Single-Stage Artwork Recognition in Cultural Sites <a href="https://arxiv.org/abs/2008.01882v3">[Image and Vision Computing 2020]</a> <a href="https://github.com/fpv-iplab/DA-RetinaNet">[Pytorch]</a> <a href="https://iplab.dmi.unict.it/EGO-CH-OBJ-UDA/">[Project]</a></li>
<li>Multi-source transfer learning of time series in cyclical manufacturing <a href="https://link.springer.com/article/10.1007/s10845-019-01499-4">[JIntellManuf2020]</a></li>
<li>Domain adaptation for regression under Beer-Lambert’s law <a href="https://www.sciencedirect.com/science/article/abs/pii/S0950705120305761">[KBS2020]</a></li>
</ul>
<p><strong>Arxiv</strong></p>
<ul>
<li>Open-Ended Visual Question Answering by Multi-Modal Domain Adaptation <a href="https://arxiv.org/abs/1911.04058">[11 Nov 2019]</a></li>
<li>DANE: Domain Adaptive Network Embedding <a href="https://arxiv.org/abs/1906.00684v1">[arXiv 3 Jun 2019]</a></li>
<li>Active Adversarial Domain Adaptation <a href="https://arxiv.org/abs/1904.07848v1">[arXiv 16 Apr 2019]</a></li>
</ul>
<h2 id="Benchmarks"><a href="#Benchmarks" class="headerlink" title="Benchmarks"></a>Benchmarks</h2><ul>
<li>Kurcuma: a kitchen utensil recognition collection for unsupervised domain adaptation <a href="https://link.springer.com/article/10.1007/s10044-023-01147-x">[PAA2023]</a> <a href="https://www.dlsi.ua.es/~jgallego/datasets/kurcuma/">[Dataset]</a></li>
<li>Meta Self-Learning for Multi-Source Domain Adaptation: A Benchmark <a href="https://arxiv.org/abs/2108.10840">[ICCV Workshop 2021]</a> <a href="https://github.com/bupt-ai-cz/Meta-SelfLearning">[Pytorch]</a></li>
<li>LLVIP: A Visible-infrared Paired Dataset for Low-light Vision <a href="https://arxiv.org/abs/2108.10831">[ICCV Workshop 2021]</a> <a href="https://github.com/bupt-ai-cz/LLVIP">[Pytorch]</a></li>
<li>Syn2Real: A New Benchmark forSynthetic-to-Real Visual Domain Adaptation <a href="https://arxiv.org/abs/1806.09755v1">[arXiv 26 Jun]</a> <a href="http://ai.bu.edu/syn2real/">[Project]</a></li>
<li>Benchmarking Neural Network Robustness to Common Corruptions and Perturbations (ImageNet-C) <a href="https://arxiv.org/abs/1903.12261">[ICLR 2019]</a> <a href="https://github.com/hendrycks/robustness">[PyTorch]</a></li>
</ul>
<h1 id="Library"><a href="#Library" class="headerlink" title="Library"></a>Library</h1><ul>
<li><a href="https://github.com/thuml/Transfer-Learning-Library">Transfer-Learning-Library</a></li>
<li><a href="https://github.com/easezyc/deep-transfer-learning">deep-transfer-learning: a PyTorch library for deep transfer learning</a></li>
<li><a href="https://domainadaptation.org/">salad: a Semi-supervised Adaptive Learning Across Domains</a></li>
<li><a href="https://github.com/KaiyangZhou/Dassl.pytorch">Dassl: a PyTorch toolbox for domain adaptation and semi-supervised learning</a></li>
<li><a href="https://github.com/jolibrain/joliGEN">joliGEN: an integrated framework for training custom generative AI image-to-image models</a></li>
</ul>
<h1 id="Lectures-and-Tutorials"><a href="#Lectures-and-Tutorials" class="headerlink" title="Lectures and Tutorials"></a>Lectures and Tutorials</h1><ul>
<li>A Primer on Domain Adaptation <a href="https://arxiv.org/abs/2001.09994v2">[PDF]</a></li>
</ul>
<h1 id="Other-Resources"><a href="#Other-Resources" class="headerlink" title="Other Resources"></a>Other Resources</h1><ul>
<li><a href="https://github.com/jindongwang/transferlearning">transferlearning</a></li>
</ul>
]]></content>
      <categories>
        <category>Reference</category>
      </categories>
      <tags>
        <tag>Computer vision</tag>
        <tag>Toturial</tag>
        <tag>Neural Network</tag>
        <tag>Deep learning</tag>
      </tags>
  </entry>
</search>
